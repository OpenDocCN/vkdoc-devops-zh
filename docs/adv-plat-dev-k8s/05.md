# 5.管道

所有的软件平台都是通过一种或另一种形式的数据通信来运行的。数据驱动服务使用数据来确定流程(或逻辑)，而事件驱动服务侦听事件来执行预定的流程和逻辑。从另一个意义上来说，这些服务是在传递数据，而数据的传输本身就是一个事件。数据驱动与事件驱动的架构通常归结为它们处理的数据的意义和价值，以及消费数据的服务如何操作、转换、处理或分析数据。

管理数据的应用程序通常是软件平台的核心组件。如何处理和维护数据通常由业务需求驱动的业务逻辑决定。一个成熟的软件平台应该提供高效灵活的应用逻辑，能够适应一系列数据管理需求，支持数据驱动和事件驱动的实现。

物联网、机器学习和区块链技术产生和消费数据，每种技术都有特定的需求:物联网设备在交流状态的同时产生和消费大量实时数据。机器学习通常需要大量的、有组织的数据目录。最后，区块链以数据的形式消费和发射事件。

本书的上下文和手头的平台认为数据尽可能通用，主要关注收集和分发，将数据的意义和价值留给更高级别的应用程序需求。本章重点介绍如何通过数据管道将数据从发布服务器移动到订阅服务器。

## 国家和 Kubernetes

数据库管理系统(数据库)是有状态应用程序。传统的整体架构通常围绕一个跨多个域共享的中央数据库，而微服务架构则倾向于多个数据库，这些数据库被隔离到特定的服务和域。不管整个系统架构如何，数据库应用程序都是有状态的，并且不是所有有状态的应用程序都能轻松利用 Kubernetes 的优势。

Kubernetes 的容器化和容器编排非常适合构建由独立的、可移植的和可伸缩的组件组成的可扩展系统。所有 Kubernetes 平台的核心是 Pods，即最好被视为短暂和无状态的容器化工作负载。pod 预计会来来去去，根据规则扩展和缩减，或者在底层基础架构出现故障或进行维护时意外终止和重新生成。设计以这种方式运行的组件实现了 Kubernetes 强大的可伸缩性、性能、冗余和自我修复能力的优势。在这种架构中操作有状态的应用程序可能很有挑战性。然而，许多应用程序可以在分布式体系结构上维护状态；这些应用非常适合 Kubernetes。

Kubernetes 适合通过无状态 pod 支持的庞大服务网络来交流数据和事件。然而，数据的存储和检索是有状态的活动，这个概念不太适合 Kubernetes 的短暂工作负载。移除或更换 Pod 后，Pod 中存储的数据会丢失。pod 可以连接到外部永久卷；然而，在多个 pod 之间共享持久卷并没有得到存储提供商的广泛支持，并且难以实现可接受的性能和稳定性。最后，在单个 Pod 中操作有状态的应用程序既不可伸缩也不容错。与 Kubernetes 配合良好的有状态应用程序(如数据库)通过分布式节点网络维护状态。

Kubernetes 以 StatefulSets 的形式提供有状态功能。StatefulSets 基本上等同于部署，但是部署枚举的 pod 能够通过卷声明模板重新附加到分配给它们的存储。StatefulSet 创建以序号索引命名的 pod，例如`PODNAME-0..n`，并提供稳定的网络 ID，例如`PODNAME-4.somenamespace.svc.cluster.local`。如果 StatefulSet 中的一个单元崩溃、升级或重新安排到另一个节点，Kubernetes 会创建一个具有相同名称和网络 ID 的新单元，并重新挂接以前与该单元名称关联的任何持久性卷。即使它们的工作负载是短暂的，但通过持久的命名和存储，pod 在概念上是有状态的。Elasticsearch、Cassandra 和 Kafka 是数据管理应用程序的一些例子，当部署在多个有状态的 pod 中时，它们工作得很好。像这样的系统管理它们的数据复制、分布式处理，并在其自我管理的集群网络中处理故障或缺失的节点。正确配置后，这些应用程序会在缺少一个节点(在本例中为一个单元)时继续执行，并且通常会提供无限的水平扩展。在某些方面，Kubernetes 使扩展和管理这些应用程序比传统方法更容易，传统方法涉及配置虚拟机或裸机服务器的繁琐任务。相比之下，Kubernetes 只需编辑和重新应用配置，就能增加所需的 pod 数量。

## 实时数据体系结构

旨在提供机器学习、物联网和区块链功能的软件平台需要收集、转换、处理和管理数据、元数据和指标的能力。接下来的几章将介绍一些能够在 Kubernetes 集群中运行的企业级应用，提供实时数据收集、路由、转换、索引以及数据和指标的管理(参见图 [5-1](#Fig1) )。

![../images/483120_1_En_5_Chapter/483120_1_En_5_Fig1_HTML.jpg](../images/483120_1_En_5_Chapter/483120_1_En_5_Fig1_HTML.jpg)

图 5-1

数据管理架构

以下部分涵盖了本章中用于组装实时分布式流平台的技术，该平台可用于任何形式的入站数据，并且是为物联网专门定制的。

### 消息和事件队列

Web 和移动应用程序通常在有限的一组已定义的数据结构上运行；自定义 API 端点在结构化数据库中验证、验证和持久化这些对象。相比之下，物联网平台通常需要能够接受各种互联设备产生的各种数据类型和结构。物联网平台主要负责向其他设备和服务分发设备状态。

发布和订阅(也称为发布/订阅)应用程序解决了收集和分发数据(通常称为事件或消息)的问题，并将几乎无限数量的生产者与几乎任意数量的感兴趣的消费者联系起来。本章重点介绍如何为所有实时数据和事件分发实现高度可用的分布式流媒体平台 Apache Kafka，以及为物联网专门构建的 MQTT 代理 Mosquitto。

#### 分布式流媒体平台

Apache Kafka 将自己标榜为分布式流媒体平台，并充当平台内实时数据操作的一种“中枢神经系统”。Kafka 可以通过六个节点和适当的配置每秒处理数十万条消息，这一能力远远超过了以数据为中心的大型企业之外的大多数用例。Kafka 将消息(数据)记录为主题内的记录，由键、值和时间戳组成。Kafka 通过其消费者、生产者、流和连接器 API 管理记录的关系、收集和分发。

Kafka 为所有主流编程语言提供了稳定成熟的客户端库，允许定制平台组件实时操作数据(事件和消息)。

#### MQTT 和物联网

MQTT <sup>[1](#Fn1)</sup> (消息队列遥测传输)是一种发布/订阅消息协议，专为物联网和 IIoT(工业物联网)实现而设计。设备可以通过 MQTT 代理发布和订阅主题，代理可以被桥接在一起。MQTT 被设计成轻量级的，能够在资源受限的环境中运行，包括 Raspberry Pi。<sup>2[2](#Fn2)T7】</sup>

考虑一个组织，该组织有一个工厂，操作数千个传感器和控制器，直接与内部 MQTT 代理通信。该组织还订阅了一个云托管的 MQTT 解决方案，用于从远程或隔离的设备收集指标。如图 [5-2](#Fig2) 所示，这些代理中的每一个都可以桥接在一起，并与一个更大的数据平台进行双向通信。

![../images/483120_1_En_5_Chapter/483120_1_En_5_Fig2_HTML.jpg](../images/483120_1_En_5_Chapter/483120_1_En_5_Fig2_HTML.jpg)

图 5-2

MQTT 网络

Mosquitto 是一个流行的开源 MQTT 代理，本章稍后将为集群内 MQTT 操作进行配置。Mosquitto 可以发布和订阅任何其他 MQTT 兼容的代理，通常是本地或基于云的 SaaS。

## 发展环境

本章使用第 [4](04.html) 章中相同的、廉价的 Scaleway 四节点集群设置。该集群包括一个用于 Kubernetes 主节点的 DEV1-M (3 个 CPU/4G RAM/40G SSD)和三个用于工作节点的 DEV1-L (4 个 CPU/8G RAM/80GB SSD)。配置和操作 Apache Kafka、Apache NiFi、Elasticsearch 和 Logstash 利用了这个小型开发集群的大部分 CPU 和 RAM。这些规格是绝对的最低要求，应该根据需要进行扩展。

Note

本章中使用的整个三节点开发集群相当于一个 12 核 CPU/32GB RAM 服务器实例，类似于典型企业配置中的单个生产节点。

### 集群范围的配置

本章使用与第 [3 章](03.html)中详述的相同的通用 Kubernetes 配置，包括入口 Nginx、证书管理器、Rook-Ceph 和监控配置。如果遵循前面的章节，从清单 [3-15](https://doi.org/10.1007/978-1-4842-5611-4_3-15) 到`cluster-apk8s-dev3/000-cluster`中的目录`cluster-apk8s-dev1/000-cluster`复制并应用配置清单，并创建目录`cluster-apk8s-dev3/003-data`(如清单 [5-1](#PC1) 所示)来保存本章使用的清单。

```
.
└── cluster-apk8s-dev1
└── cluster-apk8s-dev2
└── cluster-apk8s-dev3
   └── 000-cluster
      ├── 00-ingress-nginx
      ├── 10-cert-manager
      ├── 20-rook-ceph
      └── 30-monitoring
   └── 003-data

Listing 5-1Development Cluster configuration layout

```

Note

以数字为前缀的目录是推断优先顺序的一个简单约定。配置清单通常可以以任何顺序应用。

### 数据命名空间

创建目录`cluster-apk8s-dev3/003-data/000-namespace`来包含名称空间范围的配置清单。考虑到其规模和演示用途，此开发集群应被视为单租户。然而，在基于租户的名称空间(例如`clientx-data`)中包含所有与数据相关的功能，允许应用细粒度的基于角色的访问控制和网络规则。因为这个开发集群只有一个租户，所以名称空间`data`是合适的。以下配置适用于新数据命名空间中的所有服务。

在`cluster-apk8s-dev3/003-data/000-namespace`目录中，在清单 [5-2](#PC2) 中的一个名为`00-namespace.yml`的文件中创建一个 Kubernetes 名称空间。

```
apiVersion: v1
kind: Namespace
metadata:
  name: data

Listing 5-2Data Namespace

```

应用命名空间配置:

```
$ kubectl apply -f 00-namespace.yml

```

### TLS 证书

在本章的后面，安全入口配置应用于`nifi.data.dev3.apk8s.dev`的 NiFi 和`kib.data.dev3.apk8s.dev`的 Kibana，提供对其用户界面的外部访问。证书管理器和集群发行者应该出现在集群中(参见第 [3](03.html) 章)。

在清单 [5-3](#PC4) 中的文件`05-certs.yml`中创建一个证书配置。

```
apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: data-cert
  namespace: data
spec:
  secretName: data-production-tls
  issuerRef:
    name: letsencrypt-production
    kind: ClusterIssuer
  commonName: data.dev3.apk8s.dev
  dnsNames:
    - data.dev3.apk8s.dev
    - nifi.data.dev3.apk8s.dev
    - kib.data.dev3.apk8s.dev
  acme:
    config:
      - http01:
          ingressClass: nginx
        domains:
          - data.dev3.apk8s.dev
          - nifi.data.dev3.apk8s.dev
          - kib.data.dev3.apk8s.dev

Listing 5-3Certificates for the data Namespace

```

应用证书配置:

```
$ kubectl apply -f 05-certs.yml

```

### 基本认证

开发集群使用基本认证 <sup>[3](#Fn3)</sup> (基本认证)作为保护入口的便利方法。跨入口使用单个基本身份验证秘密简化了开发过程中身份验证的使用，并且在需要时可以用更复杂的方法(如 OAuth)来代替。

用实用程序`htpasswd` : <sup>[4](#Fn4)</sup> 创建一个名为 auth 的文件

```
$ htpasswd -c ./auth sysop

```

用前面生成的`auth`文件创建一个名为`sysop-basic-auth`的通用 Kubernetes 秘密:

```
$ kubectl create secret generic sysop-basic-auth \
--from-file auth -n data

```

## 阿帕奇动物园管理员

Apache Zookeeper<sup>[5](#Fn5)</sup>已经成为许多需要分布式协作的流行应用的标准，包括 Hadoop、 <sup>[6](#Fn6)</sup> HBase、 <sup>[7](#Fn7)</sup> Kafka、NiFi 和 Elasticsearch。在这一章中，Kafka 和 NiFi 都使用了一个共享的 Zookeeper 集群。Elasticsearch 在这个环境中作为单个节点运行；然而，更大的弹性搜索集群也可以利用共享的动物园管理员。

Zookeeper 可以被缩放以容忍给定数量的故障节点；然而，一些架构倾向于多个独立的安装，以避免单点故障。缩放或多个 Zookeeper 配置是一个生产问题，而在开发环境中共享该服务可以更好地利用有限的资源。

创建目录`cluster-apk8s-dev3/003-data/010-zookeeper`。在新的`010-zookeeper`目录中，从清单 [5-4](#PC8) 中创建一个名为`10-service.yml`的文件。

```
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
  namespace: data
spec:
  ports:
    - name: client
      port: 2181
      protocol: TCP
      targetPort: client
  selector:
    app: zookeeper
  sessionAffinity: None
  type: ClusterIP

Listing 5-4Zookeeper Service

```

应用 Zookeeper 服务配置:

```
$ kubectl apply -f 10-service.yml

```

Kafka 和 NiFi 等 Zookeeper 客户端管理它们与单个节点的关系。Kubernetes 中的标准服务定义被分配了一个 IP 地址，通常被配置为将与该服务的任何通信路由到与指定选择器和端口匹配的任何 Pod。然而，像 Zookeeper 这样的应用程序要求每个 Zookeeper 节点(作为 Pod 运行)能够与它的对等节点(作为 Pod 运行)进行通信。标准的 Kubernetes 服务不足以在对等感知集群中使用，因为每个节点都必须能够专门与其他节点通信，而不仅仅是与选择器和端口匹配的任何节点。Kubernetes 通过无头服务的概念提供这种功能， <sup>[8](#Fn8)</sup> ，这是一种没有定义`ClusterIP`(`clusterIP: None`)的服务。下面的服务定义创建了一个无头服务，为匹配选择器 app 的 pod 返回 DNS 条目:zookeeper，如下一节描述的 StatefulSet 中所定义的。

在清单 [5-5](#PC10) 中的文件`10-service-headless.yml`中创建一个无头服务配置。

```
apiVersion: v1
kind: Service
metadata:
  name: zookeeper-headless
  namespace: data
spec:
  clusterIP: None
  ports:
    - name: client
      port: 2181
      protocol: TCP
      targetPort: 2181
    - name: election
      port: 3888
      protocol: TCP
      targetPort: 3888
    - name: server
      port: 2888
      protocol: TCP
      targetPort: 2888
  selector:
    app: zookeeper
  sessionAffinity: None
  type: ClusterIP

Listing 5-5Zookeeper Headless Service

```

应用 Zookeeper 无头服务配置:

```
$ kubectl apply -f 10-service-headless.yml

```

接下来，在清单 [5-6](#PC12) 中的文件`40-statefulset.yml`中创建一个 StatefulSet 配置。

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zookeeper
  namespace: data
spec:
  podManagementPolicy: OrderedReady
  replicas: 2
  revisionHistoryLimit: 1
  selector:
    matchLabels:
      app: zookeeper
  serviceName: zookeeper-headless
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      containers:
        - command:
            - /bin/bash
            - -xec
            - zkGenConfig.sh && exec zkServer.sh start-foreground
          env:
            - name: ZK_REPLICAS
              value: "2"
            - name: JMXAUTH
              value: "false"
            - name: JMXDISABLE
              value: "false"
            - name: JMXPORT
              value: "1099"
            - name: JMXSSL
              value: "false"
            - name: ZK_CLIENT_PORT
              value: "2181"
            - name: ZK_ELECTION_PORT
              value: "3888"
            - name: ZK_HEAP_SIZE
              value: 1G
            - name: ZK_INIT_LIMIT
              value: "5"
            - name: ZK_LOG_LEVEL
              value: INFO
            - name: ZK_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZK_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZK_MIN_SESSION_TIMEOUT
              value: "4000"
            - name: ZK_PURGE_INTERVAL

              value: "0"
            - name: ZK_SERVER_PORT
              value: "2888"
            - name: ZK_SNAP_RETAIN_COUNT
              value: "3"
            - name: ZK_SYNC_LIMIT
              value: "10"
            - name: ZK_TICK_TIME
              value: "2000"
          image: gcr.io/google_samples/k8szk:v3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - zkOk.sh
            failureThreshold: 3
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: zookeeper
          ports:
            - containerPort: 2181
              name: client
              protocol: TCP
            - containerPort: 3888
              name: election
              protocol: TCP
            - containerPort: 2888
              name: server
              protocol: TCP
          readinessProbe:
            exec:
              command:

                - zkOk.sh
            failureThreshold: 3
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
            - mountPath: /var/lib/zookeeper
              name: data
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      terminationGracePeriodSeconds: 30
      volumes:
        - emptyDir: {}
          name: data
  updateStrategy:
    type: OnDelete

Listing 5-6Zookeeper StatefulSet

```

最后，应用 Zookeeper StatefulSet 配置:

```
$ kubectl apply -f 40-statefulset.yml

```

Zookeeper 是 Kafka 和 NiFi 的共享依赖项，现在可以在开发集群上使用。下一节将建立一个双节点 Apache Kafka 集群。

## 阿帕奇卡夫卡

Apache Kafka”用于构建实时数据管道和流媒体应用。它具有横向可伸缩性、容错性、极快的速度，并在数千家公司的生产中运行。” <sup>[9](#Fn9)</sup> 以下配置建立了一个双节点 Kafka 集群，非常适合小规模开发环境和数据科学活动。如前所述，Apache Kafka 是这个数据驱动平台的中枢神经系统。除了在每一种主流编程语言中提供稳定且功能丰富的客户端库之外，许多数据管理应用程序还开发了一流的连接器来发布和订阅 Kafka 事件，包括 Logstash 和 NiFi，这将在本章的后面进行演示；见图 [5-3](#Fig3) 。

![../images/483120_1_En_5_Chapter/483120_1_En_5_Fig3_HTML.jpg](../images/483120_1_En_5_Chapter/483120_1_En_5_Fig3_HTML.jpg)

图 5-3

阿帕奇卡夫卡

Apache Kafka 等高性能、低延迟事件队列在高度优化的专用裸机服务器上以最高效率运行。在容器中，在具有抽象存储和覆盖网络的共享虚拟实例上运行 Kafka(如本书中定义的 Kubernetes 集群)，会显著降低其效率和吞吐量。然而，在某些情况下，最佳性能的降低可能不会被注意到，或者可能很容易通过缩放来补偿。在 Kubernetes 中运行 Kafka 带来了许多优势，包括统一网络、DNS、可伸缩性、自我修复、安全性、监控以及与其他组件的统一控制平面。在概念层面上，Kafka 是构成本书所描述的基于 Kubernetes 的数据平台的众多组件之一。虽然 Kafka 本身可能不会从 Kubernetes 内部的管理中获得大量利润，但更大的数据平台通过其基本组件之间更高的凝聚力而受益于 Kafka 的包容性。

以下配置以类似于 Zookeeper(在上一节中配置)的方式设置 Kafka，并添加了持久卷，如图 [5-4](#Fig4) 所示。

![../images/483120_1_En_5_Chapter/483120_1_En_5_Fig4_HTML.png](../images/483120_1_En_5_Chapter/483120_1_En_5_Fig4_HTML.png)

图 5-4

阿帕奇卡夫卡和动物园管理员 Kubernetes 配置

创建目录`cluster-apk8s-dev3/003-data/020-kafka`。在新的`020-kafka`目录中，从清单 [5-7](#PC14) 中创建一个名为`10-service.yml`的文件。

```
apiVersion: v1
kind: Service
metadata:
  name: kafka
  namespace: data
spec:
  ports:
    - name: broker
      port: 9092
      protocol: TCP
      targetPort: kafka
  selector:
    app: kafka
  sessionAffinity: None
  type: ClusterIP

Listing 5-7Kafka Service

```

应用 Kafka 服务配置:

```
$ kubectl apply -f 10-service.yml

```

接下来，在清单 [5-8](#PC16) 中名为`10-service-headless.yml`的文件中为 Kafka 创建一个无头服务配置。

```
apiVersion: v1
kind: Service
metadata:
  name: kafka-headless
  namespace: data
spec:
  clusterIP: None
  ports:
    - name: broker
      port: 9092
      protocol: TCP
      targetPort: 9092
  selector:
    app: kafka
  sessionAffinity: None
  type: ClusterIP

Listing 5-8Kafka Headless Service

```

应用 Kafka Headless 服务配置:

```
$ kubectl apply -f 10-service-headless.yml

```

接下来，在清单 [5-9](#PC18) 中名为`40-statefulset.yml`的文件中为 Kafka 创建一个 StatefulSet 配置。

以下配置使用由 Confluent Inc. <sup>[10](#Fn10)</sup> 维护的 Kafka 容器 Confluent 为其围绕 Kafka 构建的开源事件流平台提供商业支持。本书中使用的 Kafka 功能可以与 Confluent 的 Kafka 发行版和标准的上游 Apache Kafka 一起使用。

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: kafka
  name: kafka
  namespace: data
spec:
  podManagementPolicy: OrderedReady
  replicas: 2
  revisionHistoryLimit: 1
  selector:
    matchLabels:
      app: kafka
  serviceName: kafka-headless
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
        - command:
            - sh
            - -exc

            - |
              unset KAFKA_PORT && \
              export KAFKA_BROKER_ID=${HOSTNAME##*-} && \
              export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_IP}:9092 && \
              exec /etc/confluent/docker/run
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: KAFKA_HEAP_OPTS
              value: -Xmx1G -Xms1G
            - name: KAFKA_ZOOKEEPER_CONNECT
              value: zookeeper-headless:2181
            - name: KAFKA_LOG_DIRS
              value: /opt/kafka/data/logs
            - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_JMX_PORT
              value: "5555"
          image: confluentinc/cp-kafka:5.3.1-1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
                - sh
                - -ec
                - /usr/bin/jps | /bin/grep -q SupportedKafka

            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kafka-broker
          ports:
            - containerPort: 9092
              name: kafka
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: kafka
            timeoutSeconds: 5
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
            - mountPath: /opt/kafka/data
              name: datadir
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 60
  updateStrategy:
    type: OnDelete
  volumeClaimTemplates:
    - metadata:
        name: datadir
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 5Gi
        storageClassName: rook-ceph-block

Listing 5-9Kafka StatefulSet

```

应用 Kafka StatefulSet 配置:

```
$ kubectl apply -f 40-statefulset.yml

```

Kubernetes Pod 中断预算 <sup>[11](#Fn11)</sup> 限制在任何给定时间允许停机的 Pod 数量，节点故障或 Pod 错误等计划外停机除外。PodDisruptionBudget 配置对于更新表示高可用性集群(如 Kafka)的有状态集特别有用。通过适当的配置和资源，Kafka 集群可以保持完全运行，而一部分节点处于离线状态。

Note

由于本章中指定的开发环境的资源有限，下面定义的 Kafka 配置是稳定的，但不是高度可用的。 <sup>[12](#Fn12)</sup>

在清单 [5-10](#PC20) 中名为`45-pdb.yml`的文件中为 Kafka 创建一个 PodDisruptionBudget 配置。

```
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: kafka
  namespace: data
  labels:
    app: kafka
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: kafka

Listing 5-10Kafka Pod Disruption Budget

```

最后，应用 Kafka PodDisruptionBudget 配置:

```
$ kubectl apply -f 45-pdb.yml

```

新的开发环境现在运行一个双节点 Kafka 集群。下一节将为测试和调试设置 Pod。

### Kafka 客户端实用程序窗格

Kafka 可以用最少的管理开销在这个数据驱动平台的几乎每个组件之间代理数据。但是，Confluence 提供的 Kafka 容器包含几个有用的脚本(见表 [5-1](#Tab1) )用于测试、备份、安全配置和一般管理功能。运行 Kafka 客户端实用程序 Pod 提供了对这个关键平台组件的命令行管理访问。

Kafka 测试客户机 Pod 运行与前面配置的操作集群相同的`confluentinc/cp-kafka:5.3.1-1`映像。然而，Pod 被配置为执行命令`tail -f /dev/null`而不是标准入口点，保持`tail`为活动进程并阻止 Pod 完成。

在清单 [5-11](#PC22) 中的文件`99-pod-test-client.yml`中创建一个 Kafka 测试客户端配置。

```
apiVersion: v1
kind: Pod
metadata:
  name: kafka-client-util
  namespace: data
spec:
  containers:
    - command:
        - sh
        - -c
        - exec tail -f /dev/null
      image: confluentinc/cp-kafka:5.3.1-1
      imagePullPolicy: IfNotPresent
      name: kafka
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File

Listing 5-11Kafka test client Pod

```

应用 Kafka 测试客户端 Pod 配置:

```
$ kubectl apply -f 99-pod-client-util.yml

```

Kafka 测试客户端 Pod 中运行的容器映像`confluentinc/cp-kafka:4.1.2-2`附带了表 [5-1](#Tab1) 中列出的实用程序。运行表 [5-1](#Tab1) 中列出的任何命令，带有`--help`标志，以获得配置参数列表。Cloudera 在其网站上提供了详细的文档: *Kafka 使用命令行工具管理*。 <sup>[13](#Fn13)</sup>

表 5-1

Kafka 客户端实用程序脚本

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

脚本

 | 

描述

 |
| --- | --- |
| 卡夫卡-acls | 用户认证、授权和访问控制列表管理。 |
| Kafka-broker-API-版本 | 列出集群中所有节点的 API 版本。 |
| 卡夫卡-config | 添加/删除主题、客户机、用户或代理的实体配置。 |
| **卡夫卡——控制台——消费者** | 用于创建、更改、列出和描述主题。 |
| **卡夫卡-控制台-制片人** | 从标准输出中读取数据，并将其写入 Kafka 主题。 |
| 卡夫卡-消费群体 | 列出当前的消费群体。 |
| 卡夫卡-消费者-抵销-检查 | 返回特定使用者组中每个使用者读取和写入的带有滞后时间的消息数。 |
| 卡夫卡-消费者-性能-测试 | 对代理和主题运行消费者性能测试。 |
| 卡夫卡-委托-代币 | 委托令牌是经纪人和客户之间共享的秘密。创建、描述、续订和终止令牌。 |
| 卡夫卡-删除-记录 | 从具有给定偏移量的主题中删除记录。 |
| 卡夫卡日志-dirs | 输出一个 JSON 对象，包含每个代理的日志信息。 |
| 卡夫卡-镜子制造者 | 复制卡夫卡集群。 <sup>[14](#Fn14)</sup> |
| 卡夫卡-首选-复制品-选举 | 重新平衡主题。 <sup>[十五](#Fn15)</sup> |
| 卡夫卡-生产者-性能-测试 | 对代理和主题运行生产者性能测试。 |
| 卡夫卡-重新分配-分区 | 将 Kafka 主题分区领导者重新分配给不同的 Kafka 代理。 |
| 卡夫卡-重播-日志-制作人 | 消费来自一个主题的消息，并在另一个主题中重放(产生)它们。 |
| 卡夫卡-复制品-验证 | 验证一个或多个主题的数据复制是否正确。 |
| 卡夫卡式的 | 提供直接调用 Kafka 类的能力；主要由其他脚本使用。 |
| Kafka-服务器-开始卡夫卡-服务器-停止 | 在管理汇合分配的情况下不使用。 |
| 卡夫卡-流-应用程序-重置 | 重置 Kafka Streams <sup>[16](#Fn16)</sup> 应用程序的内部状态。 |
| **卡夫卡-主题** | 创建、列出、配置和删除主题。 |
| 卡夫卡-可验证的-生产者卡夫卡-可验证的-消费者 | 生成并使用一定数量的消息进行测试。 |

Testing Kafka

1.  在新的 Kafka 测试客户端 Pod 上参加一个 Bash <sup>[17](#Fn17)</sup> 会话。

    ```
    $ kubectl exec -it kafka-client-util bash -n data

    ```

2.  在新 Pod 的命令行中，创建主题`test`和**一个分区**和**一个副本**。

    ```
    # kafka-topics --zookeeper zookeeper-headless:2181 \
    --topic test --create --partitions 1 --replication-factor

    ```

3.  列出 Kafka 集群中的所有主题。新的集群应该只有测试主题以及一个或多个以两个下划线开头的内部主题。

    ```
    # kafka-topics --zookeeper zookeeper-headless:2181 --list

    ```

4.  听新的测试题目。Kafka-控制台-消费者向控制台打印消息。

    ```
    # kafka-console-consumer --bootstrap-server kafka:9092 \
    --topic test

    ```

5.  从一个单独的终端打开 Kafka test client Pod 上的一个附加 Bash 会话。

    ```
    $ kubectl exec -it kafka-client-util bash -n data

    ```

6.  使用 kafka-console-producer 实用程序发送测试消息。

    ```
    # kafka-console-producer --broker-list kafka:9092 \
    --topic test

    ```

7.  键入一条消息并返回。每行文本作为消息发送到主题，并显示在运行 Kafka 控制台消费者的终端中(步骤 4)。

开发环境现在运行一个双节点 Kafka 集群，已经过测试，可以接收和发送消息了。考虑用雅虎开发的 Kafka Manager、 <sup>[18](#Fn18)</sup> 添加一个管理 web 界面！。LinkedIn 开发了陋居 <sup>[19](#Fn19)</sup> 实用程序来监控消费者延迟时间，并通过 HTTP 端点提供统计数据。同样由 LinkedIn 开发的 Cruise-control、 <sup>[20](#Fn20)</sup> ，以及由 Pinterest 开发的 doctor Kafka<sup>[21](#Fn21)</sup>自动执行动态工作负载再平衡和自我修复。Kafka 拥有一个庞大且不断发展的公用事业和第三方支持生态系统。 <sup>[22](#Fn22)</sup>

Kafka 被设计为在一个巨大的规模上工作，作为一个高度可用的、容错的、分布式的、通用的发布/订阅系统，这是将它保持在数据驱动架构的中心的一个很好的理由。Kafka 可以轻松处理来自几乎任何来源的海量数据和指标，尤其是物联网设备。然而，下一节将介绍另一个名为 Mosquitto 的发布/订阅应用，它实现了专为物联网设计的 MQTT 协议。

## 蚊子(MQTT)

MQTT(和 AMQP)等协议专注于轻量级客户端到消息代理通信，适用于广泛且不断增长的消费和工业物联网设备。相比之下，Apache Kafka 是一个更重的事件队列实现，能够处理和持久存储大量数据。虽然这些系统在概念上是相似的，但是在本书中向数据平台添加 MQTT 功能演示了各种协议以及在它们之间交换消息的能力。

想象一个工厂，机器状态通过内部 MQTT 代理进行控制和通信；远程数据平台中的 MQTT 代理充当客户端桥梁，将这些消息转发给 Kafka。机器学习模型对 Kafka 的最后一个小时的滚动数据进行预测分析，并做出调整特定机器状态的决定，并通过 MQTT 将该决定传达回来(见图 [5-5](#Fig5) )。

![../images/483120_1_En_5_Chapter/483120_1_En_5_Fig5_HTML.jpg](../images/483120_1_En_5_Chapter/483120_1_En_5_Fig5_HTML.jpg)

图 5-5

Apache Kafka 和 MQTT 事件队列

Mosquitto <sup>[24](#Fn24)</sup> 是由 Eclipse Foundation<sup>[25](#Fn25)</sup>维护的开源 MQTT 代理，作为*iot.eclipse.org 项目*提供的众多组件之一。

创建目录`cluster-apk8s-dev3/003-data/050-mqtt`。在新的`050-mqtt`目录中，从清单 [5-12](#PC30) 中创建一个名为`10-service.yml`的文件。

```
apiVersion: v1
kind: Service
metadata:
  name: mqtt
  namespace: data
  labels:
    app: mqtt
spec:
  selector:
    app: mqtt
  ports:
    - protocol: "TCP"
      port: 1883
      targetPort: 1883
  type: ClusterIP

Listing 5-12Mosquitto MQTT Service

```

应用 MQTT 服务配置:

```
$ kubectl apply -f 10-service.yml

```

接下来，在清单 [5-13](#PC32) 中名为`20-configmap.yml`的文件中为 Mosquitto 创建一个配置图。这里定义的小配置文件指示服务器以用户`mosquitto`的身份运行，并监听端口 1883。在这个例子中没有加密或认证，因为这个开发环境中的服务器没有直接向公众公开。有关配置选项的详细列表，请参见联机文档。 <sup>[26](#Fn26)</sup>

Caution

不要在没有启用身份验证和加密的情况下将 MQTT 代理暴露给公共互联网。所有客户端都应该被信任。强烈建议使用 VPN 或配置良好的防火墙来保护远程连接。

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: mqtt
  namespace: data
  labels:
    app: mqtt
data:
  mosquitto.conf: |-
    user mosquitto
    port 1883

Listing 5-13Mosquitto configuration ConfigMap

```

应用 Mosquitto 配置配置图:

```
$ kubectl apply -f 20-configmap.yml

```

接下来，在清单 [5-14](#PC34) 中的一个名为`30-deployment.yml`的文件中为 Mosquitto 创建一个部署。这里定义的 Mosquitto 旨在作为单个实例运行。为生产而扩展 Mosquitto 通常涉及为特定客户或客户群提供代理。还有许多其他的开源和商业 MQTT 代理，包括 VerneMQ、 <sup>[27](#Fn27)</sup> 一个高度可用的分布式实现，以及流行的 RabbitMQ <sup>[28](#Fn28)</sup> 支持 AMQP、 <sup>[29](#Fn29)</sup> STOMP、 <sup>[30](#Fn30)</sup> 以及 MQTT，它们都是开源的，用 Erlang 编写。

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mqtt
  namespace: data
  labels:
    app: mqtt
spec:
  replicas: 1 # keep at 1 for
  revisionHistoryLimit: 1
  selector:
    matchLabels:
      app: mqtt
  template:
    metadata:
      labels:
        app: mqtt
    spec:
      volumes:
        - name: mqtt-config-volume
          configMap:
            name: mqtt
      containers:
        - name: mqtt
          image: eclipse-mosquitto:1.6.6
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: mqtt-config-volume
              mountPath: /mosquitto/config
          ports:
            - name: mqtt
              containerPort: 1883

Listing 5-14Mosquitto Deployment

```

应用 Mosquitto 部署:

```
$ kubectl apply -f 30-deployment.yml

```

一个 MQTT 代理安装并运行在 Kubernetes 开发集群中。诸如 MQTT.fx <sup>[31](#Fn31)</sup> 和 mqtt-spy <sup>[32](#Fn32)</sup> 之类的实用程序非常适合测试和调试 MQTT 代理。下面的练习使用新的 Mosquitto Pod 上的`mosquitto_sub`实用程序以及在本地工作站上运行的 MQTT.fx 来测试新的代理。

Testing Mosquitto

1.  在本地工作站上下载并安装 mqtt . FX 1 . 7 . 1 版。参观 [`https://mqttfx.jensd.de/index.php/download`](https://mqttfx.jensd.de/index.php/download) 。

2.  进入 Mosquitto MQTT 代理 Pod 上的 Bash 会话(使用`kubectl get pods -n data`找到它的名称)。

    ```
    $ kubectl exec -it mqtt-6899646f75-g65sf sh -n data

    ```

3.  从 Mosquitto MQTT 窗格中的命令行开始监听关于`dev/test`主题的消息。

    ```
    # mosquitto_sub -t dev/apk8s

    ```

4.  从本地工作站，使用 kubectl 将`mqtt` Kubernetes 服务移植到运行 MQTT.fx 的本地工作站。

    ```
    $ kubectl port-forward svc/mqtt 1883:1883 -n data

    ```

5.  Open MQTT.fx and select “local mosquitto” from the connect drop-down, and then click Connect as shown in Figure [5-6](#Fig6).

    ![../images/483120_1_En_5_Chapter/483120_1_En_5_Fig6_HTML.jpg](../images/483120_1_En_5_Chapter/483120_1_En_5_Fig6_HTML.jpg)

    图 5-6

    在本地工作站上运行的 MQTT.fx 应用程序

6.  提供相同的主题；`mosquitto_sub`是从步骤 3 开始监听，本例中为`dev/apk8s`。在大文本区提供消息，点击**发布**。

注简单的消息传递也可以通过`mosquitto_sub`旁边的`mosquitto_pub`实用程序来完成。

1.  观察从`mosquitto_sub`输出打印的信息。

## 摘要

该平台现在包括两个受欢迎的活动队列，Kafka 和 Mosquitto。Kafka 旨在作为平台的“中枢神经系统”,负责与其他平台组件之间的状态、指标和数据的实时通信。Mosquitto 为流行的物联网通信协议 MQTT 提供支持。清单 [5-15](#PC39) 显示了本章中开发的配置清单的概述。

```
.
└── cluster-apk8s-dev3
    ├── 000-cluster
    └── 003-data
        ├── 000-namespace
  │   ├── 00-namespace.yml
  │   └── 05-certs.yml
        ├── 010-zookeeper
  │   ├── 10-service-headless.yml
  │   ├── 10-service.yml
  │   └── 40-statefulset.yml
  ├── 020-kafka
  │   ├── 10-service-headless.yml
  │   ├── 10-service.yml
  │   ├── 40-statefulset.yml
  │   ├── 45-pdb.yml
  │   └── 99-pod-test-client.yml
        └── 050-mqtt
      ├── 10-service.yml
      ├── 20-configmap.yml
      └── 30-deployment.yml

Listing 5-15Data Pipeline Development Cluster configuration layout

```

接下来的章节将演示索引、分析、可视化、仓储和路由流经本章中配置的队列的数据的方法。

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

[T2`https://mqtt.org`](https://mqtt.org)

  [2](#Fn2_source)

[T2`https://appcodelabs.com/introduction-to-iot-build-an-mqtt-server-using-raspberry-pi`](https://appcodelabs.com/introduction-to-iot-build-an-mqtt-server-using-raspberry-pi)

  [3](#Fn3_source)

[T2`https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication)

  [4](#Fn4_source)

[T2`https://httpd.apache.org/docs/2.4/programs/htpasswd.html`](https://httpd.apache.org/docs/2.4/programs/htpasswd.html)

  [5](#Fn5_source)

[T2`https://zookeeper.apache.org`](https://zookeeper.apache.org)

  [6](#Fn6_source)

[T2`https://hadoop.apache.org/`](https://hadoop.apache.org/)

  [7](#Fn7_source)

[T2`https://hbase.apache.org`](https://hbase.apache.org)

  [8](#Fn8_source)

[T2`https://kubernetes.io/docs/concepts/services-networking/service/#headless-services`](https://kubernetes.io/docs/concepts/services-networking/service/%2523headless-services)

  [9](#Fn9_source)

[T2`https://kafka.apache.org`](https://kafka.apache.org)

  [10](#Fn10_source)

[T2`https://www.confluent.io`](https://www.confluent.io)

  [11](#Fn11_source)

[T2`https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#how-disruption-budgets-work`](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/%2523how-disruption-budgets-work)

  [12](#Fn12_source)

[T2`www.loudera.com/documentation/kafka/latest/topics/kafka_ha.html`](http://www.loudera.com/documentation/kafka/latest/topics/kafka_ha.html)

  [13](#Fn13_source)

[T2`www.cloudera.com/documentation/enterprise/latest/topics/kafka_admin_cli.html`](http://www.cloudera.com/documentation/enterprise/latest/topics/kafka_admin_cli.html)

  [14](#Fn14_source)

[T2`https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330`](https://cwiki.apache.org/confluence/pages/viewpage.action%253FpageId%253D27846330)

  [15](#Fn15_source)

[T2`https://blog.imaginea.com/how-to-rebalance-topics-in-kafka-cluster/`](https://blog.imaginea.com/how-to-rebalance-topics-in-kafka-cluster/)

  [16](#Fn16_source)

[T2`https://kafka.apache.org/documentation/streams/`](https://kafka.apache.org/documentation/streams/)

  [17](#Fn17_source)

[T2`www.gnu.org/software/bash/`](http://www.gnu.org/software/bash/)

  [18](#Fn18_source)

[T2`https://github.com/yahoo/kafka-manager`](https://github.com/yahoo/kafka-manager)

  [19](#Fn19_source)

[T2`https://github.com/linkedin/Burrow`](https://github.com/linkedin/Burrow)

  [20](#Fn20_source)

[T2`https://github.com/linkedin/cruise-control`](https://github.com/linkedin/cruise-control)

  [21](#Fn21_source)

[T2`https://github.com/pinterest/doctorkafka`](https://github.com/pinterest/doctorkafka)

  [22](#Fn22_source)

[T2`https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem`](https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem)

  [23](#Fn23_source)

[T2`https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines`](https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines)

  [24](#Fn24_source)

[T2`https://mosquitto.org/`](https://mosquitto.org/)

  [25](#Fn25_source)

[T2`https://eclipse.org/`](https://eclipse.org/)

  [26](#Fn26_source)

[T2`https://mosquitto.org/man/mosquitto-conf-5.html`](https://mosquitto.org/man/mosquitto-conf-5.html)

  [27](#Fn27_source)

[T2`https://vernemq.com`](https://vernemq.com)

  [28](#Fn28_source)

[T2`www.rabbitmq.com`](http://www.rabbitmq.com)

  [29](#Fn29_source)

[T2`www.amqp.org`](http://www.amqp.org)

  [30](#Fn30_source)

[T2`https://stomp.github.io`](https://stomp.github.io)

  [31](#Fn31_source)

[T2`https://mqttfx.jensd.de`](https://mqttfx.jensd.de)

  [32](#Fn32_source)

[T2`http://kamilfb.github.io/mqtt-spy/`](http://kamilfb.github.io/mqtt-spy/)

 </aside>