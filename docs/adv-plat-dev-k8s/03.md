# 三、开发环境

开发一个支持区块链、机器学习和物联网数据管理等组件的平台至少需要 Kubernetes 集群提供的三个基本功能领域:入口、TLS 和持久数据存储。本章建立了一个满足平台开发最低要求的小规模开发集群。以下群集可以根据需求增加资源，轻松扩展节点数量和每个节点的大小。

许多 Kubernetes 教程使用 Minikube 的本地副本引导您完成示例。Minikube 只需要在本地工作站上消耗一点 CPU 和内存。然而，一个健康的开发环境应该反映生产环境的缩小版本，尽管 Minikube 对于实验和一些本地开发来说很棒，但是它在反映生产环境的挑战方面能力有限。开发集群可能需要在远程开发人员之间共享，或者可供外部资源用于收集数据、指标和事件；这对于基于本地工作站的 Kubernetes 安装来说是一个挑战。如果您真的想在 Kubernetes 上测试您的平台的云原生和厂商中立方面，那么在一个厂商上有一个开发环境，在另一个厂商上有一个测试和生产环境可能是有益的。开发在基于通用虚拟机(VM)的集群上运行的应用，然后发布到反映您的生产集群的测试环境中，可以确保您在供应商之间的可移植性，只需要最小的配置差异，并且没有重大的架构更改。

## 定制开发 Kubernetes 集群

今天有几个生产就绪的，交钥匙的 Kubernetes 产品，如谷歌 Kubernetes 引擎(GKE)，亚马逊 Kubernetes 弹性容器服务(亚马逊 EKS)，Azure Kubernetes 服务(AKS)，和 IBM 云 Kubernetes 服务。对于商业平台的生产部署来说，这些选项可能是一个安全的选择，这些提供商提供了深度集成到他们更广泛的服务套件中的机会。为了保持平台的可移植性和厂商中立性，一个定制的、自我管理的 Kubernetes 集群可以为开发提供一个经济有效的中立环境。将开发分离到一个定制的 Kubernetes 集群中对平台开发人员来说有很多好处，迫使他们更深入地理解底层基础设施，同时又不会失去抽象的好处。定制 Kubernetes 集群上的平台开发确保了可移植性，并通过与多个供应商的兼容性提供了冗余机会。

在前一章中，GitLab 是在运行 k3s 的单个 Vultr 实例上设置的。Kubernetes 的开发是为了在数百甚至数千个节点上调度和管理容器工作负载，而 k3s 仅在单个节点上利用其特性。设置一个定制的开发 Kubernetes 集群涉及多个服务器，一个(或多个)主节点，以及一个或多个工作节点。

下一节将更深入地研究 Kubernetes 节点，并介绍在 Digital Ocean 上设置自定义 Kubernetes 集群的过程。像 Vultr 一样，Digital Ocean 提供了他们称之为 Droplets 的廉价计算实例。设置定制 Kubernetes 的说明可以很容易地转换到任何提供通用计算实例或虚拟或裸机的替代服务中。对于为关键的、备受瞩目的生产平台运行定制 Kubernetes 集群的可管理性和成本效益，一直存在争议。然而，当您最终拥有一个可工作的开发环境，并且很可能拥有一个备份或冗余版本以用于生产时，通过安装、配置和维护定制集群所获得的知识和技能会带来巨大的好处。

## 节点

Kubernetes 集群由节点组成。一个或多个主节点，从 Kubernetes 1.14 开始，总共不超过 5000 个节点。如果您在 1U 服务器上运行所有 5000 个节点，您将占用一个典型数据中心的 100 多个机架，这还不包括路由器、电源管理和所有其他所需的基础设施。Kubernetes 不仅从平台中抽象出单个服务器，而且一些联合的 Kubernetes 集群也可以很好地抽象出一个小型数据中心。

Kubernetes 集群的初始规模是一个生产问题，最好通过运行已开发应用平台的测试或开发集群的性能以及预计利用率来计算。扩展 Kubernetes 就像添加计算资源并将其作为节点加入一样简单。需要注意的是，随着集群的增长，任何主节点的大小都可能需要调整。然而，对于本书中的小型开发集群设置，一个小型的单个主节点就足够了。

Note

Kubernetes 集群中的单个主节点不一定是整个集群的单点故障。Kubernetes 主节点的故障是一个紧急问题；然而，网络和容器工作负载(不依赖于 Kubernetes API)继续在没有 Kubernetes 控制面板的情况下运行。 <sup>[1](#Fn1)</sup>

本书中的示例利用了本节中设置的一个小型开发集群，从一个主节点和两个工作节点开始(见图 [3-1](#Fig1) )。前一章使用 k3s 操作一个缩小的主节点来运行工作负载。但是，在多节点集群中，主节点应该只负责管理集群。

![../images/483120_1_En_3_Chapter/483120_1_En_3_Fig1_HTML.jpg](../images/483120_1_En_3_Chapter/483120_1_En_3_Fig1_HTML.jpg)

图 3-1

Kubernetes Nodes

### 服务器设置

本节将在 Digital Ocean 上设置三台 Ubuntu 18.04 x64 服务器(Droplets)，最低硬件规格为 2 个 CPU 和 2 GB RAM。

Note

当未来需求可能需要更多 CPU 或 RAM 用于主节点或工作节点时，数字海洋允许液滴按比例增加。

建立帐户并登录数字海洋后，点击*创建*按钮，并从出现的菜单中选择*水滴*。通过选择*分布*下的 *Ubuntu 18.04 x64* 为水滴选择映像(参见图 [3-2](#Fig2) )。

![../images/483120_1_En_3_Chapter/483120_1_En_3_Fig2_HTML.jpg](../images/483120_1_En_3_Chapter/483120_1_En_3_Fig2_HTML.jpg)

图 3-2

产生水滴

接下来，在 Standard starter plans 下，找到并选择带有 2 GB/2 个 CPU 的选项，如图 [3-3](#Fig3) 所示。在撰写本文时，带有这些选项的 Droplet 的成本是每月 15 美元(0.022 小时)。在这个价位上，这里组装的开发集群每月花费 45 美元(每小时 0.066 美元)。

![../images/483120_1_En_3_Chapter/483120_1_En_3_Fig3_HTML.png](../images/483120_1_En_3_Chapter/483120_1_En_3_Fig3_HTML.png)

图 3-3

选择一个计划

接下来，选择一个离您或您的开发团队最近的数据中心区域。数字海洋为一个地区提供了一个或多个选择。图 [3-4](#Fig4) 显示旧金山/2 被选中。

![../images/483120_1_En_3_Chapter/483120_1_En_3_Fig4_HTML.jpg](../images/483120_1_En_3_Chapter/483120_1_En_3_Fig4_HTML.jpg)

图 3-4

选择一个地区

接下来，选择附加选项专用网络和监控(参见图 [3-5](#Fig5) )。Kubernetes API 后来使用私有网络在节点之间进行私有通信。此外，对于 Digital Ocean 和大多数提供商，通过专用网络进行的数据传输不受数据传输限制。

![../images/483120_1_En_3_Chapter/483120_1_En_3_Fig5_HTML.jpg](../images/483120_1_En_3_Chapter/483120_1_En_3_Fig5_HTML.jpg)

图 3-5

专用网络和监控

最后，为这个开发集群选择三个 Droplets，并提供描述性的主机名，如图 [3-6](#Fig6) 所示。这本书使用主机名`dosf2-n01.apk8s.dev`，主机名`dosf2-n02.apk8s.dev`，主机名`dosf2-n03.apk8s.dev`。

![../images/483120_1_En_3_Chapter/483120_1_En_3_Fig6_HTML.jpg](../images/483120_1_En_3_Chapter/483120_1_En_3_Fig6_HTML.jpg)

图 3-6

水滴数量和主机名

一旦新的 Droplet(服务器)启动并运行，请注意分配给每个 Droplet 的公共和私有 IP 地址。图 [3-7](#Fig7) 显示分配给`dosf2-n01.apk8s.dev`的公有 IP 地址为`138.68.18.212`，私有 IP 地址为`10.138.28.155`。

![../images/483120_1_En_3_Chapter/483120_1_En_3_Fig7_HTML.jpg](../images/483120_1_En_3_Chapter/483120_1_En_3_Fig7_HTML.jpg)

图 3-7

水滴细节

### 准备节点

使用终端，ssh 作为用户根进入每个新的 Droplet(服务器)。如果您在设置 Droplet 时没有添加 SSH 密钥，Digital Ocean 会通过电子邮件向您发送一个生成的一次性 root 用户密码。按照以下说明更新软件包，并在每台服务器上安装 Docker 和 Kubernetes 实用程序:

```
$ ssh root@PUBLIC_IP

```

#### 安装依赖项

更新和升级所有软件包，以确保服务器拥有最新的软件包，提供任何必要的安全和性能更新:

```
$ apt update && apt upgrade -y

```

安装以下软件包:`apt-transport-https`允许 apt 软件包管理器从 HTTPS 端点获取软件包。`ca-certificates`安装来自证书颁发机构的 SSL/TLS 证书以及更新程序。`curl`是一个命令行 HTTP 客户端，对于从命令行与基于 HTTP 的端点进行交互至关重要。`gnupg-agent`为 GPG 签名和密钥管理提供系统守护程序。`software-properties-common`支持管理后来为 **WireGuard** 、 **Docker** 和 **Kubernetes** 添加的独立软件供应商库。

用一个命令安装依赖项:

```
$ apt install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common

```

#### 安装 WireGuard VPN

所有云提供商都提供专用网络，即分配 IP 地址的能力，该能力只能在提供商的内部网络中访问，并且通常只能在同一区域内访问。虽然这些私有 IP 地址不能通过公共互联网访问，但根据提供商的不同，如果在同一地区运行实例，其他客户可以访问它们。加密集群中节点之间的所有流量始终是一种好的做法。

这个练习是可选的。在大多数情况下，没有必要使用 VPN 来加密专用网络上的流量。在过去，一些云提供商与多个客户端共享私有网络，虽然不向公众公开， <sup>[2](#Fn2)</sup> 网络流量对同一网络上的其他客户端来说是潜在可见的。尽管大多数提供商已经放弃了共享专用网络，但本练习演示了如何使用 VPN 在不可信网络的高安全性环境中保护所有网络流量。

在 VPN 上运行 Kubernetes 集群可以加密节点之间的网络流量；但是，在群集内，默认情况下不会加密网络流量。Istio <sup>[4](#Fn4)</sup> 和 Linkerd <sup>[5](#Fn5)</sup> 等服务网格为 pod 之间的相互 TLS 通信提供了选项。

WireGuard <sup>[6](#Fn6)</sup> 是一种快速、安全的 VPN，易于安装、配置和路由所有内部集群流量。接下来的步骤是为每台服务器生成公钥和私钥，添加 WireGuard 配置文件，启动服务，并创建一个覆盖网络来通过 Kubernetes 流量。

添加 WireGuard 存储库:

```
$ add-apt-repository -y ppa:wireguard/wireguard

```

现在已经添加了 WireGuard 存储库，请更新程序包列表:

```
$ apt update

```

安装电线保护装置:

```
$ apt install -y wireguard

```

为每台服务器生成私钥和公钥。下面是一个用于生成三个密钥对的小 shell 命令。生成密钥后，请安全地存储它们，以便在每台服务器上配置 WireGuard 时使用。私钥(`priv`)用于每个服务器上的 VPN 接口，公钥(`pub`)用于与对等方通信。

![../images/483120_1_En_3_Chapter/483120_1_En_3_Fig8_HTML.jpg](../images/483120_1_En_3_Chapter/483120_1_En_3_Fig8_HTML.jpg)

图 3-8

VPN 接口和对等体

```
$ for i in {1..3}; do prvk=$(wg genkey); \
echo "$i - priv: $prvk pub: $(echo $prvk | wg pubkey)"; done

```

接下来，为每台服务器配置 WireGuard。第一台服务器的以下示例配置将设置一个名为 **wg0** 的新网络接口，IP 地址为 **10.0.1.1** 。根据每台服务器调整配置；第二个服务器的接口 IP 应该是第三个的 **10.0.1.2** 和 **10.0.1.3** (见图 [3-8](#Fig8) )。使用先前生成的公钥或私钥。

```
$ cat <<EOF >/etc/wireguard/wg0.conf
[Interface]
Address = 10.0.1.1
PrivateKey = SERVER_1_PRIVATE_KEY
ListenPort = 51820

[Peer]
PublicKey = SERVER_2_PUBLIC_KEY
AllowedIps = 10.0.1.2/32
Endpoint = SERVER_2_PRIVATE_IP:51820

[Peer]
PublicKey = SERVER_3_PUBLIC_KEY
AllowedIps = 10.0.1.3/32
Endpoint = SERVER_3_PRIVATE_IP:51820
EOF

```

接下来，确保启用了 IP 转发。如果运行`sysctl net.ipv4.ip_forward`返回`0`，那么需要运行以下命令:

```
$ echo "net.ipv4.ip_forward=1" >> /etc/sysctl.conf
$ sysctl -p

```

添加配置文件后，在每台服务器上启动 WireGuard VPN:

```
$ systemctl start wg-quick@wg0
$ systemctl enable wg-quick@wg0

```

#### 安装 Docker

Kubernetes 支持几种基于容器运行时接口 <sup>[7](#Fn7)</sup> (CRI)的运行时，包括 rkt、 <sup>[8](#Fn8)</sup> frakti、 <sup>[9](#Fn9)</sup> cri-o、 <sup>[10](#Fn10)</sup> 和 cri-containerd。 <sup>[11](#Fn11)</sup> Docker 是默认的 CRI，也是一个成熟的行业标准，尽管每种替代方案都有值得探索的好处。以下命令介绍了安装 Docker 的过程。

添加Docker仓库 GPG 键:

```
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

```

添加 Docker 存储库:

```
$ add-apt-repository -y \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) stable"

```

现在已经添加了 Docker 存储库，请更新包列表:

```
$ apt update

```

安装最新版本的 Docker CE(社区版)守护进程、Docker 命令行界面、containerd: <sup>[12](#Fn12)</sup>

```
$ apt install -y docker-ce docker-ce-cli containerd.io

```

Ubuntu 操作系统使用 Systemd <sup>[13](#Fn13)</sup> 跟踪使用 Linux cgroups、<sup>T5】14</sup>的进程，默认情况下 Docker 使用 cgroupfs。拥有单独的 cgroup 管理器会在管理负载资源时导致不稳定。通过提供一个配置文件来配置 Docker 使用 Systemd。 <sup>[15](#Fn15)</sup>

```
$ cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

```

为 Docker 创建一个 Systemd 放入目录:

```
$ mkdir -p /etc/systemd/system/docker.service.d

```

启用 Docker 服务:

```
$ systemctl enable docker.service

```

重启 Docker:

```
$ systemctl daemon-reload && systemctl restart docker

```

#### 安装库公用程式

除了 Docker，每个节点还会需要`kubelet` <sup>[16](#Fn16)</sup> 和`kubeadm`。<sup>[17](#Fn17)</sup>`kubectl`<sup>[18](#Fn18)</sup>是可选的，但在需要直接从节点调试时非常有用。

为 apt 添加谷歌 GPG 密钥:

```
$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg \
| apt-key add -

```

添加库 apt 存储库:

```
$ cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF

```

现在已经添加了 Kubernetes 存储库，请更新包列表:

```
$ apt update

```

安装套装软体:

```
$ apt install -y kubelet kubeadm kubectl

```

将已安装的软件包锁定到其当前版本:

```
$ apt-mark hold kubelet kubeadm kubectl

```

### 安装主节点

是官方的 Kubernetes 安装工具，负责配置主节点的所有工作。如果您希望更深入地了解如何安装和配置每个组件，请学习 Kelsey Hightower 的“Kubernetes The Hard Way”<sup>[19](#Fn19)</sup>。否则，在节点 1 上运行`kubeadm`安装程序，配置主节点的私有 VPN IP 地址(`--apiserver-advertise-address`)进行通告，并添加一个公共地址(`--apiserver-cert-extra-sans`)作为额外地址包含在生成的 TLS 证书中，允许外部访问公共接口上的 API。域`api.cluster.dev1.apk8s.dev`被分配了一个 DNS `A`记录，该记录具有 DNS 部分后面的主节点的公共 IP 地址。

```
$ kubeadm init \
--apiserver-advertise-address=10.0.1.1 \
--apiserver-cert-extra-sans=api.cluster.dev1.apk8s.dev

```

成功安装后，将出现消息“`Your Kubernetes control-plane has initialized successfully!`”以及配置 kubectl 的说明。

```
$ mkdir -p $HOME/.kube
$ cp /etc/kubernetes/admin.conf $HOME/.kube/config

```

在安装过程之后，提供了加入工作节点的附加说明，类似于下面的命令(keys redacted)。复制并保存所提供的命令，以便在下一节“加入工作节点”中使用

```
kubeadm join 10.0.1.1:6443 --token REDACTED --discovery-token-ca-cert-hash REDACTED

```

接下来，pod 网络对于集群上的 pod 之间的通信是必要的。目前有十几种选择，每一种都有广泛的特性，值得在设置网络密集型生产集群时探索。Weave Net <sup>[20](#Fn20)</sup> 是只需要简单联网和网络策略的开发或生产集群的绝佳选择。Weave 可以在一个命令中安装，不需要外部数据库或其他依赖项。

使用先前安装在主节点(节点 1)上的`kubectl`实用程序安装 Weave:

```
$ kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')&env.IPALLOC_RANGE=10.32.0.0/16"

```

**可选**—安装编织范围 <sup>[21](#Fn21)</sup> 用于网络可视化和监控；

```
$ kubectl apply -f https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\n')

```

通过 WireGuard VPN 路由所有 Pod 网络(Weave) `10.96.0.0/16`流量。在集群中的每台服务器上执行此操作，第一个节点上是 10.0.1.1，第二个节点上是 10.0.1.2，第三个节点上是 10.0.1.3。

```
$ ip route add 10.96.0.0/16 dev wg0 src 10.0.1.1

```

在每台服务器上使用以下文件保留此路由，将 10.0.1.1 替换为与服务器对应的 VPN 接口:

```
$ cat <<EOF >/etc/systemd/system/overlay-route.service
[Unit]
Description=Overlay network route for Wireguard
After=wg-quick@wg0.service

[Service]
Type=oneshot
User=root
ExecStart=/sbin/ip route add 10.96.0.0/16 dev wg0 src 10.0.1.1

[Install]
WantedBy=multi-user.target
EOF

```

在每台服务器上应用新配置:

```
$ systemctl enable overlay-route.service

```

### 加入工作节点

第二台和第三台服务器是 Kubernetes 工作节点。登录到每台服务器，运行主节点安装后提供的 join 命令。

```
$ kubeadm join 10.0.1.1:6443 --token REDACTED --discovery-token-ca-cert-hash REDACTED

```

添加额外的节点只需要服务器满足 2 个 CPU 和 2 GB RAM 的最低硬件规格，以及本节前面安装的 WireGuard、Docker 和 Kubernetes 应用。必须向 WireGuard VPN 网络添加额外的节点，并通过其接口路由 Pod 网络的 IP 子网`10.96.0.0/16`。

### 域名服务器(Domain Name Server)

这个定制的 Kubernetes 开发集群并不支持复杂的(通常也是昂贵的)负载均衡器；相反，它使用基本的 DNS 循环技术在集群中分发请求。

通过为同一个主机名添加多个 DNS `A`记录来实现 DNS 循环。开发集群有两条`A`记录，每条记录都指向一个使用通配符主机名`*.dev1`的工作者节点的公共 IP 地址(参见图 [3-9](#Fig9) )。通配符允许任意数量的`*.dev1.apk8s.dev`子域解析为集群上的节点。有关添加`A`记录的说明，请咨询您的域名/DNS 提供商。

![../images/483120_1_En_3_Chapter/483120_1_En_3_Fig9_HTML.jpg](../images/483120_1_En_3_Chapter/483120_1_En_3_Fig9_HTML.jpg)

图 3-9

开发群集 DNS

接下来，为指向主节点(节点 1)的公共 IP 的 Kubernetes API 添加一个额外的`A`记录`api.cluster.dev1.apk8s.dev`。

### 远程存取

“k3s + GitLab”部分介绍了一种通过复制和粘贴所生成配置的集群、用户和上下文部分来配置 kubectl 进行远程访问的方法。以下说明给出了配置`kubectl`的另一种方法。

安装主节点后，`kubeadm`提供了将生成的`kubectl`配置复制到`$HOME/.kube/config`的指令。以 root 用户身份登录到主节点，该路径将解析为`/root/.kube/config`。使用以下命令将`kubeadm`生成的配置安全复制到位置工作站:

```
$ scp root@api.cluster.dev1.apk8s.dev:/root/.kube/config ~/.kube/apk8s-dev1

```

打开文件，用**API . cluster . de v1 . apk8s . dev**替换 IP 地址 10.0.1.1。或者，将上下文`kubernetes-admin@kubernetes`更改为更具描述性的内容。以下命令使用命令`sed`内联编辑文件:

```
$ sed -i .bak \
 's/10.0.1.1/api.cluster.dev1.apk8s.dev/g' \
 ~/.kube/apk8s-dev1

$ sed -i .bak 's/kubernetes-admin@kubernetes/apk8s-dev1/g' ~/.kube/apk8s-dev1

```

在同一终端中，将环境变量`KUBECONFIG`设置为`~/.kube/apk8s-dev1`:

```
$ export KUBECONFIG=~/.kube/apk8s-dev1

```

导出环境变量使其仅可用于当前终端会话。虽然这不是最有效的切换`kubectl`上下文的方法，但是在处理大量集群时，这是一个很好的选择，可以防止默认的`~/.kube/config`变得过于混乱。测试新的配置和上下文:

```
$ kubectl get nodes

NAME        STATUS   ROLES    AGE   VERSION
dosf2-n01   Ready    master   15m   v1.15.0
dosf2-n02   Ready    <none>   11m   v1.15.0
dosf2-n03   Ready    <none>   11m   v1.15.0

```

新的开发集群已经准备好配置额外的组件，以满足本书剩余部分的依赖关系。TLS 证书、持久存储和入口是后来添加的物联网、机器学习和区块链功能的基本要求。下一节“配置”将介绍用于满足这些需求的 YAML 清单的组织和应用，并使用新的 GitLab 实例来保持它们的版本和文档。

## 配置

Kubernetes 支持声明式配置；这意味着我们告诉 Kubernetes 我们想要它是什么，而不是发布一个实现某个状态所需的命令列表。这本书用 YAML 来描述库伯涅茨的理想状态；这个状态是构成操作平台的部署、状态集、配置映射、服务和其他的集合。将状态配置作为一组静态 YAML 文件保存在 Git 存储库中，不仅可以反映平台的当前状态，还可以提供随时间变化的有价值的提交日志。

企业软件平台的配置管理技术和实用程序有一个不断发展的生态系统。这本书试图通过坚持把普通 YAML 作为配置的标准手段来保持事情的简单。这本书展示了一种组织 Kubernetes 配置清单的简单方法，描述了添加到平台的各种应用。随着平台和配置需求的增长，可能会分层加入更复杂的配置工具，如 Jsonnet、 <sup>[22](#Fn22)</sup> Kapitan、 <sup>[23](#Fn23)</sup> 或 Kustomize。<sup>[24](#Fn24)</sup>GitOps<sup>[25](#Fn25)</sup>等新概念旨在直接用 Git 完全管理活动配置状态。Kubernetes 提供了无限的方法来创造性地管理配置。然而，从按环境、名称空间、组件和对象类型分类的组织良好的目录结构开始，可以提供一个合理的基础。

### 贮藏室ˌ仓库

既然开发集群已经可以运行，并且`kubectl`可以从本地工作站访问它，那么一系列包含名称空间、卷、入口和监控的附加配置的 YAML 文件将被应用到集群。开发集群会产生越来越多的 YAML 配置文件。这些配置文件(或清单)不仅定义了 Kubernetes 所需的状态，它们还为开发人员和系统管理员提供了详细而准确的文档，尤其是在伴随有补充性(`README.md` ) markdown 风格的文档时。

本书中的配置清单和文档组织在前一章安装的自托管 GitLab 存储库中。一个名为 **apk8s** 的 GitLab 小组持有项目 **k8s** 。组织多个基于 Kubernetes 的项目的常见方法是将它们嵌套在 GitLab 组下，每个项目都有一个专用的 k8s 存储库。两个独立平台项目的一个例子可以包括保存该平台的集群配置的 [`https://gitlab.apk8s.dev/apk8s/k8s`](https://gitlab.apk8s.dev/apk8s/k8s) ，以及保存“另一个平台”的 Kubernetes 配置的 [`https://gitlab.apk8s.dev/another-platform/k8s`](https://gitlab.apk8s.dev/another-platform/k8s) 。

Note

GitHub 提供了特性 **Organizations** ，具有与 GitLab groups 相似的功能。

一旦在 GitLab 中设置了组 **apk8s** 和项目 **k8s** ，在本地工作站上创建一个与组(apk8s)匹配的目录，并将新项目克隆到其中。

```
$ mkdir -p ~/workspace/apk8s
$ cd ~/workspace/apk8s
$ git clone ssh://git@gitlab.apk8s.dev:32222/apk8s/k8s.git
$ cd k8s

```

Note

如果在同一个集群上开发多个项目，建议将集群范围的配置存储为一个单独的组，如 **devops/k8s** ，并将每个项目提交给 it 部门，以了解集群范围的要求和文档。

创建一个目录来保存集群范围的配置。这些配置特定于集群(`cluster-apk8s-dev1`)。

```
$ mkdir -p cluster-apk8s-dev1/000-cluster

```

### 进入

在这个平台中，大多数 Kubernetes 服务 <sup>[26](#Fn26)</sup> 被分配了一个集群 IP，并且只能从集群内部访问。Kubernetes Ingress 允许外部 HTTP 和 HTTPS 连接到集群内的服务。Kubernetes 入口资源定义了一个必须由入口控制器支持的配置。

Kubernetes 没有提供默认的入口控制器，让管理员和系统架构师选择最适合平台需求的入口控制器。有大量有能力的入口控制器可用。在前一章中，Traefik 与 k3s Kubernetes 发行版打包在一起，并通过服务 GitLab 安装演示了它的使用。本章使用入口 Nginx <sup>[27](#Fn27)</sup> 控制器配置新的 *dev1* 集群，展示各种入口选项。

本章根据从 k8s 存储库克隆的目录`~/workspace/apk8s/k8s`和上一节设置的 GitLab 组`apk8s`构建配置。从这个位置创建一个目录来存储入口 Nginx 配置清单:

```
$ mkdir -p cluster-apk8s-dev1/000-cluster/00-ingress-nginx
$ cd cluster-apk8s-dev1/000-cluster/00-ingress-nginx

```

从清单 [3-1](#PC40) 中为入口 Nginx 名称空间创建一个名为`00-namespace.yml`的配置文件。

```
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx

Listing 3-1Ingress Nginx Namespace

```

应用入口 Nginx 名称空间配置:

```
$ kubectl apply -f 00-namespace.yml

```

从清单 [3-2](#PC42) 中为名为`05-serviceaccount.yml`的 Ingress Nginx 创建一个描述基于角色的访问控制(RBAC) Kubernetes 服务帐户的配置文件。

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx

Listing 3-2Ingress Nginx RBAC Service Account

```

应用入口 Nginx 服务帐户配置:

```
$ kubectl apply -f 05-serviceaccount.yml

```

创建一个配置文件，描述清单 [3-3](#PC44) 中名为`06-clusterrole.yml`的入口 Nginx 的 RBAC 集群角色。

```
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
rules:
  - apiGroups: [""]
    resources: ["configmaps",
                "endpoints",
                "nodes",
                "pods",
                "secrets"]
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get"]
  - apiGroups: [""]
    resources: ["services"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["ingresses"]
    verbs: ["get","list","watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "patch"]
  - apiGroups: ["extensions"]
    resources: ["ingresses/status"]
    verbs: ["update"]

Listing 3-3RBAC Ingress Nginx Cluster Role

```

应用入口 Nginx 群集角色配置:

```
$ kubectl apply -f 06-clusterrole.yml

```

创建一个配置文件，描述清单 [3-4](#PC46) 中名为`07-role.yml`的入口 Nginx 的 RBAC 角色。

```
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
rules:
  - apiGroups: [""]
    resources: ["configmaps",
                "pods",
                "secrets",
                "namespaces"]
    verbs: ["get"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames:
      - "ingress-controller-leader-nginx"
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get"]

Listing 3-4RBAC Ingress Nginx Role

```

应用入口 Nginx 角色配置:

```
$ kubectl apply -f 07-role.yml

```

从清单 [3-5](#PC48) 中创建一个配置文件，描述名为`08-rolebinding.yml`的入口 Nginx 的 RBAC 角色绑定。

```
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

Listing 3-5RBAC Ingress Nginx Role Binding

```

应用入口 Nginx 角色绑定配置:

```
$ kubectl apply -f 08-rolebinding.yml

```

创建一个配置文件，描述清单 [3-6](#PC50) 中名为`09-clusterrolebinding.yml`的入口 Nginx 的 RBAC 集群角色绑定。

```
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

Listing 3-6RBAC Ingress Nginx Cluster Role Binding

```

应用入口 Nginx 群集角色绑定配置:

```
$ kubectl apply -f 09-clusterrolebinding.yml

```

创建一个配置文件，描述清单 [3-7](#PC52) 中名为`10-services.yml`的入口 Nginx 的两个 Kubernetes 服务。

```
apiVersion: v1
kind: Service
metadata:
  name: default-http-backend
  namespace: ingress-nginx
  labels:
    app: default-http-backend
spec:
  ports:
    - port: 80
      targetPort: 8080
  selector:
    app: default-http-backend
---
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 80
      targetPort: 80
      protocol: TCP
    - name: https
      port: 443
      targetPort: 443
      protocol: TCP
  selector:
    app: ingress-nginx

Listing 3-7Ingress Nginx Services

```

应用入口 Nginx 服务配置:

```
$ kubectl apply -f 10-services.yml

```

创建一个配置文件，描述清单 [3-8](#PC54) 中名为`20-configmaps.yml`的入口 Nginx 的三个空 Kubernetes 配置映射。

```
kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app: ingress-nginx
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx

Listing 3-8Ingress Nginx Services ConfigMaps

```

应用入口 Nginx 配置映射配置:

```
$ kubectl apply -f 20-configmaps.yml

```

创建一个配置文件，描述清单 [3-9](#PC56) 中名为`30-deployment.yml`的默认 HTTP 后端服务器的 Kubernetes 部署。

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: default-http-backend
  labels:
    app: default-http-backend
  namespace: ingress-nginx
spec:
  replicas: 2
  revisionHistoryLimit: 1
  selector:
    matchLabels:
      app: default-http-backend
  template:
    metadata:
      labels:
        app: default-http-backend
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - default-http-backend
                topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 60
      containers:
        - name: default-http-backend
          image: gcr.io/google_containers/defaultbackend:1.4
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            timeoutSeconds: 5
          ports:
            - containerPort: 8080
          resources:
            limits:
              cpu: 10m
              memory: 20Mi
            requests:
              cpu: 10m
              memory: 20Mi

Listing 3-9Ingress Nginx Deployment. Default back ends

```

应用入口 Nginx 默认 HTTP 服务器后端配置部署:

```
$ kubectl apply -f 30-deployment.yml

```

最后，创建一个配置文件，描述清单 [3-10](#PC58) 中名为`40-daemonset.yml`的入口 Nginx 控制器的 Kubernetes DaemonSet。DaemonSet 指示 Kubernetes 确保每个节点上运行一个入口 Nginx 控制器。入口 Nginx 控制器监听 TCP 端口 80 (HTTP)和 443 (HTTPS)。在本章的前面，DNS 部分为`*.dev1.apk8s.dev`配置了两个`A`记录，指向每个 worker 节点。

Note

本章中定义的定制 Kubernetes 开发集群不使用负载均衡器 <sup>[28](#Fn28)</sup> ，而是依靠 DNS 记录来指向每个工作节点。使用 DaemonSet 可以确保每个工作节点都有一个 Nginx 入口 Pod 侦听端口 80 (HTTP)和 443 (HTTPS)。

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
spec:
  revisionHistoryLimit: 1
  selector:
    matchLabels:
      app: ingress-nginx
  template:
    metadata:
      labels:
        app: ingress-nginx
      annotations:
        prometheus.io/port: '10254'
        prometheus.io/scrape: 'true'
    spec:
      serviceAccountName: nginx-ingress-serviceaccount
      hostNetwork: true
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.14.0
          args:
            - /nginx-ingress-controller
            - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
            - --configmap=$(POD_NAMESPACE)/nginx-configuration

            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --annotations-prefix=nginx.ingress.kubernetes.io
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
              hostPort: 80
            - name: https
              containerPort: 443
              hostPort: 443
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          securityContext:
            runAsNonRoot: false

Listing 3-10Ingress Nginx DaemonSet

```

应用入口 Nginx 默认 HTTP 服务器后端配置部署:

```
$ kubectl apply -f 40-daemonset.yml

```

Ingress Nginx 现在已经配置好并运行在新的开发集群上，可以通过每个节点上的端口 80 和 443 接受 web 流量。本练习构建了每个配置清单，并将其存储在目录`~/workspace/apk8s/k8s/cluster-apk8s-dev1/000-cluster/00-ingress-nginx`中。这组入口 Nginx 配置清单准确地表示了集群的当前或期望状态，此外还向其他人提供了文档，并且能够在以后或在另一个集群上再现这种状态。

最后，开发集群需要能够生成 TLS 证书，Nginx 可以使用这些证书为加密的 HTTPS(端口 443)流量提供服务。下一节“使用证书管理器的 TLS/HTTPS”将介绍如何设置证书管理器，以便使用 Let's Encrypt 自动生成免费的 TLS 证书。

### 带证书管理器的 TLS/HTTPS

Cert Manager 用于“自动管理和发布来自各种发布源的 TLS 证书” <sup>[29](#Fn29)</sup> 这本书利用 Let's Encrypt <sup>[30](#Fn30)</sup> 进行安全、免费的 TLS 证书颁发，后来配置了一个名为 ClusterIssuer 的证书管理器自定义资源。为证书管理器配置清单创建目录`~/workspace/apk8s/k8s/cluster-apk8s-dev1/000-cluster/10-cert-manager`。

用清单 [3-11](#PC60) 的内容创建文件`00-namespace.yml`。

```
apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager
  labels:
    certmanager.k8s.io/disable-validation: "true"

Listing 3-11Cert Manager Namespace

```

为证书管理器应用新的命名空间:

```
$ kubectl apply -f 00-namespace.yml

```

获取证书管理器的自定义资源定义(CRD)并将它们保存到文件`02-crd.yml`:

```
$ curl -L https://github.com/jetstack/cert-manager/releases/download/v0.8.0/cert-manager.yaml >02-crd.yml

```

应用证书管理器 CRDs:

```
$ kubectl apply -f 02-crd.yml

```

已安装证书管理器。确保所有支持证书管理器的 pod 都已达到运行状态(`kubectl get pods -n cert-manager`)。

证书管理器定义了称为 ClusterIssuer 和 Certificate 等新的自定义资源。证书描述了所需的 TLS 证书以及使用发行者从权威机构(如 Let's Encrypt)检索 TLS 证书。本书对所有证书都使用了 ClusterIssuer，但是您可以在 Cert Manager 的官方文档中阅读更多关于 Issuer 和 ClusterIssuer 的内容。

用清单 [3-12](#PC64) 的内容创建文件`03-clusterissuer.yml`。

```
apiVersion: certmanager.k8s.io/v1alpha1
kind: ClusterIssuer
metadata:
  name: letsencrypt-production
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: YOUR_EMAIL_ADDRESS
    privateKeySecretRef:
      name: letsencrypt-production
    http01: {}

Listing 3-12Cert Manager Cluster Issuer

```

应用证书管理器群集颁发者:

```
$ kubectl apply -f 03-clusterissuer.yml

```

群集中的任何命名空间都可以使用新的 ClusterIssuer。使用命令:`kubectl get clusterissuers`获取集群发布者列表。后面的小节在定义由 Ingress Nginx 用于 TLS 通信的证书(存储为 Kubernetes Secrets)时使用这个新的 ClusterIssuer `letsencrypt-production`。

新的开发集群现在能够接受入站 HTTP 和 HTTPS，并自动生成 TLS 证书。合适的开发环境的最后一个基本要求是持久存储，这将在下一节讨论。

### 具有 Rook Ceph 的持久卷

对于一些 Kubernetes 部署来说，持久存储是一个基本的、通常也是棘手的需求。Kubernetes Pods 及其文件系统被认为是暂时的。外部数据库是将应用容器获得的数据保存在 Pod 中的一种很好的方式。然而，一些 Pod 可能代表数据库或文件系统本身，因此任何连接的数据卷必须在 Pod 本身的生命周期之后继续存在。

本节启用 Kubernetes 持久卷 <sup>[31](#Fn31)</sup> 由 Ceph <sup>[32](#Fn32)</sup> 由 Rook 编排。 <sup>[33](#Fn33)</sup> Ceph 是一个分布式存储集群，为基于对象、块和文件系统的存储提供 Kubernetes 持久卷。Rook 操作符用于在后台安装和管理 Ceph。

以下指令安装 Rook 操作符并创建两个存储类(块和文件系统)供 Kubernetes 持久性卷使用。Ceph <sup>[34](#Fn34)</sup> 的官方 Rook 文档建议从他们的示例配置清单开始，并在需要的地方定制它们。与前面的例子保持一致，创建目录:`~/workspace/apk8s/k8s/cluster-apk8s-dev1/000-cluster/20-rook-ceph`。从以下网址下载 Rook 自定义资源定义(CRD)、操作符、集群和工具箱部署。

获取 Rook Ceph 名称空间 CRD 并将它们保存到文件`00-namespace-crd.yml`:

```
$ curl -L \
https://github.com/rook/rook/raw/release-1.0/cluster/examples/kubernetes/ceph/common.yaml >00-namespace-crd.yml

```

应用 Rook Ceph 名称空间和自定义资源定义:

```
$ kubectl apply -f 00-namespace-crd.yml

```

获取 Rook Ceph 操作员部署配置，并将其保存到文件`30-deployment-oper.yml`:

```
$ curl -L \
https://github.com/rook/rook/raw/release-1.0/cluster/examples/kubernetes/ceph/operator.yaml >30-deployment-oper.yml

```

应用 Rook Ceph 操作员部署:

```
$ kubectl apply -f 30-deployment-oper.yml

```

获取示例 Rook Ceph 集群配置，并将其保存到文件`60-cluster-rook-ceph.yml`:

```
$ curl -L \
https://github.com/rook/rook/raw/release-1.0/cluster/examples/kubernetes/ceph/cluster-test.yaml >60-cluster-rook-ceph.yml

```

应用 Rook Ceph 集群部署配置:

```
$ kubectl apply -f 60-cluster-rook-ceph.yml

```

获取 Rook Ceph 工具箱部署配置，并将其保存到文件`30-deployment-toolbox.yml`:

```
$ curl -L \
https://github.com/rook/rook/raw/release-1.0/cluster/examples/kubernetes/ceph/toolbox.yaml >30-deployment-toolbox.yml

```

应用 Rook Ceph 集群工具箱部署配置:

```
$ kubectl apply -f 30-deployment-toolbox.yml

```

`rook-ceph`名称空间现在包含用于管理底层 Ceph 集群的 pod，以及从持久卷声明中提供持久卷的能力。

查看在`rook-ceph`名称空间(`kubectl get pods -n rook-ceph`)中运行的窗格列表；以`rook-ceph-tools-`为前缀的 Pod 包含了`ceph`命令行实用程序。在这个 Pod 上执行`bash` shell，然后发出命令`ceph status`来查看新存储集群的状态。

```
$ kubectl exec -it rook-ceph-tools-5f49756bf-m6dxv \
-n rook-ceph bash
$ ceph status

```

示例输出:

```
  cluster:
    id:     f67747e5-eb2a-4301-8045-c1e210488433
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum a (age 22m)
    mgr: a(active, since 21m)
    osd: 2 osds: 2 up (since 21m), 2 in (since 21m)

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   9.1 GiB used, 107 GiB / 116 GiB avail
    pgs:

```

#### 块存储器

需要持久卷的 pod 可以通过 Kubernetes 持久卷声明(PVC)来实现。PVC 需要一个由 Rook 使用的已定义的存储类来提供一个永久卷。

设置一个名为`rook-ceph-block`的新存储类，该类由一个 CephBlockPool 支持，能够根据持久卷声明请求提供持久卷。

如果从上一节继续，用清单 [3-13](#PC76) 的内容在目录`~/workspace/apk8s/k8s/cluster-apk8s-dev1/000-cluster/20-rook-ceph`中创建文件`70-rook-ceph-block.yml`。

```
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  failureDomain: host
  replicated:
    size: 1
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: ceph.rook.io/block
parameters:
  blockPool: replicapool
  clusterNamespace: rook-ceph
  fstype: xfs
reclaimPolicy: Delete

Listing 3-13CephBlockPool and StorageClass

```

应用 CephBlockPool 和 StorageClass 配置:

```
$ kubectl apply -f 70-rook-ceph-block.yml

```

开发集群现在支持(但不限于)Kubernetes StatefulSets 通常使用的持久卷声明(PVC)。稍后，本书将 PVC 用于有状态应用，如数据库、数据索引和事件队列。下一节将介绍由 Ceph 支持的集群范围的共享文件系统的实现。

#### 共享文件系统

共享文件系统提供了围绕文件管理划分责任的机会。共享文件系统允许这样的场景，其中一组 Pods 允许用户上传文件，比如映像，而另一组 Pods 检索和处理它们。尽管有许多其他方法可以跨部署共享文件，但是 Ceph 支持的共享文件系统在集群中构建以数据为中心的平台时提供了灵活的选择。

用清单 [3-14](#PC78) 的内容在目录`~/workspace/apk8s/k8s/cluster-apk8s-dev1/000-cluster/20-rook-ceph`下创建文件`75-rook-ceph-clusterfs.yml`。

```
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: rook-ceph-clusterfs
  namespace: rook-ceph
spec:
  metadataPool:
    replicated:
      size: 1
  dataPools:
    - failureDomain: host
      replicated:
        size: 2
  metadataServer:
    activeCount: 1
    activeStandby: true

Listing 3-14CephFilesystem

```

应用 CephFilesystem 配置:

```
$ kubectl apply -f 75-rook-ceph-clusterfs.yml

```

开发集群现在能够通过 Nginx 接受和路由入站 web 流量，通过 Cert Manager 创建和使用 TLS 证书，并通过 Rook 和 Ceph 提供共享文件系统。当主要的云提供商在交钥匙解决方案中提供大部分这种栈时，将这种定制的 Kubernetes 集群与这些基本功能结合起来似乎需要很大的努力。然而，本章中配置的开发集群几乎可以在任何提供者上运行，这使得它真正具有可移植性、云原生性和厂商中立性。

开发集群是完整的，因为它可以支持本书中区块链、机器学习和物联网数据处理组件所需的大多数功能。

下一节将简要介绍监控。监控不是操作需求，而是集群管理的一个重要工具，它提供了关于性能和资源利用率的管理指标。

### 监视

Kubernetes 监控解决方案的生态系统非常庞大，并且日益成熟。从商业产品到新颖独特的开源项目，集群监控领域在关于 Kubernetes 管理的书籍、博客和教程中都有涉及。

本节使用 CoreOS 的开源项目`kube-prometheus`建立了一个最小的监控解决方案。 <sup>[35](#Fn35)</sup> 生产环境可能会有更多定制的监控和警报配置。因此，开发集群的监控组件不需要在其他环境中具有可重复性。因此，只需要记录用于创建开发集群的监控解决方案的命令。

为了与前面章节中集群配置的组织保持一致，创建目录`~/workspace/apk8s/k8s/cluster-apk8s-dev1/000-cluster/30-monitoring`，添加文件`README.md`以及清单 [3-15](#PC80) 的内容。

```
# kube-prometheus
Installation guide for the **monitoring** namespace.

```bash
git clone git@github.com:coreos/kube-prometheus.git
cd kube-prometheus
git checkout v0.1.0

kubectl create -f manifests/

# Verify the resources are ready before proceeding.

until kubectl get customresourcedefinitions servicemonitors.monitoring.coreos.com ; do date; sleep 1; echo ""; done

until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo ""; done

# Apply the manifests

.
# This command sometimes may need to be done twice
# (to work around a race condition).

kubectl apply -f manifests/
```

Listing 3-15Monitoring README

```

执行````bash`和`````之间的每个命令。新的`README.md`中的指令克隆了 kube-prometheus 项目，为 CoreOS prometheus 创建了新的自定义资源定义，并应用了几个部署，这些部署代表了使用 Prometheus 和 Grafana 监控集群所需的组件。

没有必要用开发集群配置清单的剩余部分来跟踪克隆的 kube-prometheus 项目。用 Git ignore 文件(`cd ../ && echo "kube-prometheus" >.gitignore`)排除 kube-prometheus 项目。

Note

当浏览 GitLab(或 GitHub)中的存储库时，以 markdown 格式记录命令提供了作为原始文本或呈现为格式化 HTML 的可读指令。

通过将 Grafana 端口转发到本地工作站，直观地监控新集群:

```
$ kubectl --namespace monitoring \
port-forward svc/grafana 3000

```

在本地工作站上打开`http://localhost:3000`，使用默认的管理员凭证登录 Grafana，用户名:`admin`，密码:`admin`。探索用于监控 Kubernetes 集群许多方面的预构建仪表板，包括节点、名称空间和 pod。

Note

Prometheus 操作员使用自定义资源定义(CRD) ServiceMonitor 来配置收集指标的目标。要定制 Prometheus 监视该集群，首先使用`kubectl get ServiceMonitor -n monitoring`查看预安装的 ServiceMonitor，并查看 service monitor 官方文档。 <sup>[36](#Fn36)</sup>

## 摘要

本章设置了一个小型定制 Kubernetes 开发集群，支持入口、自动 TLS 证书生成、块和文件系统存储以及基本的监控。本章中开发的配置文件旨在提交给 Git 存储库(并可选地托管在前一章中设置的 GitLab 实例上)。

将平台开发为机器学习、区块链和物联网数据管理等特定领域功能的集合，并与定制应用绑定在一起的过程，受益于连接到这一新开发集群的高度集成的 DevOps 工具链。下一章将介绍 CI/CD，并将第 [2](02.html) 章中的 GitLab 安装与这个新的开发集群相结合。

Note

清单 [3-16](#PC82) 显示了本章过程中开发的配置文件的最终列表和组织。从目录`~/workspace/apk8s/k8s`开始，列表 [3-16](#PC82) 表示从 GitLab 组`apk8s`和项目`k8s`克隆的存储库的当前状态。

```
.
└── cluster-apk8s-dev1
   └── 000-cluster
      ├── 00-ingress-nginx
      │  ├── 00-namespace.yml
      │  ├── 05-serviceaccount.yml
      │  ├── 06-clusterrole.yml
      │  ├── 07-role.yml
      │  ├── 08-rolebinding.yml
      │  ├── 09-clusterrolebinding.yml
      │  ├── 10-services.yml
      │  ├── 20-configmaps.yml
      │  ├── 30-deployment.yml
      │  └── 40-daemonset.yml
      ├── 10-cert-manager
      │  ├── 00-namespace.yml
      │  ├── 02-crd.yml
      │  └── 03-clusterissuer.yml
      ├── 20-rook-ceph
      │  ├── 00-namespace-crd.yml
      │  ├── 30-deployment-oper.yml
      │  ├── 30-deployment-toolbox.yml
      │  ├── 60-cluster-rook-ceph.yml
      │  ├── 70-rook-ceph-block.yml
      │  └── 75-rook-ceph-clusterfs.yml
      └── 30-monitoring
         └── README.md

Listing 3-16Development Cluster configuration layout

```

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

[T2`https://kubernetes.io/docs/concepts/#kubernetes-control-plane`](https://kubernetes.io/docs/concepts/%2523kubernetes-control-plane)

  [2](#Fn2_source)

[T2`https://blog.digitalocean.com/introducing-private-networking/`](https://blog.digitalocean.com/introducing-private-networking/)

  [3](#Fn3_source)

[T2`www.digitalocean.com/docs/platform/release-notes/2018/private-networking/`](http://www.digitalocean.com/docs/platform/release-notes/2018/private-networking/)

  [4](#Fn4_source)

[T2`https://istio.io/docs/tasks/security/authentication/mutual-tls/`](https://istio.io/docs/tasks/security/authentication/mutual-tls/)

  [5](#Fn5_source)

[T2`https://linkerd.io/2/features/automatic-mtls/`](https://linkerd.io/2/features/automatic-mtls/)

  [6](#Fn6_source)

[T2`www.wireguard.com/`](http://www.wireguard.com/)

  [7](#Fn7_source)

[T2`https://kubernetes.io/docs/setup/cri/`](https://kubernetes.io/docs/setup/cri/)

  [8](#Fn8_source)

[T2`https://github.com/rkt/rkt`](https://github.com/rkt/rkt)

  [9](#Fn9_source)

[T2`https://github.com/kubernetes/frakti`](https://github.com/kubernetes/frakti)

  [10](#Fn10_source)

[T2`https://cri-o.io/`](https://cri-o.io/)

  [11](#Fn11_source)

[T2`https://github.com/containerd/cri`](https://github.com/containerd/cri)

  [12](#Fn12_source)

[T2`https://containerd.io/`](https://containerd.io/)

  [13](#Fn13_source)

[T2`www.freedesktop.org/wiki/Software/systemd/`](http://www.freedesktop.org/wiki/Software/systemd/)

  [14](#Fn14_source)

[T2`www.kernel.org/doc/Documentation/cgroup-v2.txt`](http://www.kernel.org/doc/Documentation/cgroup-v2.txt)

  [15](#Fn15_source)

[T2`https://github.com/kubernetes/kubeadm/issues/1394#issuecomment-462878219`](https://github.com/kubernetes/kubeadm/issues/1394%2523issuecomment-462878219)

  [16](#Fn16_source)

[T2`https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/`](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)

  [17](#Fn17_source)

[T2`https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/`](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/)

  [18](#Fn18_source)

[T2`https://kubernetes.io/docs/reference/kubectl/overview/`](https://kubernetes.io/docs/reference/kubectl/overview/)

  [19](#Fn19_source)

[T2`https://github.com/kelseyhightower/kubernetes-the-hard-way`](https://github.com/kelseyhightower/kubernetes-the-hard-way)

  [20](#Fn20_source)

[T2`www.weave.works/oss/net/`](http://www.weave.works/oss/net/)

  [21](#Fn21_source)

[T2`www.weave.works/docs/scope/latest/introducing/`](http://www.weave.works/docs/scope/latest/introducing/)

  [22](#Fn22_source)

[T2`https://jsonnet.org/`](https://jsonnet.org/)

  [23](#Fn23_source)

[T2`https://github.com/deepmind/kapitan`](https://github.com/deepmind/kapitan)

  [24](#Fn24_source)

[T2`https://github.com/kubernetes-sigs/kustomize`](https://github.com/kubernetes-sigs/kustomize)

  [25](#Fn25_source)

[T2`www.weave.works/technologies/gitops/`](http://www.weave.works/technologies/gitops/)

  [26](#Fn26_source)

[T2`https://kubernetes.io/docs/concepts/services-networking/service/`](https://kubernetes.io/docs/concepts/services-networking/service/)

  [27](#Fn27_source)

[T2`https://kubernetes.github.io/ingress-nginx/`](https://kubernetes.github.io/ingress-nginx/)

  [28](#Fn28_source)

[T2`https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/`](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/)

  [29](#Fn29_source)

在 Kubernetes:jet stack/Cert-Manager 中自动提供和管理 TLS 证书。Jetstack，2019。 [`https://github.com/jetstack/cert-manager`](https://github.com/jetstack/cert-manager) 。

  [30](#Fn30_source)

[T2`https://letsencrypt.org/`](https://letsencrypt.org/)

  [31](#Fn31_source)

[T2`https://kubernetes.io/docs/concepts/storage/persistent-volumes/`](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)

  [32](#Fn32_source)

[T2`https://ceph.com/`](https://ceph.com/)

  [33](#Fn33_source)

[T2`https://rook.io/`](https://rook.io/)

  [34](#Fn34_source)

[T2`https://rook.io/docs/rook/v1.0/ceph-examples.html`](https://rook.io/docs/rook/v1.0/ceph-examples.html)

  [35](#Fn35_source)

[T2`https://github.com/coreos/kube-prometheus`](https://github.com/coreos/kube-prometheus)

  [36](#Fn36_source)

[T2`https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#servicemonitorspec`](https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md%2523servicemonitorspec)

 </aside>