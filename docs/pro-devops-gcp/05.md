# 五、GCP 和 Jenkins 持续交付

持续交付作为持续集成的延伸，本质上是后者的进化，也是 DevOps 的支柱之一。有各种各样的软件和工具可以将 CD 系统放置到位，比如 Travis CI、GoCD 和 Bitbucket，但是最常用的工具可能是 Jenkins。在本章中，您将看到如何使用 Jenkins 在谷歌云平台(GCP)中设计和定义我们的 CD 流水线。

## Jenkins 简介

Jenkins 可能是与持续集成和交付(CI/CD)相关的最著名的软件。Jenkins 是一个用 Java 编写的开源软件，用于帮助自动化软件开发过程中出现的所有非人类过程。

Jenkins 几乎可以使用任何类型的存储库，比如 CVS、Git、Subversion、Mercurial 等等。可以非常容易地自动化与 CI/CD 相关的所有过程。

Jenkins 中最重要的概念是插件。几乎可以找到任何必需品的插件。插件被分成不同的用途，例如

*   。净发展

*   Android 开发

*   认证和用户管理

*   命令行界面

*   构建通知程序

*   部署

这只是插件有用的所有不同领域的简短列表。有了插件，我们可以使用 Jenkins 来覆盖持续集成和交付的每个方面。

Jenkins 的受欢迎是由于以下原因:

*   跨不同操作系统易于安装和提供一致的界面

*   模块化的可能性，为扩展 Jenkins 写一个插件

*   使用和配置简单的界面

*   通过主/从架构，在不同的操作系统中使用主和从的可能性，以从相同的构建中获得不同的结果

*   简单的配置

*   轻松报告构建的历史状态

这些就是让 Jenkins 无处不在的原则。Jenkins 经常用于 CI，因为 CI 的焦点是提交后的快速构建和任务的快速可靠的自动化。使用 Jenkins，可以在提交后使用触发器来启动构建，并且我们可以自动化与此相关的任何单个任务。这意味着每次我们发布时，我们都要执行构建软件所需的所有任务。

## 与 Jenkins 的持续集成和交付

Jenkins 为创建 CI 和 CD 流水线提供了非常好的支持。我使用术语*流水线*是因为我们必须执行并重复一系列步骤来达到相同的结果(见图 [5-1](#Fig1) )。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig1_HTML.png](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig1_HTML.png)

图 5-1

持续集成和交付的流水线

CI/CD 的流水线要求将一些步骤落实到位。我现在将讨论这些步骤，并解释如何使用 Jenkins 来自动化这些步骤，从而为 CI/CD 创建一个完整的流水线。

### 密码

创建流水线的第一步是代码。当代码准备好了，并且开发人员已经完成了它并且在本地测试了它，它通常被发布到中央存储库。因为我们正在实现持续集成，所以我们通常在另一个分支中发布。通常，这与分支策略有关。例如，我们可以创建一个包含我们想要实现的任务数量的分支。使用分支策略有助于定义实际开发中的不同任务，并确定实际发布的代码部分。

### 单元测试

开发流水线的下一步是单元测试。这一步包括为代码编写测试，并在我们开始 CI 和 CD 的过程时直接执行它。Jenkins 可以用一种简单的方式自动完成这一过程。使用 Jenkins，当代码被直接提交给 repo 时，可以连接一个存储库并触发单元测试。Jenkins 从回购处下载了代码。下载完成后，我们可以构建软件并执行测试套件。建成后，如果测试通过，我们就可以进入下一步了。当我们配置项目并将代码连接到回购时，Jenkins 可以轻松管理整个过程。

### 代码集成

当代码被测试后，是时候将它集成到主分支中了。这可以是自动的，或者在大多数情况下，在代码审查策略之后。使用代码评审策略来加强代码的质量，并帮助共享系统的知识。根据这个策略，在代码被合并到主分支之前，我们需要至少两个人首先批准代码。当代码被集成时，我们可以在主分支上开始另一组单元测试。对于 CD 策略，代码必须自动集成，但是这意味着没有人工执行的审查。在这种情况下，我们可以使用*静态代码分析*来识别代码的潜在问题。使用 Jenkins，如果构建失败，可以使用工具来执行静态代码分析，有时称为代码分析。

### 系统试验

当代码集成到主分支中时，就有可能开始一些系统测试。这种测试不同于单元测试，因为我们使用真实的数据，而不是模拟的数据。这种测试有时被称为集成测试。通过这种类型的测试，我们集成并测试系统的所有组件。这种类型的测试的范围是识别新特性引入的任何新错误，并且数据来自系统的真实组件，而不是模拟组件，因为我们想要测试整个系统。

### 阶段释放

当系统测试被批准后，Jenkins 可以负责软件的发布。这可能以不同的方式发生，取决于我们如何发布软件。例如，如果我们想要发布 Docker，Jenkins 可以将代码直接放入我们用于 stage 服务器的注册表中。有了 Jenkins，我们还可以在 Bash 或 Windows 中执行脚本，在服务器上部署软件。在阶段服务器中发布对于确保软件质量至关重要。该服务器由 QA 工程团队使用，必须与生产环境中使用的服务器相似。通过这种方式，可以模拟真实环境，并在发布到生产环境之前捕获更多错误。

### 用户接受度

用户接受是构建持续发布流水线的一个重要阶段。从用户的角度来看，它由一系列验证功能的测试组成。这个阶段可以是自动的，基于 QA 工程团队编写的测试，但是它可以包括一些手动执行。关于连续交付，这个测试首先使用 Jenkins 来执行，在一些 canary 服务器上发布代码之后，QA 团队手工测试软件。该测试用于与最终用户就最需要发布的软件达成一致。

### 生产发布

流水线流程的最后也是最关键的阶段是产品发布。为了与连续交付策略保持一致，这个阶段可以一天发生多次。此外，我们可以决定不直接发布到产品中，而是选择在 canary 服务器上发布一个示例，并使用这些服务器在有限数量的用户中测试该版本。这有助于识别任何问题并在系统中创建警报。在 CD 中，我们自动化了所有的过程，并将代码中的任何变化直接发布到产品中。这可以是一个简单的映像变化或一个标签或一个错误的修复。有了纯粹的连续交付策略，我们基本上每天发布更多次软件的一小部分。Jenkins 可以帮助自动化该阶段的所有方面，并为 CD 创建完整的渠道。

## 设计良好的分支策略

设计一个好的分支策略对于良好的持续集成和交付是必不可少的。最流行的分支策略是为每个特性创建一个分支(图 [5-2](#Fig2) )。当我们在分支上发布软件时，我们执行单元测试，如果它是绿灯，我们可以合并代码。为此，Git 工作流软件非常有用。该软件由 Atlassian 构建和管理，是事实上的行业标准。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig2_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig2_HTML.jpg)

图 5-2

特征分支策略

这个策略是最简单的，并且是完全自动的。每次我们将代码合并到主分支中时，Jenkins 都会在构建就绪时构建代码。Jenkins 执行一些其他步骤来检查我们发布的软件的质量。

第一步是代码分析或静态代码分析。这是使用插件来执行的。如果代码分析是肯定的，我们可以进入下一步，即*系统测试*或*集成测试*。这通常是为了用真实数据而不是模拟数据测试系统而设计的。目的是使用真实组件进行测试。

我们执行的最后一步是*验收测试*。通常，这个测试是由工程师编写的，但在某些情况下，这个测试可以由 QA 工程团队编写，目标是从用户的角度测试功能。该测试旨在检查质量，以确保所有功能都已完全实现。该测试的另一个用途是确保旧的功能与新的软件实现一起工作。

如果所有阶段都已正确执行，我们就创建了一个新的构建，它可以安装在生产环境中，或者根据策略安装在 canary 服务器中。每当我们提交一个新特性或修复一个 bug 时，这些步骤都会被执行。例如，在存储库中，当我们只更新一个标签或更改一个映像时，这个过程就可以开始了。

借助 Jenkins，可以使用多分支流水线作业创建流水线。这种工作告诉 Jenkins 在回购上创建新的分支时创建新的流水线。这个特性由 multiline 插件管理。

当我们想在公司实施 CI 和 CD 时，我们可能需要为我们的 Jenkins 创建一个多分支系统。可以使用 *Jenkinsfile* 创建一个多分支项目。

当我们决定使用分支策略时，多分支解决方案是公认的选择。这是因为我们可以为回购中创建的每个新分支创建 CI/CD 渠道。

## 在 GCP 利用 Jenkins

到目前为止，您已经了解了 Jenkins 以及我们可以用来创建 CI/CD 流水线的一些特性。现在你将开始学习如何在 GCP 使用 Jenkins。在云中创建我们的 CI/CD 系统的原因是，我们可以根据我们的业务需求，使用它来启动新的服务器并进行扩展。

要在 GCP 上使用 Jenkins，我们需要在通过 Kubernetes 创建的集群中使用 Google Kubernetes 引擎和 Jenkins。使用 Kubernetes 创建我们的 CD 流水线给系统带来了一些重要的好处。

*   在微服务架构或多操作系统环境中，在 Kubernetes 中创建的一个虚拟主机可以针对不同的操作系统运行作业。服务器从架构使这成为可能。

*   有了 Kubernetes，我们有了短暂的执行者，这意味着我们可以在每次执行新任务时在一个干净的环境中执行构建。这可以消除不干净环境中的错误。

*   构建执行器在几秒钟内运行。

*   Kubernetes 集群仅在我们的特性处于活动状态时使用，这样可以节省资源并让集群空闲下来。

首先，我们必须设置我们的 Kubernetes 集群引擎，它是创建我们的 Jenkins 环境的基础。

### 在 Kubernetes 上配置 Jenkins

为了创建 Jenkins CI/CD 渠道，我们必须遵循以下步骤:

1.  创建一个新的 Kubernetes 集群。

2.  在集群上安装 Jenkins。

3.  安装用于 Kubernetes 的 Jenkins 插件。

4.  配置 Kubernetes 以启动 Jenkins 进程。

第一步是连接 GCP，打开 Google Shell，然后创建我们的 Kubernetes 集群。首先，我们为 Kubernetes 集群创建一个新的网络组件。创建网络的命令是

```
gcloud compute networks create jenkinscicd

```

此命令创建新的虚拟专用云(VPC)网络。我们使用这个网络来创建我们的 Kubernetes 引擎。操作结果如清单 [5-1](#PC2) 所示。

```
pierluigi_riti@practicaldevopsgcpcli:~$ gcloud compute networks create jenkinscicd
Created [https://www.googleapis.com/compute/v1/projects/practicaldevopsgcpcli/global/networks/jenkinscicd].
NAME         SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4
jenkinscicd  AUTO         REGIONAL
Instances on this network will not be reachable until firewall rules
are created. As an example, you can allow all internal traffic between
instances as well as SSH, RDP, and ICMP by running:
$ gcloud compute firewall-rules create <FIREWALL_NAME> --network jenkinscicd --allow tcp,udp,icmp --source-ranges <IP_RANGE>
$ gcloud compute firewall-rules create <FIREWALL_NAME> --network jenkinscicd --allow tcp:22,tcp:3389,icmp

Listing 5-1Result for the Kubernetes VPC Creation

```

在我们的例子中，我们不需要将我们的集群暴露在外面，我们可以很容易地跳过防火墙的配置。现在我们已经创建了一个新的 VPC，我们可以开始构建我们的 Kubernetes 集群了。

### 注意

虚拟私有云是谷歌能够在互联网上显示或隐藏我们的实例的方式。默认情况下，对于 Google 实例，我们可以看到一个默认的 VPC 被启用和公开。我们可以通过防火墙管理连接，允许或阻止特定端口或协议的流量。在每个项目中，可能有多个 VPC，并且所有 VPC 都是 IPv4，因为 IPv6 不受支持。需要记住的重要一点是，每个 VPC 都有一个“隐藏”的防火墙规则:每个传输控制协议(TCP)连接在 10 分钟不活动后都会被断开。

我们创建 VPC 是因为我们不想使用默认的 VPC。这是因为 Jenkins 用于内部应用，我们不想配置不同的规则。如果我们呆在同一个网络中，我们创建的规则会在整个网络中反映出来。

现在我们已经创建了新的网络，我们可以使用 Kubernetes 引擎构建新的 Kubernetes 集群。

```
gcloud container clusters create jenkins-cd \
--network jenkinscicd \
--zone us-east1-b \
--scopes "https://www.googleapis.com/auth/projecthosting,storage-rw"

```

我们添加一个属性`–scopes`。它允许集群访问*谷歌云存储*和*谷歌云容器注册表*。参数`–scopes`用于指定节点实例的类型。我们可以为实例定义不同的范围。我们也可以定义多个作用域，但是必须用逗号分隔。使用完整的 URI 定义范围。在 GCP，我们可以定义这个 URI。(见表 [5-1](#Tab1) 。)

表 5-1

最常见的谷歌云资源 URI

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

别名

 | 

上呼吸道感染

 |
| --- | --- |
| `bigquery` | [T2`https://www.googleapis.com/auth/bigquery`](https://www.googleapis.com/auth/bigquery) |
| `cloud-platform` | [T2`https://www.googleapis.com/auth/cloud-platform`](https://www.googleapis.com/auth/cloud-platform) |
| `cloud-source-repos` | [T2`https://www.googleapis.com/auth/source.full_control`](https://www.googleapis.com/auth/source.full_control) |
| `cloud-source-repos-ro` | [T2`https://www.googleapis.com/auth/source.read_only`](https://www.googleapis.com/auth/source.read_only) |
| `compute-ro` | [T2`https://www.googleapis.com/auth/compute.readonly`](https://www.googleapis.com/auth/compute.readonly) |
| `compute-rw` | [T2`https://www.googleapis.com/auth/compute`](https://www.googleapis.com/auth/compute) |
| `datastore` | [T2`https://www.googleapis.com/auth/datastore`](https://www.googleapis.com/auth/datastore) |
| `default` | [T2`https://www.googleapis.com/auth/devstorage.read_only`](https://www.googleapis.com/auth/devstorage.read_only) |
|   | [T2`https://www.googleapis.com/auth/logging.write`](https://www.googleapis.com/auth/logging.write) |
|   | [T2`https://www.googleapis.com/auth/monitoring.write`](https://www.googleapis.com/auth/monitoring.write) |
|   | [T2`https://www.googleapis.com/auth/pubsub`](https://www.googleapis.com/auth/pubsub) |
|   | [T2`https://www.googleapis.com/auth/service.management.readonly`](https://www.googleapis.com/auth/service.management.readonly) |
|   | [T2`https://www.googleapis.com/auth/servicecontrol`](https://www.googleapis.com/auth/servicecontrol) |
|   | [T2`https://www.googleapis.com/auth/trace.append`](https://www.googleapis.com/auth/trace.append) |
| `gke-default` | [T2`https://www.googleapis.com/auth/devstorage.read_only`](https://www.googleapis.com/auth/devstorage.read_only) |
|   | [T2`https://www.googleapis.com/auth/logging.write`](https://www.googleapis.com/auth/logging.write) |
|   | [T2`https://www.googleapis.com/auth/monitoring`](https://www.googleapis.com/auth/monitoring) |
|   | [T2`https://www.googleapis.com/auth/service.management.readonly`](https://www.googleapis.com/auth/service.management.readonly) |
|   | [T2`https://www.googleapis.com/auth/servicecontrol`](https://www.googleapis.com/auth/servicecontrol) |
|   | [T2`https://www.googleapis.com/auth/trace.append`](https://www.googleapis.com/auth/trace.append) |
| `logging-write` | [T2`https://www.googleapis.com/auth/logging.write`](https://www.googleapis.com/auth/logging.write) |
| `monitoring` | [T2`https://www.googleapis.com/auth/monitoring`](https://www.googleapis.com/auth/monitoring) |
| `monitoring-write` | [T2`https://www.googleapis.com/auth/monitoring.write`](https://www.googleapis.com/auth/monitoring.write) |
| `pubsub` | [T2`https://www.googleapis.com/auth/pubsub`](https://www.googleapis.com/auth/pubsub) |
| `service-control` | [T2`https://www.googleapis.com/auth/servicecontrol`](https://www.googleapis.com/auth/servicecontrol) |
| `service-management` | [T2`https://www.googleapis.com/auth/service.management.readonly`](https://www.googleapis.com/auth/service.management.readonly) |
| `sql` | [T2`https://www.googleapis.com/auth/sqlservice`](https://www.googleapis.com/auth/sqlservice) |
| `sql-admin` | [T2`https://www.googleapis.com/auth/sqlservice.admin`](https://www.googleapis.com/auth/sqlservice.admin) |
| `storage-full` | [T2`https://www.googleapis.com/auth/devstorage.full_control`](https://www.googleapis.com/auth/devstorage.full_control) |
| `storage-ro` | [T2`https://www.googleapis.com/auth/devstorage.read_only`](https://www.googleapis.com/auth/devstorage.read_only) |
| `storage-rw` | [T2`https://www.googleapis.com/auth/devstorage.read_write`](https://www.googleapis.com/auth/devstorage.read_write) |
| `taskqueue` | [T2`https://www.googleapis.com/auth/taskqueue`](https://www.googleapis.com/auth/taskqueue) |
| `trace` | [T2`https://www.googleapis.com/auth/trace.append`](https://www.googleapis.com/auth/trace.append) |
| `userinfo-email` | [T2`https://www.googleapis.com/auth/userinfo.email`](https://www.googleapis.com/auth/userinfo.email) |

我们可以使用 URI 来定义实例的范围。属性`--network`标识了我们想要为我们的集群使用的 VPC。(该命令需要一些时间才能完成。)我们还必须定义要创建集群的区域。这是由属性`–zone`定义的。最后，我们看到这个结果:

```
WARNING: Starting in 1.12, new clusters will have basic authentication disabled by default. Basic authentication can be enabled (or disabled) manually using the `--[no-]enable-basic-auth` flag.
WARNING: Starting in 1.12, new clusters will not have a client certificate issued. You can manually enable (or disable) the issuance of the client certificate using the `--[no-]issue-client-certificate` flag.
WARNING: Currently VPC-native is not the default mode during cluster creation. In the future, this will become the default mode and can be disabled using `--no-enable-ip-alias` flag.
Use `--[no-]enable-ip-alias` flag to suppress this warning.
This will enable the autorepair feature for nodes. Please see
https://cloud.google.com/kubernetes-engine/docs/node-auto-repair for more
information on node autorepairs.
WARNING: The behavior of --scopes will change in a future gcloud release: service-control and service-management scopes will no longer be added to what is specified in --scopes.
To use these scopes, add them explicitly to --scopes. To use the new behavior, set container/new_scopes_behavior property (gcloud config set container/new_scopes_behavior true).
WARNING: Starting in Kubernetes v1.10, new clusters will no longer get compute-rw and storage-ro scopes added to what is specified in --scopes (though the latter will remain included in the default --scopes).
To use these scopes, add them explicitly to --scopes. To use the new behavior, set container/new_scopes_behavior property (gcloud config set
container/new_scopes_behavior true).
Creating cluster jenkins-cd...done.
Created [https://container.googleapis.com/v1/projects/practicaldevopsgcpcli/zones/us-east1-b/clusters/jenkins-cd].
To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-east1-b/jenkins-cd?project=practicaldevopsgcpcli
kubeconfig entry generated for jenkins-cd.
NAME        LOCATION    MASTER_VERSION  MASTER_IP       MACHINE_TYPE   NODE_VERSION  NUM_NODES  STATUS
jenkins-cd  us-east1-b  1.9.7-gke.3     35.231.107.135  n1-standard-1  1.9.7-gke.3   3          RUNNING

```

在这个配置中，我们使用 Google Cloud Container Registry 和 Google Cloud Storage，因为我们必须允许 Jenkins 访问服务。

通过 Google Cloud Container Registry，我们允许 Jenkins 访问我们可以用来存储代码的存储库，在本例中是`projecthosting`。另一个范围是 Google 云存储，它允许 Jenkins 访问云存储，并存储容器或我们创建的构建。参数`-rw`表示读写时的访问。下一步是为我们的环境创建 Jenkins/Kubernetes 架构。

我们必须获取我们创建的集群的凭据。这些凭证用于访问托管我们的 Jenkins 部署的 Kubernetes 集群。为此，我们可以使用以下命令:

```
gcloud container clusters get-credentials <cluster name>

```

在我们的例子中，集群名是`jenkins-cd`。该命令的结果如下:

```
pierluigi_riti@practicaldevopsgcpcli:~$ gcloud container clusters get-credentials --zone us-east1-b jenkins-cd
Fetching cluster endpoint and auth data.
kubeconfig entry generated for jenkins-cd.

```

凭证现在已经配置好了，我们现在可以考虑并实现我们的 Jenkins 架构了。

### 设计 Jenkins 架构

我们要设计的是主/从 Jenkins 架构。这种类型的架构用于管理分布式 Jenkins 架构。简而言之，主设备与一个从设备一起启动作业，并监控从设备以检查作业的状态。

这种类型的体系结构尤其适用于我们想要扩展 Jenkins 配置的情况。作业可以在任何可用的从节点上执行，或者我们可以配置主节点在特定的从节点上执行作业。例如，当我们必须在特定的 SO 中执行作业时，或者当我们必须使用特定的配置时，可以在从属节点中固定作业。

图 [5-3](#Fig3) 显示了我们想要用来在 Kubernetes 中实现 Jenkins CD 流水线的架构。这个配置使用两个节点构建，这是 Kubernetes 集群的最低要求。集群允许我们扩展测试的必要性，增加另一个节点，以防测试数量增加。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig3_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig3_HTML.jpg)

图 5-3

库伯内特斯的 Jenkins 主/从建筑

这种架构本质上是在多节点集群中部署 Jenkins。我们可以有两个以上的节点。为了不使其他网络过载，我们将 Jenkins master 部署到 Kubernetes 集群中一个单独的名称空间中。在单独的名称空间中部署 Jenkins 有两个主要优势:

*   命名空间允许我们创建特定的配额。

*   命名空间用于创建与 Jenkins 和另一个部署的逻辑分离。

首先，为了继续我们的部署，我们必须更好地理解 Kubernetes 中的名称空间、pod、服务、配额和部署的概念。

### Kubernetes 中的名称空间、pod、服务、配额和部署

在 GCP 建造 Jenkins 流水线需要大量使用 Kubernetes。首先，要继续介绍 Jenkins 的配置，我必须定义并解释一些与 Kubernetes 相关的重要概念。

#### 命名空间

名称空间是我们在集群中部署的一个逻辑部分。在集群中对不同的资源进行逻辑组织是很重要的。

一个 Kubernetes 集群可以满足多个用户或用户组，这意味着不同的用户可以在同一个集群中拥有不同的项目。使用名称空间有助于识别每个团队的项目。这是在我们满足两个要求时完成的:

*   提供名称空间的名称

*   启动一种机制来定义和附加不同的策略和授权，以访问集群的一个子部分

每个用户或用户组可能想要为我们的资源创建一个不同的隔离环境，对于这个环境，我们想要定义我们自己的策略和授权。这是通过创建名称空间来完成的。每个名称空间都允许用户执行一些独特的功能。

*   唯一命名的资源

*   将管理权限委托给定义的用户

*   使用配额限制资源消耗的能力

首先，为了创建我们的名称空间，我们想看看是否有另一个名称空间可以使用。使用 Kubernetes，我们可以看到所有名称空间的列表，命令如下:

```
kubectl get namespace

```

该命令产生类似于以下内容的输出:

```
pierluigi_riti@practicaldevopsgcpcli:~$ kubectl get namespace
NAME            STATUS        AGE
default         Active        2d
kube-public     Active        2d
kube-system     Active        2d

```

Kubernetes GCP 有三个基本的名称空间:

*   `default`:该名称空间用于所有没有特定名称空间的对象。

*   `kube-public`:这个名称空间对所有用户都是可读的，包括那些没有经过身份验证的用户。

*   `kube-system`:这个名称空间用于由 Kubernetes 系统创建的对象。

这些名称空间存在于每个集群中，是在我们构建集群本身时创建的。建立集群后，我们可以使用以下命令查看其摘要:

```
kubectl get namespace <name>

```

例如，如果我们想要查看关于默认名称空间的所有细节，我们可以使用命令`kubectl get namespace default`。结果看起来像这样:

```
pierluigi_riti@practicaldevopsgcpcli:~$ kubectl get namespace default
NAME          STATUS       AGE
default       Active       2d

```

如果我们想要更多的细节，我们可以使用这个命令:

```
kubectl describe namespace <name>

```

结果如下:

```
pierluigi_riti@practicaldevopsgcpcli:~$ kubectl describe namespace default
Name: default
Labels: <none>
Annotations: <none>
Status: Active
No resource quota.
Resource Limits
Type       Resource  Min  Max  Default Request  Default Limit Max Limit/                                                              Request Ratio
----       --------  ---  ---  ---------------  ------------- -------------
Container  cpu       -    -    100m             -             -

```

我们可以看到，该命令为我们提供了关于名称空间的详细信息。为了在 Kubernetes 中设计一个名称空间，我们必须用创建名称空间所需的值构造一个 YAML 文件。该值是命名空间的名称，必须与 DNS 规则兼容。清单 [5-2](#PC13) 中给出了这个文件的一个例子。

```
apiVersion: v1
kind: Namespace
metadata:
  name: practicaldevopsgcpnamespace

Listing 5-2The Code for Creating a Namespace in Kubernetes

```

文件准备好后，我们可以运行命令:

```
Kubectl create -f ./<namespace file>.yaml

```

#### Pods

在库伯内特人的世界里，吊舱是最小的可部署单位。它是由一个或多个容器组成的组，例如 Docker，具有共享的网络/存储。例如，在图 [5-3](#Fig3) 中，pod 共享已安装节点的网络。

例如，pod 可用于运行软件，如 NGINX。我们可以使用多个 pod 来创建一个栈，但是 pod 的主要作用是操作和支持协同定位和共同管理的软件。我们在架构中使用 pod 来操作和管理 CI/CD 流水线的 Jenkins 节点。

#### 服务

服务是一种抽象，它定义了 pod 的逻辑组和访问它们的策略。我们可以将服务视为微服务。本质上，我们用不同的 pod 创建了一组服务分组。因为每个 pod 基本上都是一个容器，所以我们可以将不同的应用组合在一起，作为一个实体进行回复。

要创建服务，我们必须用创建服务的参数定义一个 YAML 文件。例如，我们可以有一个类似清单 [5-3](#PC15) 中的文件:

```
kind: Service
apiVersion: v1
metadata:
  name: practicaldevopgcp-service
spec:
  selector:
    app: PracticalDevOpsGCPApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080

Listing 5-3A YAML file for Defining a Service

```

服务文件使用元数据将名称分配给服务本身。在这种情况下，`practicaldevopgcp-service`。该服务指出任何带有标签`PracticalDevOpsGCPApp`和端口 8080 暴露的 pod。该服务公开了一个公共端口，在本例中是 80。因为我们使用了选择器，所以服务创建了称为`practicaldevopgcp-service`的端点。我们可以创建一个没有选择器的服务，在这种情况下，我们必须手动创建一个端点文件，这个文件是暴露服务的 IP 和端口所必需的(参见清单 [5-4](#PC16) )。

```
kind: Endpoints
apiVersion: v1
metadata:
  name: practicaldevopgcp-service
subsets:
  - addresses:
      - IP: 1.2.3.4
    ports:
      - port: 8080

Listing 5-4The End Points YAML File Definition

```

#### 定额

配额是限制 Kubernetes 集群资源使用的一种方式。使用对象`ResourceQuota`定义配额，这由集群的管理员启动。

我们可以限制的资源类型如下:

*   *CPU* :我们可以限制请求的数量或者非终端状态下 pod 使用的 CPU 数量。

*   *内存*:我们可以限制请求的数量，或者限制非终结状态下 pod 可以使用的内存。

*   *存储*:我们可以限制所有存储卷的请求总数和存储卷的数量。

为配额设置正确的限制对于定义我们的集群至关重要。

#### 部署

部署用于声明和管理 pod 和副本集的状态。我们可以在部署对象中描述对象的状态，部署控制器将对象的状态更改为所需的状态。这基本上是定义高可用性的基础，因为部署会处理状态，并且在状态发生变化时，会强制 pod 或复制集以部署中定义的状态运行。

部署本质上是 Kubernetes 集群的核心。它用于创建我们想要的 pod 数量，并更改 pod 的状态。清单 [5-5](#PC17) 中提供了一个示例部署。

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80

Listing 5-5Sample YAML Deployment

```

在这种情况下，我们为 NGINX 服务创建一个新的部署。我们可以定义定义价值副本的单元的数量，在我们的例子中是三个。这表示部署必须创建的单元数量。

部署定义了运行所需的容器种类，在本例中，是 NGINX 版本 1.7.9 的副本。这在容器一节中定义。模板部分定义了与容器相关联的标签，在本例中是 NGINX。这意味着所有的 pod 使用相同的名称并公开相同的端口。

### 创建 Jenkins 服务

既然我们已经讨论了 Kubernetes 的一般用途，我们将把我们的 Jenkins 图转换成一组 Kubernetes 文件。这是创建 CI/CD 流水线所必需的。

我们架构的第一部分定义了 Kubernetes 集群所需的两个 Jenkins 服务:

*   Jenkins UI

*   Jenkins 发现

我们创建的第一个服务是 UI。相关文件在清单 [5-6](#PC18) 中给出。

```
  kind: Service
  apiVersion: v1
  metadata:
    name: jenkins-ui
    namespace: jenkinsgcp
  spec:
    type: NodePort
    selector:
      app: master
    ports:
      - protocol: TCP
        port: 8080
        targetPort: 8080
        name: ui

Listing 5-6The Code for Creating the 
service-ui

```

代码定义了一个名为`jenkis-ui`的类型化服务，它在名称空间`jenkinsgcp`中定义。这使用了类型`NodePort`，用于允许外部服务和 pod 访问 Jenkins UI。该服务是我们必须为集群创建的两个服务中的第一个。

我们必须创建的另一个服务是发现服务。创建该服务的 YAML 文件如下(列表 [5-7](#PC19) ):

```
  kind: Service
  apiVersion: v1
  metadata:
    name: jenkins-discovery
    namespace: jenkinsgcp
  spec:
    selector:
      app: master
    ports:
      - protocol: TCP
        port: 50000
        targetPort: 50000
        name: slaves

Listing 5-7Code for Creating the Discovery Service

```

在本例中，我们公开了端口 50000，内部 Jenkins 服务使用该端口与主服务器通信并执行作业。完整的文件如清单 [5-8](#PC20) 所示。

```
# [START jenkins_service_ui]
---
  kind: Service
  apiVersion: v1
  metadata:
    name: jenkins-ui
    namespace: jenkinsgcp
  spec:
    type: NodePort
    selector:
      app: master
    ports:
      - protocol: TCP
        port: 8080
        targetPort: 8080
        name: ui
# [END jenkins_service_ui]
# [START jenkins_service_discovery]
---
  kind: Service

  apiVersion: v1
  metadata:
    name: jenkins-discovery
    namespace: jenkinsgcp
  spec:
    selector:
      app: master
    ports:
      - protocol: TCP
        port: 50000
        targetPort: 50000
        name: slaves
# [END jenkins_service_discovery]

Listing 5-8Code for the Complete Service File Created

```

我们必须创建的最后也是最重要的文件是部署文件，它在清单 [5-9](#PC21) 中给出。

```
# [START jenkins_deployment]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: jenkins
  namespace: jenkinsgcp
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: master
    spec:
      containers:
      - name: master
        image: jenkins/jenkins:tls
        ports:
        - containerPort: 8080
        - containerPort: 50000
        readinessProbe:
          httpGet:
            path: /login
            port: 8080

          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 2
          failureThreshold: 5
        env:
        - name: JENKINS_OPTS
          valueFrom:
            secretKeyRef:
              name: jenkins
              key: options
        - name: JAVA_OPTS
          value: '-Xmx1400m'
        volumeMounts:
        - mountPath: /var/jenkins_home
          name: jenkins-home
        resources:
          limits:
            cpu: 500m
            memory: 1500Mi
          requests:
            cpu: 500m
            memory: 1500Mi
      volumes:
      - name: jenkins-home
        gcePersistentDisk:
          pdName: jenkins-home
          fsType: ext4
          partition: 1
# [END jenkins_deployment]

Listing 5-9Deployment File Code for the Kubernetes Jenkinsfile

```

部署文件用于定义服务以及每个服务需要多少副本。对于主服务，我们定义了 1 的副本。

```
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: master

```

这是因为我们希望确保此时集群中只有一个主节点是活动的。如果集群的一个节点出现故障，Kubernetes 会在集群的其他地方启动另一个节点。部署定义了我们想要在这个部分中运行的容器的类型。

```
    spec:
      containers:
      - name: master
        image: jenkins/jenkins:tls
        ports:
        - containerPort: 8080
        - containerPort: 50000

```

本节定义了我们想要使用的 Docker 映像，在本例中，是 Jenkins 的最新版本，以及两个容器端口。现在我们定义端口 8080 和 50000。UI 服务使用的端口是 8080，发现服务使用的端口是 50000。

我们可以使用`readinessProbe`部分定义何时必须重启容器。当我们试图创建一个新的 Docker 映像时,`readinessProbe`非常重要，因为有些映像有很多数据，不能立即使用。例如，如果我们启动一个新的 Jenkins，它可以在几分钟内变成活动的。在这个场景中，Docker 删除了映像。通过`readinessProbe`部分，我们可以预先指出等待检查的秒数。我们还可以指定一个超时时间，以及在终止映像之前要尝试多少次。`periodSeconds`参数表示我们尝试执行探测的频率。`timeoutSeconds`参数表示第一次探测后等待的时间。`successThreshold`参数设置映像激活前要考虑的成功探测次数。`failureThreshold`参数表示镜像启动失败前需要考虑的失败次数。这个的代码是

```
        readinessProbe:
          httpGet:
            path: /login
            port: 8080
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 2
          failureThreshold: 5

```

本节定义了 *kubelet* 使用的参数，用于识别容器何时准备好接受流量。当容器中的所有容器都准备好时，容器就准备好了。

通过`readinessProbe`，我们定义了用于监控容器的参数，并检查容器是否有生命。第一个参数是`periodSeconds`。这个值定义了 kubelet 检查容器需要多少秒钟，在我们的例子中是每十秒钟。

有些容器需要一些外部参数才能正常工作。在我们的部署中，我们使用以下代码段发送该参数:

```
env:
        - name: JENKINS_OPTS
          valueFrom:
            secretKeyRef:
              name: jenkins
              key: options
        - name: JAVA_OPTS
          value: '-Xmx1400m'

```

最后两个部分用于定义与 Jenkins master 相关的配额和容量。配额的代码是

```
        resources:
          limits:
            cpu: 500m
            memory: 1500Mi
          requests:
            cpu: 500m
            memory: 1500Mi

```

资源部分需要一点解释，特别是关于如何定义资源。我们定义的第一个资源是 CPU。该值为 1，表示 1 个 GCP 核心。在我们的例子中，因为我们使用 GCP，所以我们设置了一个值`500m`。该值表示我们希望使用最大 500 毫核。我们看到的另一个价值是内存。内存使用以字节表示。我们可以用一个后缀来表示我们想要使用的内存值。这些后缀是 E、P、T、G、M 和 k。在我们的例子中，我们也可以有两个字母。在本例中，我们在后缀后添加了字母 I。这里我们定义了 1500 兆字节的限制。

资源的最后一部分是卷。这表明数据存储在哪里，在我们的例子中，它被分成两部分。首先是

```
        volumeMounts:
        - mountPath: /var/jenkins_home
          name: jenkins-home

```

第一部分指出了我们想要挂载的路径及其名称。第二部分定义了我们使用什么类型的卷。

```
      volumes:
      - name: jenkins-home
        gcePersistentDisk:
          pdName: jenkins-home
          fsType: ext4
          partition: 1

```

在这种情况下，我们定义使用只有一个分区的 ext4 文件系统。现在，我们可以定义创建 Jenkins 部署所需的所有参数。

### 将 Jenkins 部署到库伯内特

准备好部署和服务的文件后，我们必须定义一些其他文件，以允许 Jenkins 正常工作。我们必须创建的第一个文件是选项文件，在这个文件中，我们可以为 Jenkins 设置密码。该文件只包含一行:

```
--argumentsRealm.passwd.jenkins=CHANGE_ME --argumentsRealm.roles.jenkins=admin

```

我们可以设置自己的密码，也可以在运行时生成密码。要生成密码，我们可以使用以下命令:

```
openssl rand -base64 15

```

这将生成一个随机密码，我们可以将它放在选项文件中。我们现在可以将密码更新为`CHANGE_ME`，更新为密码生成的值。下一步是在 Kubernetes 中创建一个秘密。为此，我们可以使用以下命令:

```
kubectl create secret generic jenkins --from-file=options --namespace=jenkinsgcp

```

### 注意

Kubernetes secret 用于保存敏感信息，如密码、OAuth 令牌或 SSH 密钥。与在 pod 或容器中存储相同的值相比，秘密提供了更多的安全性和灵活性。

该命令的结果是一个简单的行，告知已经正确创建了秘密。

```
pierluigi_riti@practicaldevopsgcpcli:~/practicalgcp-jenkins$ kubectl create secret generic jenkins --from-file=options --namespace=jenkinsgcp
secret "Jenkins" created

```

创建好密码后，我们希望将我们的帐户添加到基于角色的访问控制(RBAC)的管理角色中。这赋予了我们管理集群的权利。这样做的命令是

```
kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account)

```

该命令创建一个集群角色绑定，并将我们的实际帐户添加到角色`cluster-admin`中。这使我们有权管理 Kubernetes 集群。这个命令的结果显示了我们添加的用户和结果。

```
pierluigi_riti@practicaldevopsgcpcli:~/practicalgcp-jenkins$ kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account)
Your active configuration is: [cloudshell-20307]
clusterrolebinding "cluster-admin-binding" created

```

配置好所有安全性之后，剩下的工作就是创建我们需要与 Jenkins 一起使用的卷。要创建这个卷，我们可以使用以下命令:

```
gcloud compute images create jenkins-home-image --source-uri https://storage.googleapis.com/solutions-public-assets/jenkins-cd/jenkins-home-v3.tar.gz
gcloud compute disks create jenkins-home --image jenkins-home-image

```

该命令的结果如清单 [5-10](#PC36) 所示。

```
pierluigi_riti@practicaldevopsgcpcli:~$ gcloud compute images create jenkins-home-image --source-uri https://storage.googleapis.com/solutions-public-assets/jenkins-cd/jenkins-home-v3.tar.gz
Created [https://www.googleapis.com/compute/v1/projects/practicaldevopsgcpcli/global/images/jenkins-home-image].
NAME                          PROJECT            FAMILY  DEPRECATED  STATUS
jenkins-home-image   practicaldevopsgcpcli                           READY

Listing 5-10Result of the Creation for the Volume

```

有了创建的卷，我们最终可以运行 Kubernetes 部署和服务。运行部署的命令是

```
kubectl apply -f k8s/

```

该命令执行文件夹 K8s 中的所有 YAML 文件。该命令的结果如下:

```
deployment "jenkins" created
service  "jenkins-ui" created
service "jenkins-discovery" created

```

在集群中，我们现在可以看到我们已经创建了一个部署和两个服务。接下来，我们必须检查吊舱是否正在运行。我们可以使用以下命令检查主服务器:

```
kubectl get pods --namespace jenkinsgcp

```

这显示了关于容器的信息。

```
NAME                     READY  STATUS             RESTARTS    AGE
jenkins-87c47bbb8-5mgh4  1/1    ContainerCreating  0           4m

```

我们可以使用以下命令检查服务的状态:

```
kubectl get svc --namespace jenkinsgcp

```

该命令的结果向我们显示了这两个服务及其相关信息(图 [5-4](#Fig4) )。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig4_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig4_HTML.jpg)

图 5-4

Jenkins 服务的状态

#### 公开服务

现在我们已经配置了服务，我们必须做的是将服务公开给互联网。Kubernetes 为使用`Ingress`资源提供了一个非常好的 API 系统。这用于允许外部资源访问内部群集，通常是 HTTP 资源。

为此，我们在 K8s 文件夹中创建一个名为`ingress.yaml`的新文件。该文件包含我们公开服务所需的所有信息。该文件如下所示:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: jenkins
  namespace: jenkinsgcp
spec:
  tls:
  - secretName: tls
  backend:
    serviceName: jenkins-ui
    servicePort: 8080

```

我们需要一个 TLS 通信来暴露端口。为此，我们必须创建自己的证书。我们可以用`openssl`命令创建证书。该命令确保证书是自签名的，因此浏览器可以引发与此相关的异常。我们必须接受证书和浏览器停止，以提出错误。

```
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls.key -out /tmp/tls.crt -subj "/CN=jenkins/O=jenkins"

```

这实际上创建了一个有效期为 365 天的证书。该命令创建证书并将其放入 tmp 文件夹。现在我们可以使用这个秘密来存储我们的证书并打开通信。

```
kubectl create secret generic tls --from-file=/tmp/tls.crt --from-file=/tmp/tls.key --namespace jenkinsgcp

```

现在我们有了秘密，我们可以运行位于`ingress`文件夹中的`ingress.yaml`文件。

```
kubectl apply -f ingress/

```

创建入口后，我们现在可以找到与之关联的 IP。为此，我们可以使用以下命令:

```
kubectl get ingress --namespace jenkinsgcp

```

此操作可能需要几分钟才能完成。因此，GCP 创建了一个新的负载均衡器。这是用来暴露 Jenkins 我们的基础设施。美丽而强大的 GCP 负责创建必要的基础设施来为我们公开服务，在这种情况下，通过负载均衡器。结果显示了一个表，其中包含与 Jenkins UI 关联的 IP。

```
NAME       HOSTS    ADDRESS         PORTS         AGE
jenkins    *        35.x.x.x        80, 443       2m

```

我们看到的 IP 是我们可以用来访问 Jenkins UI 的 IP。首先，要使用 UI，我们必须检查负载均衡器，实例在哪里是健康的。我们可以使用以下命令来检查负载均衡器:

```
kubectl describe ingress jenkins --namespace jenkinsgcp

```

该命令返回集群的详细信息，通过这些信息，我们可以看到 UI 何时可用(图 [5-5](#Fig5) )。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig5_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig5_HTML.jpg)

图 5-5

库伯内特人的健康状况

我们可以看到，在 Google Console 中创建的负载均衡器移动到网络服务，然后从菜单中选择负载均衡选项。要查看负载均衡器的详细信息，单击它，会显示类似于图 [5-6](#Fig6) 的内容。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig6_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig6_HTML.jpg)

图 5-6

由 GCP 创建的负载均衡器详细信息

根据群集的运行状况，我们现在可以访问 Jenkins。要访问 Jenkins UI，我们可以使用负载均衡器的 IP，在我的例子中是 35.186.199.190。这向我们展示了 Jenkins 登录页面(图 [5-7](#Fig7) )。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig7_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig7_HTML.jpg)

图 5-7

Jenkins 页面在集群上运行

对于访问，我们可以使用用户名 Jenkins 密码与我们在文件中定义的密码相同。我们现在准备创建我们的 CD 流水线。

## 创建连续交付项目

随着 Jenkins 页面的启动和运行，我们终于可以创建我们的 CD 流水线了。为了测试我们的 Jenkins 功能，我们创建了一个非常简单的应用，如下所示:

```
For trying the feature of Continuous Delivery in GCP we use the example code build by Google, this is a very simple page showing the information about the system because what we really need is understand only how to create the environment, the project is based on the gceme image present in the Google repository, gcr.io/cloud-solutions-images/gceme

```

为了创建 CD 流水线，我们首先必须创建用于发布软件的不同环境。我们实际上创造了三种环境。

*   *生产*:这是我们发布软件用于生产的环境。

*   *服务*:我们用服务环境来描述我们有多少层。在我们的例子中，我们有后端和前端。这不是一个类似于生产或金丝雀的环境，但是我们可以用它来对我们的服务层进行更合理的划分。这样可以更好地管理它。

*   *Canary* :创建用于系统的 Canary 服务器。

### 注意

canary 服务器是在 CD 中使用的服务器，用于在发布用于生产之前在真实环境中测试一个特性。canary 服务器模拟生产环境，只是以一种有限的方式。canary 服务器可用于隔离代码中的错误，通常用于为生产目的而选择的有限数量的用户。使用这些服务器不仅有助于发现错误，还有助于测试应用的 UI/UX。

这些环境用于模拟 CD 流水线开发的各个阶段。为了模拟它们，我们首先必须在 Kubernetes 中创建新的名称空间。

```
kubectl create ns practical-gcp-production

```

这个命令在 Kubernetes 集群中创建新的名称空间。我们使用这个名称空间来构建我们的生产环境。创建了新的名称空间后，我们现在可以创建代码所需的服务了。我们创建的第一个服务是用于生产。生产有两个服务:后端和前端(见清单 [5-11](#PC51) )。

```
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: practical-gcp-backend-production
spec:
  replicas: 1
  template:
    metadata:
      name: backend
      labels:
        app: gceme
        role: backend
        env: production
    spec:
      containers:
      - name: backend
        image: gcr.io/cloud-solutions-images/gceme:1.0.0
        resources:
          limits:
            memory: "500Mi"
            cpu: "100m"
        imagePullPolicy: Always
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
        command: ["sh", "-c", "app -port=8080"]
        ports:
        - name: backend
          containerPort: 8080

Listing 5-11Kubernetes File for Creating the Back-End Service in the Production Namespace

```

前面是用于为生产创建后端服务的 Kubernetes 文件。现在我们可以看到服务使用了应用`gceme`，版本 1.0.0。

```
containers:
      - name: backend
        image: gcr.io/cloud-solutions-images/gceme:1.0.0
        resources:
          limits:
            memory: "500Mi"
            cpu: "100m"
        imagePullPolicy: Always

```

该映像来自 Google 存储库，构建在 Kubernetes 文件中。我们需要创建的另一个服务是前端服务(清单 [5-12](#PC53) )。

```
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: practical-gcp-frontend-production
spec:
  replicas:
  template:
    metadata:
      name: frontend
      labels:
        app: gceme
        role: frontend
        env: production
    spec:
      containers:
      - name: frontend
        image: gcr.io/cloud-solutions-images/gceme:1.0.0
        resources:
          limits:
            memory: "500Mi"
            cpu: "100m"
        imagePullPolicy: Always
        readinessProbe:
          httpGet:
            path: /healthz
            port: 80
        command: ["sh", "-c", "app -frontend=true -backend-service=http://gceme-backend:8080 -port=80"]
        ports:
        - name: frontend
          containerPort: 80

Listing 5-12Front-End Production Service

```

这些文件在同一个文件夹中，我们可以将它们用于部署。部署用于创建和设置 pod 和副本集的特定状态。我们使用以下命令来部署我们的应用:

```
kubectl --namespace=practical-gcp-production apply -f k8s/production

```

该命令的结果是

```
deployment gceme-backend-production" created
deployment "gceme-frontend-production" created

```

现在，当我们创建服务时，我们可以为服务和金丝雀创建其他部署。本质上，我们告诉 Kubernetes 创建一个 Kubernetes 服务，即服务线，并为 canary 服务创建一个部署。第一个命令创建一个服务。Kubernetes 中的服务是一个类似于 pod 的 REST 对象。当我们创建一个服务时，我们实际上是将资源提交给 API 服务器，以创建新的资源。当我们创建金丝雀时，我们实际上创建了一个新的部署。在 Kubernetes 中，这遵循每个部署的规则:应用位于前一个 Kubernetes 应用之下。在这种情况下，我们可以使用负载均衡器部署到互联网上，并且可以从外部访问。

```
kubectl --namespace=practical-gcp-production apply -f k8s/services
kubectl --namespace=practical-gcp-production apply -f k8s/canary

```

我们不改变名称空间，因为我们想使用相同的环境。创建所有部署后，我们可以扩展前端的生产。我们这样做是因为我们希望在前端应用中运行多个实例。

```
kubectl --namespace=practical-gcp-production\
           scale deployment practical-gcp-frontend \
          --replicas=4

```

扩展部署后，我们可以使用以下命令确定分配给应用的外部 IP:

```
kubectl --namespace=practical-gcp-production get service practical-gcp-frontend

```

这个命令显示了我们可以用来访问我们的应用的服务的 IP 地址(图 [5-8](#Fig8) )。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig8_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig8_HTML.jpg)

图 5-8

我们应用的 IP，部署在 Kubernetes

我们可以看到正在运行的应用何时将公共 IP 放入浏览器。它看起来有点像图 [5-9](#Fig9) 。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig9_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig9_HTML.jpg)

图 5-9

应用启动并运行

### 创建存储库

CD 开发过程的下一步是创建存储库，我们可以将代码放入其中，以用于我们的流水线。在谷歌云中，我们有一个谷歌云存储库。这是一个私有的 Git 库，我们可以用它来维护我们的代码。

我们必须创建一个存放代码的仓库。为此，打开 Google Cloud 控制台，在 find 文本框中，键入 *repository* 。这将产生一个下拉菜单。选择源代码，打开源代码库窗口(图 [5-10](#Fig10) )。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig10_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig10_HTML.jpg)

图 5-10

谷歌云平台中的源库服务

创建一个名为 *practicaldevopscgp* 的新存储库。随着回购的创建，我们看到类似图 [5-11](#Fig11) 的东西。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig11_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig11_HTML.jpg)

图 5-11

带有新存储库的源代码

创建了 repo 之后，我们现在可以开始初始化 Git，将代码推送到远程存储库，插入 *sample-app* 文件夹，并使用命令初始化存储库。为此，我们必须执行以下命令:

```
gcloud init && git config credential.https://source.developers.google.com.helper gcloud.cmd

```

这为我们的 Git 存储库创建了基本配置。之后，我们必须使用以下命令使我们的存储库远程化:

```
git remote add google https://source.developers.google.com/p/practicaldevopsgcpcli/r/practicaldevopscgp

```

创建并配置好回购后，我们可以开始推送回购中的第一个文件。首先，我们必须将文件添加到本地存储库中。

```
git add .
git commit -m "First commit"

```

前面的命令用于在我们的本地存储库中添加和提交文件。提交文件后，我们现在必须在远程存储库中推送文件。

```
git push --all google

```

这将推送远程存储库中的所有代码，并创建一个名为 master 的新分支(图 [5-12](#Fig12) )。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig12_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig12_HTML.jpg)

图 5-12

我们存储库中的第一个提交

我们最终将所有文件提交给了远程存储库。我们可以直接从谷歌平台上管理和查看有关文件的信息。我们只需要连接到平台并移动到存储库部分(图 [5-13](#Fig13) )。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig13_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig13_HTML.jpg)

图 5-13

提交了文件的回购

### 创建 Jenkins 流水线

存储库完成后，我们必须在 Jenkins 中为 CD 创建一个流水线。为此，我们设置了服务必须使用的凭证。因为它是一个自动服务，并且我们没有到 UI 的用户连接，所以我们想为该服务提供一个不同的凭证。要添加凭证，请连接到 Jenkins UI，打开凭证部分，然后单击链接 Global，这将打开一个页面，在该页面上可以在服务器上添加新凭证(图 [5-14](#Fig14) )。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig14_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig14_HTML.jpg)

图 5-14

Jenkins 的证件区

单击窗口左侧面板上的添加凭据链接。这将打开另一个页面，我们可以在其中选择凭据。从元数据中选择谷歌服务账户(图 [5-15](#Fig15) )。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig15_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig15_HTML.jpg)

图 5-15

Google 帐户的凭证

单击“确定”,为服务添加凭据。这会将凭据添加到系统中。因为 Jenkins 在 GKE 手下工作，谷歌暴露了他的身份。该凭证可以由 Jenkins 自动获取并在其中配置。凭证现在看起来如图 [5-16](#Fig16) 所示。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig16_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig16_HTML.jpg)

图 5-16

更新的凭据

配置好凭证后，我们现在需要的是为构建创建作业。在 Jenkins UI 中，选择创建新作业的链接，然后选择项目的多分支流水线类型。这将打开项目的配置部分(图 [5-17](#Fig17) )。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig17_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig17_HTML.jpg)

图 5-17

多分支项目配置部分

要为流水线添加源代码，必须单击 Add Source 下拉菜单，然后选择 Git 存储库、之前创建的凭证，最后是我们之前创建的代码存储库的链接。最后应该会看到类似图 [5-18](#Fig18) 的东西。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig18_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig18_HTML.jpg)

图 5-18

Jenkins GIT 配置部分

现在我们必须定义何时执行构建。因为我们希望拥有 CD，所以我们每次提交存储库时都会触发构建。这个实践引导着 CD，我们每天可以有数百个构建。要配置 Jenkins 做到这一点，我们必须选择 GitHub hook trigger for git SCM polling 选项(图 [5-19](#Fig19) )。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig19_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig19_HTML.jpg)

图 5-19

GitHub 触发器的配置

在主页面上，我们可以看到 GitHub 钩子的日志，因为我们已经将我们的 Jenkins 与 Git 连接起来了(图 [5-20](#Fig20) )。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig20_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig20_HTML.jpg)

图 5-20

带有 GitHub 钩子日志的菜单

保存配置。这开始了我们在分支中的第一次构建(图 [5-21](#Fig21) )。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig21_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig21_HTML.jpg)

图 5-21

Jenkins 的第一栋建筑

因为我们的项目是一个多流水线项目，所以我们必须配置一个 Jenkinsfile 来配置和管理构建。您现在必须学习的是如何创建 Jenkinsfile。

## 创建 Jenkinsfile

我们现在已经让 Jenkins 在服务器上运行了。现在我们必须为应用和 CD 流水线创建结构。

要用 Jenkins 构建 CD 流水线，我们必须创建一个流水线项目和一个 Jenkinsfile。这个文件用于描述我们想要部署的分支，并且必须放在我们创建的每个分支上。当我讨论微服务时，您会看到关于这些步骤的更多细节。现在，理解 Jenkinsfile 的结构很重要(清单 [5-13](#PC63) )。

```
node {
  def project = 'practicaldevopsgcp-197023'
  def appName = ' pdopsgcp'
  def feSvcName = "${appName}-frontend"
  def imageTag = "gcr.io/${project}/${appName}:${env.BRANCH_NAME}.${env.BUILD_NUMBER}"

  checkout scm

  stage 'Build image'
  sh("docker build -t ${imageTag} .")

  stage 'Run Go tests'
  sh("docker run ${imageTag} go test")

  stage 'Push image to registry'
  sh("gcloud docker -- push ${imageTag}")

  stage "Deploy Application"
  switch (env.BRANCH_NAME) {
    // Roll out to canary environment
    case "canary":
        // Change deployed image in canary to the one we just built
        sh("sed -i.bak 's#gcr.io/cloud-solutions-images/pdopsgcp:1.0.0#${imageTag}#' ./k8s/canary/*.yaml")
        sh("kubectl --namespace=production apply -f k8s/services/")
        sh("kubectl --namespace=production apply -f k8s/canary/")
        sh("echo http://`kubectl --namespace=production get service/${feSvcName} --output=json | jq -r '.status.loadBalancer.ingress[0].ip'` > ${feSvcName}")
        break

    // Roll out to production
    case "master":
        // Change deployed image in canary to the one we just built
        sh("sed -i.bak 's#gcr.io/cloud-solutions-images/ pdopsgcp:1.0.0#${imageTag}#' ./k8s/production/*.yaml")
        sh("kubectl --namespace=production apply -f k8s/services/")
        sh("kubectl --namespace=production apply -f k8s/production/")
        sh("echo http://`kubectl --namespace=production get service/${feSvcName} --output=json | jq -r '.status.loadBalancer.ingress[0].ip'` > ${feSvcName}")
        break

    // Roll out a dev environment
    default:
        // Create namespace if it doesn't exist
        sh("kubectl get ns ${env.BRANCH_NAME} || kubectl create ns ${env.BRANCH_NAME}")
        // Don't use public load balancing for development branches
        sh("sed -i.bak 's#LoadBalancer#ClusterIP#' ./k8s/services/frontend.yaml")
        sh("sed -i.bak 's#gcr.io/cloud-solutions-images/ pdopsgcp:1.0.0#${imageTag}#' ./k8s/dev/*.yaml")
        sh("kubectl --namespace=${env.BRANCH_NAME} apply -f k8s/services/")
        sh("kubectl --namespace=${env.BRANCH_NAME} apply -f k8s/dev/")
        echo 'To access your environment run `kubectl proxy`'
        echo "Then access your service via http://localhost:8001/api/v1/proxy/namespaces/${env.BRANCH_NAME}/services/${feSvcName}:80/"
  }
}

Listing 5-13The Jenkinsfile Necessary for Continuous Deployment

```

pipelines 项目使用这个文件来构建我们制作的软件。现在我们可以看到名为 Dockerfile 的文件。这是因为我们为我们的软件构建 Docker 映像。

该文件用于定义 Jenkins 可以自动执行流水线的阶段，特别是我们在以下阶段找到的文件:

*   *Build* :用于创建最后一个代码的 Docker 映像

*   *运行*:对刚刚创建的映像运行一些测试

*   *Push* :如果测试通过，将映像推送到 repo 上

*   *部署*:用于在不同的服务器上部署应用

这些步骤是我们流水线的基础(图 [5-22](#Fig22) )。现在我们可以为分支配置一些参数。每当开发人员创建一个新的分支时，这些都可以改变，它们代表了创建多分支策略的最佳方式。

### 警告

在 Jenkinsfile 中选择项目 ID 时要小心。你可以从我们 GCP 的主页上获得这些信息。

![../images/464715_1_En_5_Chapter/464715_1_En_5_Fig22_HTML.jpg](../images/464715_1_En_5_Chapter/464715_1_En_5_Fig22_HTML.jpg)

图 5-22

已建流水线

现在我们可以看到 Jenkins 根据我们在 Jenkinsfile 中定义的阶段创建了流水线。

```
  stage 'Build image'
  sh("docker build -t ${imageTag} .")

  stage 'Run Go tests'
  sh("docker run ${imageTag} go test")

  stage 'Push image to registry'
  sh("gcloud docker -- push ${imageTag}")

```

现在，当我们对代码进行任何更改时，我们从头到尾执行所有流水线，这允许我们从代码开始创建一个完整的软件。

## 结论

本章介绍如何为 CD 创建流水线项目。当我们想要实现一个完整的 DevOps 资源时，这是非常重要的。Jenkins 为实现 CD 提供了一个非常有效的工具，通过 GCP 的 Kubernetes，我们可以创建一个非常强大的 CD 系统。有了我们新的多分支项目，我们可以有一个其阶段的图形表示。Jenkinsfile 是一个非常简单而强大的配置 CD 系统的工具。我们可以使用代码来定义我们需要的每个阶段和每个环境。我们可以发布带有代码的文件，通过这种方式，每个开发人员都正确地配置了系统，这减少了维护时间，同时提高了项目的稳定性。