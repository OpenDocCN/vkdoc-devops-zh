# 三、部署和扩展

一个*部署*是一组统一管理的 Pod 实例，它们都基于相同的 Docker 映像。一个 Pod 实例被称为*副本*。部署控制器使用多个副本来实现*高可伸缩性*，通过提供比单个单体单元更多的计算能力，以及集群内*高可用性*，通过将流量从不健康的单元分流(在服务控制器的帮助下，我们将在第 [4](4.html) 章中看到)并在它们出现故障或阻塞时重新启动或重新创建它们。

根据这里给出的定义，部署可能只是“Pod 集群”的一个花哨名称，但“部署”实际上并不是用词不当；部署控制器的真正力量在于其实际的*发布能力*—向其消费者部署几乎零停机时间的新 Pod 版本(例如，使用蓝/绿或滚动更新)以及不同扩展配置之间的无缝过渡，同时保留计算资源。

本章一开始，我们将概述部署控制器、复制集控制器和 pod 之间的关系。然后，我们将学习如何启动、监视和控制部署。我们还将看到定位和引用 Kubernetes 作为指示部署的结果而创建的对象的各种方法。

一旦涵盖了部署的要点，我们将重点关注可用的部署策略，包括滚动和蓝/绿部署，以及允许在资源利用率和服务消费者影响之间实现最佳折衷的参数。

最后，我们将讨论在 Pod 级别使用 Kubernetes 的开箱即用水平 Pod 自动缩放器(HPA)和在节点级别使用 GKE 在集群创建时的自动缩放标志进行自动缩放的主题。

## 复制集

由于历史原因，运行副本的过程是通过一个名为 *ReplicaSet* 的独立组件来处理的。这个组件又替换了一个更老的组件，叫做*复制控制器*。

为了避免任何混淆，让我们花一些时间来理解部署控制器和复制集控制器之间的关系。部署是更高级别的控制器，它管理部署转换(例如滚动更新)以及通过复制集的复制条件。并不是部署*替换*或者*嵌入*replica set 对象(Camel大小写拼写用于指代对象名)，他们只是简单地控制它；尽管鼓励用户通过部署与复制集进行交互，但复制集仍然作为离散的 Kubernetes 对象可见。

总之，在 Kubernetes 中，ReplicaSet 是一个独立的、完全合格的对象，但是不鼓励在部署之外运行 ReplicaSet，而且，只要 ReplicaSet 在部署的控制之下，所有的交互都应该由部署对象进行。

## 我们的第一次部署

部署是 Kubernetes 中的一个基本特性，创建一个部署比创建一个单一的单体 Pod 更容易。如果我们看一下上一章的例子，我们会注意到每一个`kubectl run`实例都必须带有一个`--restart=Never`标志。嗯，创建部署的一种“廉价”方式是简单地*丢弃*这个标志:

```
$ kubectl run nginx --image=nginx
deployment.apps/nginx created

```

省略`--restart=Never`标志具有戏剧性的效果。我们现在已经创建了两个额外的对象，而不是创建一个单元:一个部署控制器，它控制一个复制集，然后复制集反过来控制一个单元！

```
$ kubectl get deployment,replicaset,pod
NAME               DESIRED CURRENT UP-TO-DATE AVAIL.
deployment.*/nginx 1       1       1          1

NAME                        DESIRED   CURRENT   READY
replicaset.*/nginx-8586cf59 1         1         1

NAME                       READY  STATUS    RESTARTS
pod/nginx-8586cf59-b72sn   1/1    Running   0

```

虽然以这种方式创建部署很方便，但是 Kubernetes 将来会反对使用`kubectl run`命令创建部署。新的*首选方法*是通过`kubectl create deployment <NAME>`命令，如下所示:

```
$ kubectl create deployment nginx --image=nginx
deployment.apps/nginx created

```

目前这两个版本是等价的，但是“老方法”，通过`kubectl run`，仍然是大多数教科书和官方 [`http://kubernetes.io`](http://kubernetes.io) 网站上的例子所使用的方法；因此，建议读者暂时记住这两种方法。

### 注意

从 Kubernetes v1.15 开始，`kubectl create <RESOURCE-TYPE>`命令仍然不能很好地替代传统的`kubectl run`方法。例如，当 Kubernetes 团队反对通过传统的基于运行的形式创建 CronJobs(在第 [7 章](7.html)中有所涉及)命令时，他们没有包括`--schedule`标志。根据 GitHub 上的功能请求，这个问题在随后的版本中得到了解决。

在`kubectl create deployment`的情况下，`--replicas`标志缺失。这并不意味着命令被“破坏”，但是它迫使用户采取更多的步骤来实现一个曾经只需要一个命令的目标。部署的副本数量仍然可以通过`kubectl scale`命令(将在下一节中介绍)或通过声明一个 JSON 片段来强制设置。

将我们的注意力转回到作为创建部署的结果而创建的对象上，给定的名称`nginx`现在应用于部署控制器实例，而不是 Pod。Pod 有一个随机的名字:`nginx-8586cf59-b72sn`。为什么POD现在有随机的名字是因为它们是短暂的。它们的数量可能不同；一些可能被杀死，一些新的可能被创造，等等。事实上，控制单个 Pod 的部署不是很有用。让我们通过使用`--replicas=<N>`标志来指定除 1(默认值)之外的副本数量:

```
# Kill the running Deployment first
$ kubectl delete deployment/nginx

# Specify three replicas
$ kubectl run nginx --image=nginx --replicas=3
deployment.apps/nginx created

```

我们现在将看到三个而不是一个 pod 在运行:

```
$ kubectl get pods
NAME                    READY STATUS   RESTARTS  AGE
nginx-64f497f8fd-8grlr  1/1   Running  0         39s
nginx-64f497f8fd-8svqz  1/1   Running  0         39s
nginx-64f497f8fd-b5hxn  1/1   Running  0         39s

```

副本的数量是动态的，可以在运行时使用`kubectl scale deploy/<NAME> --replicas=<NUMBER>`命令指定。例如:

```
$ kubectl scale deploy/nginx --replicas=5
deployment.extensions/nginx scaled

$ kubectl get pods
NAME                    READY STATUS   RESTARTS  AGE
nginx-64f497f8fd-8grlr  1/1   Running  0         5m
nginx-64f497f8fd-8svqz  1/1   Running  0         5m
nginx-64f497f8fd-b5hx   1/1   Running  0         5m
nginx-64f497f8fd-w8p6k  1/1   Running  0         1m
nginx-64f497f8fd-x7vdv  1/1   Running  0         1m7

```

同样，指定的 Pod 映像也是动态的，可以使用`kubectl set image deploy/<NAME> <CONTAINER-NAME>=<URI>`命令更改。例如:

```
$ kubectl set image deploy/nginx nginx=nginx:1.9.1
deployment.extensions/nginx image updated

```

## 关于列出部署的更多信息

`kubectl get deployments`命令显示许多列:

```
$ kubectl get deployments
NAME    DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
nginx   5        5        5           5          10m

```

显示的列是指部署中的 *Pod 副本*的数量:

*   `DESIRED`:在`deployment.spec.replicas`中指定的*目标状态*

*   `CURRENT`:运行但不一定可用的副本数量:在`deployment.status.replicas`中指定

*   `UP-TO-DATE`:已经被更新以达到当前状态的 Pod 副本的数量:在`deployment.status.updatedReplicas`中指定

*   `AVAILABLE`:用户实际可用的副本数量:在`deployment.status.availableReplicas`中指定

*   `AGE`:部署控制器自首次创建以来已经运行了多长时间

## 部署清单

一个最小但完整的部署清单的例子可能会令人生畏。因此，更容易将部署清单视为一个两步过程。

第一步是定义一个 Pod 模板。Pod 模板几乎与独立 Pod 的定义相同，只是我们只填充了`metadata`和`spec`部分:

```
# Pod Template
...
spec:
  template:
    metadata:
      labels:
        app: nginx-app # Pod label
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.7.1

```

还需要声明一个显式的 Pod 标签键/对，因为我们需要引用 Pod。前面我们已经使用了`app: nginx-app,`，然后在部署规范中使用它来将控制器对象绑定到 Pod 模板:

```
# Deployment Spec
...
spec:
  replicas: 3        # Specify number of replicas
  selector:
    matchLabels:     # Select Pod using label
      app: nginx-app

```

在`spec:`下，我们还指定了副本的数量，这相当于命令形式中使用的`--replicas=<N>`标志。

最后，我们将这两个定义组合成一个完整的部署清单:

```
# simpleDeployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-declarative
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.7.1

```

首先使用`kubectl apply -f <FILE>`命令创建这个部署:

```
$ kubectl apply -f simpleDeployment.yaml
deployment.apps/nginx-declarative created

```

效果将类似于它的命令性对应物的效果；将创建三个单元:

```
$ kubectl get pods
NAME                      READY STATUS  RESTARTS AGE
nginx-declarative-*-bj4wn 1/1   Running 0        3m
nginx-declarative-*-brhvw 1/1   Running 0        3m
nginx-declarative-*-tc6hv 1/1   Running 0        3m

```

## 监视和控制部署

`kubectl rollout status deployment/<NAME>`命令用于监控正在进行的部署。例如，假设我们为 Nginx 创建了一个新的强制性部署，并且我们想要跟踪它的进度:

```
$ kubectl run nginx --image=nginx --replicas=3 \
    ; kubectl rollout status deployment/nginx
deployment.apps/nginx created
Waiting for deployment "nginx" rollout to finish:
0 of 3 updated replicas are available...
Waiting for deployment "nginx" rollout to finish:
1 of 3 updated replicas are available...
Waiting for deployment "nginx" rollout to finish:
2 of 3 updated replicas are available...
deployment "nginx" successfully rolled out

```

简单的部署——尤其是那些不涉及对现有部署进行更新的部署——通常在几秒钟内即可执行；然而，更复杂的部署可能需要几分钟。在这种情况下，我们可能希望暂停正在进行的部署，以清理资源或执行额外的监视。

分别使用`kubectl rollout pause deploy/<NAME>`和`kubectl rollout resume deploy/<NAME>`命令暂停和恢复部署。例如:

```
$ kubectl rollout pause deploy/nginx
deployment "nginx" paused

$ kubectl rollout resume deploy/nginx
deployment "nginx" resumed

```

## 找出部署的副本集

副本(Pod 实例)不是由部署控制器直接控制的，而是由中间媒介 ReplicaSet 控制器控制的。因此，确定哪些复制集控制器从属于给定的部署控制器通常是有用的。

这可以通过*标签选择器*，使用`kubectl describe`命令或者简单地依靠*视觉匹配*来实现。

让我们从标签选择器方法开始。在这种情况下，我们简单地使用`kubectl get rs`命令列出复制集，但是添加了`--selector=<SELECTOR-EXPRESSION>`标志以匹配部署清单中 Pod 的标签和选择器表达式。例如:

```
$ kubectl get rs --selector="run=nginx"
NAME               DESIRED   CURRENT   READY     AGE
nginx-64f497f8fd   3         3         3         2m

```

请注意标签`run=nginx`是由`kubectl run`命令自动添加的；在`simpleDeployement.yaml`，我们使用了一个自定义标签:`app=nginx-app`。

现在，让我们考虑一下`kubectl describe`方法。在这里，我们只需键入`kubectl describe deploy/<NAME>`命令并定位`OldReplicaSets`和`NewReplicaSet`字段。例如:

```
$ kubectl describe deploy/nginx
...
OldReplicaSets: <none>
NewReplicaSet:  nginx-declarative-381369836
                (3/3 replicas created)
...

```

最后一种更简单的方法是简单地输入`kubectl get rs`并识别前缀是部署名称的复制集。

有时，我们可能想要找出给定复制集的父部署控制器。为此，我们可以使用`kubectl describe rs/<NAME>`命令并搜索`Controlled By`字段的值。例如:

```
$ kubectl describe rs/nginx-381369836
...
Controlled By:  Deployment/nginx
...

```

或者，如果需要更程序化的方法，我们可以使用 JSONPath:

```
$ kubectl get pod/nginx-381369836-g4z5r \
    -o jsonpath \
    --template="{.metadata.ownerReferences[*].name}"
nginx-381369836

```

## 找出复制体的POD

在上一节中，我们已经看到了如何识别部署的副本集。反过来，复制集控制POD；因此，下一个自然的问题是如何找出哪些是在给定复制集控制下的荚果。幸运的是，我们使用了之前见过的三种技术:标签选择器、`kubectl describe`命令和视觉匹配。

让我们从标签选择器开始。这与之前完全相同，使用了`--selector=<SELECTOR-EXPRESSION>`标志，除了我们向`kubectl get`请求类型为`pod` (Pod)的对象，而不是`rs`(复制集)。例如:

```
$ kubectl get pod --selector="run=nginx"
NAME                   READY  STATUS   RESTARTS  AGE
nginx-64f497f8fd-72vfm 1/1    Running  0         18m
nginx-64f497f8fd-8zdhf 1/1    Running  0         18m
nginx-64f497f8fd-skrdw 1/1    Running  0         18m

```

类似地，`kubectl describe`命令的使用包括简单地指定复制集的对象类型(`rs`)和名称:

```
$ kubectl describe rs/nginx-381369836
...
Events:
  FirstSeen LastSeen Count Message
  --------- -------- ----- -------
  55m       55m      1     Created pod: nginx-*-cv2xj
  55m       55m      1     Created pod: nginx-*-8b5z9
  55m       55m      1     Created pod: nginx-*-npkn8
...

```

视觉匹配技术是最简单的。我们可以通过考虑 Pod 的两个前缀字符串来计算出副本集的名称。例如，对于 Pod `nginx-64f497f8fd-72vfm`，它的控制复制集将是`nginx-64f497f8fd`:

```
nginx-64f497f8fd-8zdhf  1/1  Running   0     18m

```

最后但同样重要的是，如果我们从一个 Pod 的对象开始，并想找出它的控制对象，我们可以使用`kubectl describe pod/<NAME>`命令并定位控制器对象，后跟`Controlled By:`属性:

```
$ kubectl describe pod/nginx-381369836-g4z5r
...
Controlled By:  ReplicaSet/nginx-381369836
...

```

如果需要更程序化的方法，可以使用 JSONPath 获得相同的结果，如下所示:

```
$ kubectl get pod/nginx-381369836-g4z5r \
    -o jsonpath \
    --template="{.metadata.ownerReferences[*].name}"
nginx-declarative-381369836

```

## 删除部署

使用触发*级联*删除的`kubectl delete deploy/<NAME>`命令删除部署；所有子副本集和相关的 Pod 对象都将被删除:

```
$ kubectl delete deploy/nginx
deployment.extensions "nginx" deleted

```

`kubectl delete`命令也可以直接从清单文件中选取部署的名称，如下所示:

```
$ kubectl delete -f simpleDeployment.yaml
deployment.apps "nginx-declarative" deleted

```

可以通过添加`--cascade=false`标志来防止级联默认删除行为(导致所有副本集和单元被删除)。例如:

```
$ kubectl delete -f simpleDeployment.yaml \
    --cascade=false
deployment.apps "nginx-declarative" deleted

```

## 版本跟踪与仅扩展部署

部署可以分为两种类型:*版本跟踪*和*仅伸缩*。

修订跟踪部署是更改 Pod 规范的某个方面，很可能是其中声明的容器映像的数量和/或版本。仅改变副本数量(强制和声明)的部署不会触发修订。

例如，发出`kubectl scale`命令不会创建一个可用于撤销比例变化的修订点。返回到先前数量的副本需要再次设置先前的数量。

通过使用`kubectl scale`命令或者通过设置`deployment.spec.replicas`属性并使用`kubectl apply -f <DEPLOYMENT-MANIFEST>`命令应用相应的文件来强制实现扩展部署。

由于仅扩展部署完全由复制集控制器管理，因此主部署控制器不提供修订跟踪(例如，回滚功能)。客观地说，尽管这种行为相当不一致，但人们可以认为改变副本的数量不如改变映像那样重要。

## 部署策略

到目前为止，我们只是将部署视为一种部署多个 Pod 副本的机制，但我们没有描述如何控制这一过程:Kubernetes 应该删除所有现有的 Pod(导致停机)还是应该以更优雅的方式进行？这就是部署策略的全部内容。Kubernetes 提供了两大类部署策略:

*   **Recreate:** Oppenheimer 升级部署的方法:首先销毁所有东西，然后创建由新部署清单声明的副本。

*   **RollingUpdate:** 微调升级流程，从一次更新一个 Pod 这样的“细致”工作，一直到完全成熟的蓝绿色部署，在部署过程中，在丢弃旧的 Pod 副本之前，会建立一整套新的 Pod 副本。

使用可设置为`Recreate`或`RollingUpdate`的`deployment.spec.strategy.type`属性配置策略。后者是默认的。在接下来的两节中，我们将详细讨论每个选项。

## 重新创建部署

重新创建部署实际上是终止所有现有的 pod，并根据指定的目标状态创建新的集。从这个意义上说，重新创建部署会导致停机，因为一旦 pod 的数量达到零，在创建新的 pod 之前会有一些延迟。

重新创建部署对于非生产场景非常有用，在这些场景中，我们希望尽快看到预期的更改，而不必等待滚动更新程序完成。

通常，部署类型会在部署清单中以声明方式指定:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-declarative
spec:
  replicas: 5
  strategy:
    type: Recreate # Recreate setting
...

```

当使用`kubectl apply -f <MANIFEST>`命令应用时，实际的重新创建部署将会发生。

## 滚动更新部署

滚动更新*通常是*一次更新一个 Pod(负载均衡器在后台进行相应的管理),这样用户就不会经历停机时间，并且资源(例如，节点数量)得到合理利用。

在实践中，*传统的“一次一个”滚动更新部署*和*蓝/绿*部署(前面几节将详细介绍蓝/绿部署)都可以通过设置`deployment.spec.rolling.Update.maxSurge`和`deployment.spec.rolling.Update.maxUnavailable`变量使用相同的滚动更新机制来实现。我们将进一步了解如何实现这一目标。

现在，让我们考虑下面的说明:

```
...
spec:
  replicas: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
...

```

这里显示的`maxSurge`和`maxUnavailable`值允许我们调整滚动更新的性质:

*   `maxSurge`:该属性指定在基线(旧副本集)上的终止过程开始之前，必须为目标(新副本集)创建的单元数量。如果该值为 1，这意味着副本的数量将在部署期间保持不变，代价是 Kubernetes 在任何给定时间为一个额外的单元分配资源。

*   `maxUnavailable`:该属性指定在任何给定时间可能不可用的节点的最大数量。如果该数量为零，如在前面的例子中，则至少需要值为 1 的`maxSurge`,因为需要备用资源来保持期望副本的数量恒定。

请注意，`maxUnavailable`和`maxSurge`变量接受百分比值。例如，在这种情况下，Kubernetes 25%的额外资源将用于保证运行的副本数量不会减少:

```
...
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 0%
...

```

这个例子相当于最初讨论的例子，因为四个副本中的 25%恰好是一个副本。

## 更高的 MaxSurge 值的利弊

在从*基线*(旧)发生转换之前，较高的`maxSurge`设置会创建目标的更多 Pod 实例，*将成为*(新)复制集。因此，`maxSurge`号越高，用户达到*基线*和*版本的过渡期就越短——只要部署与第 [4](4.html) 章中控制的服务结合使用。*

缺点是集群需要额外的资源来创建新的 pod，同时保持旧的 pod 运行。例如，假设基准复制集计算利用率为 100%，50%的`maxSurge`设置在迁移高峰时将正好需要 150%的计算资源。

## 高 MaxUnavailable 值的利弊

在理想情况下，`maxUnavailable`属性应该简单地设置为 0，以确保副本的数量保持不变。然而，这并不总是*可能的*，或者*必需的*。

如果 Kubernetes 集群中的计算资源不足，那么`maxSurge`必须为 0。在这种情况下，从*基线*到*为*状态的转换将涉及取下一个或多个吊舱。

例如，以下设置可确保不分配额外的资源，但在部署期间的任何给定时间，至少有 75%的节点可用:

```
...
maxSurge: 0%
maxUnavailable: 25%
...

```

更高的`maxUnavailable`值的缺点是会减少集群的容量——包括与部署相关的副本，而不是整个 Kubernetes 集群。然而，这未必是一件*坏事*。部署可以在需求较低的时候执行，减少副本的数量可能不会产生明显的效果。此外，即使该值越高，可用的副本数量越少，整个过程也会更快，因为可以同时更新多个 pod。

## 蓝色/绿色部署

蓝/绿部署是指我们提前将整个新的*部署为* Pod 集群，当准备就绪时，我们一次性将流量从*基线* Pod 集群中切换出来。在服务控制器的帮助下，整个过程被透明地编排(参见第 [4 章](4.html))。

在 Kubernetes，这并不是一个全新的策略类型；我们仍然使用`RollingUpdate`部署类型，但是我们将`maxSurge`属性设置为 100%，将`maxUnavailability`属性设置为 0，如下所示:

```
...
maxSurge: 100%
maxUnavailable: 0%
...

```

假设部署构成了修订变更—底层映像类型或映像版本发生了变化—Kubernetes 将分三大步骤执行蓝/绿部署:

1.  为要成为新副本集的*创建新单元，直到达到`maxSurge`限制；在这种情况下是 100%。*

2.  将流量重定向到新的副本集——这需要服务控制器的帮助，我们将在第 [4](4.html) 章中介绍。

3.  终止旧副本集中的节点。

## 最大浪涌和最大不可用设置摘要

正如我们在前面几节中看到的，大多数部署策略包括将`maxSurge`和`maxUnavailability`属性设置为不同的值。表 [3-1](#Tab1) 总结了最典型的用例以及相关的权衡和样本值。

表 3-1

不同部署策略的适当值

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

方案

 | 

权衡取舍

 | 

`maxSurge`

 | 

`maxUnavailbility`

 |
| --- | --- | --- | --- |
| 销毁和部署* | 容量和效用。 | Zero | 100% |
| 一次一个滚动更新 | 资源 | one | Zero |
| 一次一个滚动更新 | 容量 | Zero | one |
| 更快的滚动更新 | 资源 | 25% | Zero |
| 更快的滚动更新 | 容量 | Zero | 25% |
| 蓝色/绿色 | 资源 | 100% | Zero |

*与重新创建的部署相同

## 受控部署

受控展开是一种允许操作员(或等效的自动化系统)对潜在的失败展开做出反应的展开。部署可能会以多种方式失败，但在两种情况下，Kubernetes 为运营商提供了更多的控制。

第一种情况是最常见的:Kubernetes 无法更新请求的副本数量，因为没有足够的集群资源，或者因为 pod 本身无法启动—例如，当指定了不存在的容器映像时。新的计算资源或容器映像不太可能突然出现并纠正这种情况。相反，我们想要的是 Kubernetes 在设定的时间后认为部署失败，而不是让部署永远进行下去。这是通过将`deployment.spec.progressDeadlineSeconds`属性设置为适当的值来实现的。

例如，让我们考虑指定大量副本(30 个)的示例，假设一个小型的三节点 Kubernetes 集群计算资源不足:

```
# nginxDeployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  progressDeadlineSeconds: 60
  replicas: 30 # excessive number for a tiny cluster
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.1
        ports:
        - containerPort: 80

```

如果 60 秒后所有副本都不可用，属性将强制 Kubernetes 使部署失败:

```
$ kubectl apply -f nginxDeployment.yaml ; \
    kubectl rollout status deploy/nginx
deployment.apps/nginx created
Waiting for deployment "nginx" rollout to finish:
0 of 30 updated replicas are available...
Waiting for deployment "nginx" rollout to finish:
7 of 30 updated replicas are available...
Waiting for deployment "nginx" rollout to finish:
11 of 30 updated replicas are available...
error: deployment "nginx" exceeded its
progress deadline

```

第二个场景是部署本身成功，但是容器由于一些内部问题(比如内存泄漏、引导代码中的空指针异常等等)而失败。在受控部署中，我们可能希望部署单个副本，等待几秒钟以确保它是稳定的，然后才继续下一个副本。Kubernetes 的默认行为是并行更新所有副本，这可能不是我们想要的。

属性允许我们定义 Kubernetes 在一个 Pod 准备好之后，在处理下一个副本之前必须等待*多长时间。该属性的值是出现最明显问题所需的时间和部署所需时间之间的权衡。*

结合`progressDeadlineSeconds`属性，`minReadySeconds`属性有助于防止灾难性的部署，尤其是在更新现有的健康部署时。

例如，假设我们有一个名为`myapp`(在名为`myApp1.yaml`的清单中声明)的健康部署，它由三个副本组成:

```
# myApp1.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  progressDeadlineSeconds: 60
  minReadySeconds: 10
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: busybox:1.27 # 1.27
        command: ["bin/sh"]
        args: ["-c", "sleep 999999;"]

```

现在让我们部署`myApp1.yaml`并通过确保三个 pod 的状态都是`Running`来检查它是否启动并运行:

```
$ kubectl apply -f myApp1.yaml
deployment.apps/myapp created

$ kubectl get pods -w
NAME                   READY  STATUS  RESTARTS   AGE
myapp-54785c6ddc-5zmvp 1/1    Running 0          17s
myapp-54785c6ddc-rbcv8 1/1    Running 0          17s
myapp-54785c6ddc-wlf8r 1/1    Running 0          17s

```

假设我们想用一个新的“有问题的”版本来更新部署，这个版本存储在一个名为`myApp2.yaml`的清单中:更多关于为什么*有问题*的信息在源代码片段之后:

```
# myApp2.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  progressDeadlineSeconds: 60
  minReadySeconds: 10
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: busybox:1.27 # 1.27
        command: ["bin/sh"]
        args: ["-c", "sleep 5; exit 1"] # bug!

```

我们在前面的清单中故意引入了一个错误；应用在五秒钟后退出，并出现错误。我们现在应用新的，*有问题的*部署清单:

```
$ kubectl apply -f myApp2.yaml
deployment.apps/myapp configured

```

如果我们观察 pod 的状态，我们将看到一个新的副本控制器`5d79979bc9`将被生成，并且它将只创建一个名为`nm7pv`的 pod。请注意，健康的副本控制器`54785c6ddc`保持不变:

```
$ kubectl get pods -w
NAME                   STATUS            RESTARTS
myapp-54785c6ddc-5zmvp Running           0
myapp-54785c6ddc-rbcv8 Running           0
myapp-54785c6ddc-wlf8r Running           0
myapp-5d79979bc9-sgqsn Pending           0
myapp-5d79979bc9-sgqsn Pending           0
myapp-5d79979bc9-sgqsn ContainerCreating 0s
myapp-5d79979bc9-sgqsn Running           0
myapp-5d79979bc9-sgqsn Error             0
myapp-5d79979bc9-sgqsn Running           1
myapp-5d79979bc9-sgqsn Error             1
myapp-5d79979bc9-sgqsn CrashLoopBackOff  1
...

```

一秒钟后，pod `sgqsn`显示为`Running,`，但是因为我们已经将`minReadySeconds`设置为十秒钟，Kubernetes 还没有开始旋转新的 pod。不出所料，五秒钟后，Pod 出现错误，Kubernetes 继续重新启动 Pod。

由于我们也将`deadlineProgressSeconds`设置为 60，新的错误部署将在一段时间后过期:

```
$ kubectl rollout status deploy/myapp
Waiting for deployment "myapp" rollout to finish:
1 out of 3 new replicas have been updated...
Waiting for deployment "myapp" rollout to finish:
1 out of 3 new replicas have been updated...
error: deployment "myapp" exceeded its
progress deadline

```

最终结果将是现有的健康复制集`myapp-54785c6ddc`保持不变；这种行为允许我们修复失败的部署，并在不中断健康 pod 用户的情况下重试:

```
$ kubectl get pods
NAME                   STATUS           RESTARTS
myapp-54785c6ddc-5zmvp Running          0
myapp-54785c6ddc-rbcv8 Running          0
myapp-54785c6ddc-wlf8r Running          0
myapp-5d79979bc9-sgqsn CrashLoopBackOff 5

```

请注意，使用`minReadySeconds`属性时，`deadlineProgressSeconds`中指定的时间可能会稍微延长。

## 首次展示历史

给定部署的首次展示历史由针对其执行的*修订跟踪*更新组成。正如我们之前所解释的，仅缩放更新不会创建修订版，也不是部署历史的一部分。修订是允许执行回滚的机制。卷展栏的历史是一个按升序排列的修订列表，可以使用`kubectl rollout history deploy/<NAME>`命令进行检索。例如:

```
$ kubectl rollout history deploy/nginx
deployments "nginx"
REVISION        CHANGE-CAUSE
1               <none>
2               <none>

```

注意,`CHANGE-CAUSE`字段的值是`<none>`,因为默认情况下不记录变更命令,`.` Kubernetes 为每个修订跟踪部署更新分配一个递增的修订号。此外，它还可以记录命令(如`kubectl apply`、`kubectl set image`等)。)用于创建每个修订。要实现这种行为，只需在每个命令后添加`--record`标志。这将填充`CHANGE-CAUSE`列—依次从`deployment.metadata.annotations.kubernetes.io/change-cause`获得。例如:

```
$ kubectl run nginx --image=nginx:1.7.0 --record
deployment.apps/nginx created

$ kubectl set image deploy/nginx nginx=nginx:1.9.0 \
    --record
deployment.extensions/nginx image updated

$ kubectl rollout history deploy/nginx
deployments "nginx"
REVISION CHANGE-CAUSE
1        kubectl run nginx --image=nginx:1.7.0 ...
2        kubectl set image deploy/nginx ...

```

了解用于创建修订版的命令可能不足以区分修订版和其他修订版，尤其是在部署的详细信息是通过文件(声明性方法)捕获的情况下，这一点也很有用。为了获得修订版的映像和其他元数据的细节，我们应该使用`kubectl rollout history --revision=<N>`命令。例如:

```
$ kubectl rollout history deploy/nginx --revision=1
deployments "nginx" with revision #1
Pod Template:
  Labels:       pod-template-hash=4217019353
        run=nginx
  Annotations:  kubernetes.io/change-cause=kubectl
                run nginx --image=nginx --record=true
  Containers:
   nginx:
    Image:      nginx
    Port:       <none>
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>
...

```

为了使修订历史中的条目数量易于管理，`deployment.spec.revisionHistoryLimit`有助于建立最大限制。

## 正在回滚部署

在 Kubernetes 中，回滚部署被称为*撤销*过程，它是使用`kubectl rollout undo deploy/<NAME>`命令执行的。撤消过程实际上创建了一个新的修订，其参数与前一个相同。通过这种方式，进一步的撤销将导致回到之前的版本。在以下示例中，我们创建了两个部署，检查了展开历史记录，执行了撤消操作，然后再次检查了展开历史记录:

```
$ kubectl run nginx --image=nginx:1.7.0 --record
deployment.apps/nginx created

$ kubectl set image deploy/nginx nginx=nginx:1.9.0 \
    --record
deployment.extensions/nginx image updated

$ kubectl rollout history deploy/nginx
deployments "nginx"
REVISION CHANGE-CAUSE
1        kubectl run nginx --image=nginx:1.7.0 ...
2        kubectl set image deploy/nginx ...

$ kubectl rollout undo deploy/nginx
deployment.extensions/nginx

$ kubectl rollout history deploy/nginx
deployments "nginx"
REVISION CHANGE-CAUSE
2       kubectl set image deploy/nginx ...
3       kubectl run nginx --image=nginx:1.7.0 ...

```

也可以回滚到特定的修订，而不是以前的修订。这是通过使用常规的撤销命令并添加`--to-revision=<N>`标志来实现的。例如:

```
$ kubectl rollout undo deploy/nginx --to-revision=2

```

使用`0`作为修订号会导致 Kubernetes 恢复到之前的版本——这相当于省略了标志。

## 水平吊舱自动缩放器

自动扩展是指运行时系统根据 CPU 负载等可观察指标，以无人值守的方式分配额外计算和存储资源的能力。特别是在 Kubernetes 中，当前事实上的自动缩放功能是由一种称为水平 Pod 自动缩放器(HPA)的服务提供的。水平 Pod 自动缩放器(HPA)是一个常规的 Kubernetes API 资源和控制器，它根据观察到的资源利用率以无人值守的方式管理 Pod 的副本数量。它可以被认为是一个机器人，它根据 Pod 的扩展标准(通常是平均 CPU 负载)代表人类管理员发出`kubectl scale`命令。

为了避免混淆，有必要理解“水平缩放”是指跨多个节点创建或删除 Pod 副本，这是在编写水平 Pod 自动缩放(HPA)服务时 Kubernetes 中正式实现的唯一一种自动缩放类型。不应将水平缩放误认为垂直缩放。垂直扩展涉及增加特定节点的计算资源(例如，RAM 和 CPU)。除了水平扩展和垂直扩展之外，实际的“其他”扩展类型是*集群扩展*，它调整节点(虚拟机或物理机)的数量，而不是固定节点数量内的单元数量。在本章的最后，我们提供了集群扩展的概述。

## 设置自动缩放

使用`kubectl autoscale`命令可以强制设置自动缩放。我们首先应该有一个正在运行的部署(或复制集)，它将由水平 Pod 自动缩放器(HPA)控制。我们还必须指定实例的最小和最大数量，最后，还要指定作为扩展基础的 CPU 百分比。完整的命令语法如下:`kubectl autoscale deploy/<NAME> --min=<N> --max=<N> --cpu-percent=<N>`。

例如，要在自动缩放模式下运行 Nginx，我们需要遵循以下两个步骤:

```
$ kubectl run nginx --image=nginx
deployment.apps/nginx created

$ kubectl autoscale deployment nginx \
    --min=1 --max=3 --cpu-percent=5
horizontalpodautoscaler.autoscaling/nginx autoscaled

```

CPU 百分比为 5 是故意的，以便在部署负载最小时，很容易观察到 HPA 的行为。

以声明方式设置自动扩展需要创建新的清单文件，该文件指定目标部署(或副本集)副本的最小和最大数量，以及 CPU 阈值:

```
# hpa.yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: nginx
spec:
  maxReplicas: 3
  minReplicas: 1
  scaleTargetRef:
    kind: Deployment
    name: nginx
  targetCPUUtilizationPercentage: 5

```

为了运行清单，使用了`kubectl apply -f <FILE>`命令，但是我们必须确保在应用自动缩放清单之前已经启动并运行了部署。例如:

```
$ kubectl run nginx --image=nginx --replicas=1
deployment.apps/nginx created

$ kubectl apply -f hpa.yaml
horizontalpodautoscaler.autoscaling/nginx created

```

## 观察自动缩放的作用

观察自动伸缩的运行只需设置一个较低的 CPU 阈值(比如 5%)，然后在运行的副本上创建一些 CPU 负载。让我们来看看这个过程的实际操作。

我们首先创建一个复制副本，并将 HPA 连接到它，如前面所示:

```
$ kubectl run nginx --image=nginx
deployment.apps/nginx created

$ kubectl autoscale deployment nginx \
    --min=1 --max=3 --cpu-percent=5
horizontalpodautoscaler.autoscaling/nginx autoscaled

```

然后，我们观察自动缩放器的行为:

```
$ kubectl get hpa -w
NAME  REFERENCE    TARGETS MINPODS MAXPODS REPLICAS
nginx Deployment/* 0% / 5% 1       3       1

```

然后，我们打开一个单独的 shell，访问部署的 Pod，并生成一个无限循环来引起一些 CPU 负载:

```
$ kubectl get pods
NAME                   READY STATUS    RESTARTS   AGE
nginx-4217019353-2fb1j 1/1   Running   0          27m

$ kubectl exec -it nginx-4217019353-sn1px \
    -- sh -c 'while true; do true; done'

```

如果我们回到观察 HPA 控制器的 shell，我们会看到它是如何增加副本数量的:

```
$ kubectl get hpa -w
NAME  REFERENCE    TARGETS   MINPODS MAXPODS REPLICAS
nginx Deployment/*  0% / 5%  1       3       1
nginx Deployment/* 20% / 5%  1       3       1
nginx Deployment/* 65% / 5%  1       3       2
nginx Deployment/* 80% / 5%  1       3       3
nginx Deployment/* 90% / 5%  1       3       3

```

如果我们中断无限循环，我们会看到副本的数量会在一段时间后减少到一个。

另一种方法是在 Nginx HTTP 服务器上生成负载。这将是一个更现实的场景，但需要一些额外的步骤来设置。首先，我们需要一个生成负载的工具，比如 ApacheBench:

```
$ sudo apt-get update
$ sudo apt-get install apache2-utils

```

然后，我们需要公开外部负载均衡器上的部署。这使用服务控制器命令—将在第 [4](4.html) 章中进一步解释:

```
$ kubectl expose deployment nginx \
    --type="LoadBalancer" --port=80 --target-port=80
service/nginx exposed

```

我们一直等到获得公共 IP 地址:

```
$ kubectl get service -w
NAME       TYPE         CLUSTER-IP    EXTERNAL-IP
kubernetes ClusterIP    10.59.240.1   <none>
nginx      LoadBalancer 10.59.245.138 <pending>
nginx      LoadBalancer 10.59.245.138 35.197.222.105

```

然后，我们向它“抛出”过多的流量，在本例中，使用 100 个单独的线程向外部 IP/端口发出 1，000，000 个请求:

```
$ ab -n 1000000 -c 100 http://35.197.222.105:80/
This is ApacheBench, Version 2.3
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/

Benchmarking 35.197.222.105 (be patient)

```

我们可以通过运行`kubectl get hpa -w`来观察结果。请注意，nginx 是高效的，显著的负载和快速连接同样需要将 pod 的 CPU 提升到 5%以上。

另一个需要考虑的方面是，当观察 HPA 的动作时，它不会立即做出反应。原因是 HPA 通常每 30 秒查询一次资源利用率，除非默认值已经更改，然后根据所有可用单元的最后一分钟平均值做出反应。这意味着，例如，在 30 秒的窗口中，单个 Pod 上短暂的 99% CPU 峰值可能不足以使聚合平均值超过定义的阈值。

此外，HPA 算法的实现方式避免了不稳定的缩放行为。HPA 在纵向扩展时比在横向扩展时相对更快，因为使服务可用优先于节省计算资源。

更多信息请参考 [`https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/`](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) 。

最后但同样重要的是，当不再需要 HPA 对象时，可以使用`kubectl delete hpa/<NAME>`命令将其处理掉。例如:

```
$ kubectl delete hpa/nginx
horizontalpodautoscaler.autoscaling "nginx" deleted

```

## 扩展 Kubernetes 集群本身

HPA 控制器受限于固定 Kubernetes 集群中的可用资源。它产生了新的 pod，而不是全新的虚拟机来托管 Kubernetes 节点。

GKE 支持集群式扩展；然而，它的原理与颐康保障户口不同。缩放触发不是 CPU 负载或类似的度量；当 Pods 请求当前集群节点中不可用的计算资源时，会分配新节点。同样，“向下”扩展包括将 pod 整合到更少的节点中；这可能会对那些无法正常管理意外关机的 pod 上运行的工作负载产生影响。

在 GCP 中启用集群式自动缩放的最实用的方法是在创建集群时使用`gcloud container clusters create <CLUSTER-NAME> --num-nodes <NUM>`(参见第 [1 章](1.html)了解出错时可能需要的其他附加标志)命令，但添加`--enable-autoscaling --min-nodes <MIN> --max-nodes <MAX>`作为附加参数。在下面的示例中，我们将`<MIN>`设置为 3，但将`<MAX>`设置为 6，以便在需要时可以提供两倍的标准集群容量:

```
$ gcloud container clusters create my-cluster \
    --num-nodes=3 --enable-autoscaling \
    --min-nodes 3 --max-nodes 6

Creating cluster my-cluster...
Cluster is being health-checked (master is healthy)
done.
Creating node pool my-pool...done.
NAME        LOCATION        MASTER_VERSION  NUM_NODES
my-cluster  europe-west2-a  1.12.8-gke.10   3

```

查看集群式自动扩展运行情况的最简单方法是启动具有大量副本的部署，并查看部署前后可用节点的数量:

```
$ kubectl get nodes
NAME                                        STATUS
gke-my-cluster-default-pool-3d996410-7307   Ready     gke-my-cluster-default-pool-3d996410-d2wz   Ready
gke-my-cluster-default-pool-3d996410-gw59   Ready

$ kubectl run nginx --image=nginx:1.9.1 --replicas=25
deployment.apps/nginx created

# After 2 minutes
$ kubectl get nodes
NAME                                        STATUS
gke-my-cluster-default-pool-3d996410-7307   Ready
gke-my-cluster-default-pool-3d996410-d2wz   Ready
gke-my-cluster-default-pool-3d996410-gw59   Ready
gke-my-cluster-default-pool-3d996410-rhnp   Ready
gke-my-cluster-default-pool-3d996410-rjnc   Ready

```

相反，删除部署会提示自动缩放器减少节点数量:

```
$ kubectl delete deploy/nginx
deployment.extensions "nginx" deleted

# After some minutes
$ kubectl get nodes
NAME                                        STATUS
gke-my-cluster-default-pool-3d996410-7307   Ready
gke-my-cluster-default-pool-3d996410-d2wz   Ready
gke-my-cluster-default-pool-3d996410-gw59   Ready

```

## 摘要

在本章中，我们学习了如何使用部署控制器使用不同的策略(例如滚动和蓝/绿部署)来扩展 pod 和发布新版本。我们还看到了如何监视和控制正在进行的部署，例如，通过暂停、恢复部署，甚至回滚到以前的版本。最后，我们学习了如何在 Pod 和节点级别设置自动伸缩机制，以实现更好的集群资源利用率。