# 4.服务发现

服务发现是从其他 pod 以及外部世界(互联网)中定位一个或多个 pod 的地址的能力。Kubernetes 提供了一个服务控制器来满足服务发现和连接用例，如 Pod 到 Pod、LAN 到 Pod 和 Internet 到 Pod。

服务发现是必要的，因为 pod 是易变的；在它们的生命周期中，它们可能会被创建和销毁多次，每次都会获得不同的 IP 地址。Kubernetes 的自我修复和扩展特性也意味着我们通常需要虚拟 IP 地址和循环负载平衡机制，而不是特定的、类似宠物的 pod 的离散地址。

在本章中，我们将首先探讨前面提到的三种连接用例(Pod 到 Pod、LAN 到 Pod 和 Internet 到 Pod)。然后，我们将研究跨不同空间的发布服务的特性以及多个端口的公开。最后，我们将思考服务控制器如何帮助实现平稳的启动和关闭，以及零停机部署。

## 连接使用案例概述

服务控制器执行各种功能，但它的主要目的是跟踪 Pods 地址和端口，并将这些信息发布给感兴趣的服务消费者。服务控制器还在群集场景中提供了单一入口点—多个 Pod 副本。为了实现其目的，它使用其他 Kubernetes 服务，如`kube-dns`和`kube-proxy`，这些服务反过来利用底层内核和来自操作系统的网络资源，如 *iptables。*

服务控制器适合各种用例，但这些是最典型的用例:

*   **Pod-to-Pod** **:** 这个场景涉及一个 Pod 连接到同一个 Kubernetes 集群中的其他 Pod。*集群 IP* 服务类型用于此目的；它由虚拟 IP 地址和 DNS 条目组成，可由同一集群内的所有 pod 寻址，并将在副本准备就绪和“未准备就绪”时分别从集群中添加和删除副本。

*   **LAN-to-Pod** **:** 在这种情况下，服务消费者通常位于 Kubernetes 集群之外，但位于同一个局域网(LAN)内。*节点端口*服务类型通常适用于满足此用例；服务控制器在每个工作节点中发布一个离散端口，该端口映射到公开的部署。

*   **Internet-to-Pod** **:** 在大多数情况下，我们会希望将至少一个部署暴露在互联网上。Kubernetes 将与 Google 云平台的负载平衡器进行交互，以便创建一个外部公共 IP 地址并将其路由到节点端口。这是一个*负载平衡器*服务类型。

还有一种特殊情况是*无头服务*主要与 StatefulSets 结合使用，以提供对每个单独 Pod 的 DNS 访问。这种特殊情况将在第 [9](9.html) 章中单独介绍。

值得理解的是，服务控制器提供了一个间接层——以 DNS 条目、额外的 IP 地址或端口号的形式——到拥有自己的离散 IP 地址并可以直接访问的 pod。如果我们需要的只是找出一个 Pods 的 IP 地址，我们可以使用`kubectl get pods -o wide -l <LABEL>`命令，其中`<LABEL>`将我们正在寻找的 Pods 与其他 Pods 区分开来。只有在大量 pod 运行时才需要`-l`标志。否则，我们可以通过它们的命名约定来区分这些豆荚。

在下一个示例中，我们首先创建三个 Nginx 副本，然后找出它们的 IP 地址:

```
$ kubectl run nginx --image=nginx --replicas=3
deployment.apps/nginx created

$ kubectl get pods -o wide -l run=nginx
NAME            READY STATUS    RESTARTS   IP
nginx-*-5s7fb   1/1   Running   0          10.0.1.13
nginx-*-fkjx4   1/1   Running   0          10.0.0.7
nginx-*-sg9bv   1/1   Running   0          10.0.1.14

```

相反，如果我们想要 IP 地址本身——比方说，通过流水线将它们传送到某个程序中——我们可以通过指定 JSON 路径查询来使用编程方法:

```
$ kubectl get pods -o jsonpath \
   --template="{.items[*].status.podIP}" -l run=nginx
10.36.1.7 10.36.0.6 10.36.2.8

```

## 单元到单元连接用例

从外部 Pod 寻址 Pod 涉及创建一个服务对象，该服务对象将观察 Pod，以便在它们分别准备就绪和*未准备就绪*时，从虚拟 IP 地址(称为*集群 IP* )添加或删除它们。如第 [2](2.html) 章所述，集装箱“准备就绪”的检测可通过自定义探头实现。服务控制器可以针对各种对象，如裸 pod 和复制集，但我们将只关注部署。

第一步是创建服务控制器。创建观察现有部署的服务的命令是`kubectl expose deploy/<NAME>`。可选标志`--name=<NAME>`将赋予服务一个不同于其目标部署的名称，只有在我们没有在目标对象上指定端口的情况下,`--port=<NUMBER>`才是必需的。例如:

```
$ kubectl run nginx --image=nginx --replicas=3
deployment.apps/nginx created

$ kubectl expose deploy/nginx --port=80
service/nginx exposed

```

声明性方法类似，但是它需要使用标签选择器(第 [2](2.html) 章)来标识目标部署:

```
# service.yaml
kind: Service
apiVersion: v1
metadata:
  name: nginx
spec:
  selector:
    run: nginx
  ports:
  - protocol: TCP
    port: 80

```

可以在创建部署后立即应用此清单，如下所示:

```
$ kubectl run nginx --image=nginx --replicas=3
deployment "nginx" created

$ kubectl apply -f service.yaml
service "nginx" created

```

到目前为止，我们已经解决了“暴露”一个部署的问题，但是结果如何呢？我们现在能做什么以前不可能的事情？首先，让我们键入`kubectl get services`来查看我们刚刚创建的对象的详细信息:

```
$ kubectl get services
NAME       TYPE      CLUSTER-IP    EXT-IP PORT(S)
kubernetes ClusterIP 10.39.240.1   <none> 443/TCP
nginx      ClusterIP 10.39.243.143 <none> 80/TCP

```

IP 地址`10.39.243.143`现在准备好从端口`80`上的任何 Pod 接收进入流量。我们可以通过运行连接到此端点的虚拟一次性 Pod 来检查这一点:

```
$ kubectl run test --rm -i --image=alpine \
    --restart=Never \
    -- wget -O - http://10.39.243.143 | grep title
<title>Welcome to nginx!</title>

```

我们有一个虚拟 IP 来访问我们部署的 Pod 副本，但是 Pod 如何首先找到 IP 地址呢？好消息是 Kubernetes 为每个服务控制器创建了一个 DNS 条目。在一个简单的、单一集群、单一名称空间的场景中——就像本文中的所有例子一样——我们可以简单地使用服务名本身。例如，假设我们在另一个 Pod 中(例如，Alpine 实例)，我们可以如下访问 Nginx web 服务器:

```
$ kubectl run test --rm -i --image=alpine \
    --restart=Never \
    -- wget -O - http://nginx | grep title
<title>Welcome to nginx!</title>

```

注意，我们现在用`http://nginx`代替`http://10.39.243.143`。对 Nginx 的每个 HTTP 请求将以循环方式命中三个 Pod 副本中的一个。如果我们想让自己相信事实确实如此，我们可以改变每个 Nginx Pod 的`index.html`内容，使其显示 Pod 的名称，而不是相同的默认欢迎页面。假设我们的三副本 Nginx 部署仍然在运行，我们可以应用建议的更改，首先提取部署的 Pod 名称，然后用`$HOSTNAME`的值覆盖每个 Pod 中的`index.html`的内容:

```
# Extract Pod names
$ pods=$(kubectl get pods -l run=nginx -o jsonpath \
       --template="{.items[*].metadata.name})"

# Change the contents of index.html for every Pod
$ for pod in $pods; \
    do kubectl exec -ti $pod \
           -- bash -c "echo \$HOSTNAME > \
           /usr/share/nginx/html/index.html"; \
    done

```

现在，我们可以再次启动临时 Pod 但是这一次，我们将循环运行对`http://nginx`的请求，直到我们按下 Ctrl+C:

```
$ kubectl run test --rm -i --image=alpine \
    --restart=Never -- \
    sh -c "while true; do wget -q -O \
    - http://nginx ; sleep 1 ; done"
nginx-dbddb74b8-t728t
nginx-dbddb74b8-h87s4
nginx-dbddb74b8-mwcg4
nginx-dbddb74b8-h87s4
nginx-dbddb74b8-h87s4
nginx-dbddb74b8-t728t
nginx-dbddb74b8-h87s4
nginx-dbddb74b8-h87s4
...

```

正如我们在结果输出中看到的，循环机制正在起作用，因为每个请求都落在一个随机的 Pod 上。

## LAN-to-Pod 连接用例

从外部主机访问 Kubernetes 集群的 Pods 涉及到使用`NodePort`服务类型公开服务(默认服务类型是`ClusterIP`)。这只是将`--type=NodePort`标志添加到`kubectl expose`命令中的问题。例如:

```
$ kubectl run nginx --image=nginx --replicas=3
deployment.apps/nginx created

$ kubectl expose deploy/nginx --type="NodePort" \
    --port=80
service/nginx exposed

```

结果是，现在可以通过任何节点 IP 地址上的离散端口访问 Nginx HTTP 服务器。首先，让我们看看分配的端口是什么:

```
$ kubectl describe service/nginx | grep NodePort
NodePort:                 <unset>  30091/TCP

```

我们可以看到自动分配的端口是`30091`。我们现在可以使用外部 IP 地址，从位于同一本地区域的 Kubernetes 集群之外的一台机器*,通过该端口上的任何 Kubernetes 工作节点向 Nginx 的 web 服务器发出请求:*

```
$ kubectl get nodes -o wide
NAME                  STATUS  AGE EXTERNAL-IP
gke-*-9777d23b-9103   Ready   7h  35.189.64.73
gke-*-9777d23b-m6hk   Ready   7h  35.197.208.108
gke-*-9777d23b-r4s9   Ready   7h  35.197.192.9

$ curl -s http://35.189.64.73:30091 | grep title
<title>Welcome to nginx!</title>
$ curl -s http://35.197.208.108:30091 | grep title
<title>Welcome to nginx!</title>
$ curl -s http://35.197.192.9:30091 | grep title
<title>Welcome to nginx!</title>

```

### 注意

除非应用进一步的安全/网络设置，否则 Lan-to-Pod 示例可能无法直接在 Google Cloud Shell 中工作。这种设置超出了本书的范围。

作为本节的总结，这里提供了`kubectl expose deploy/nginx --type="NodePort" --port=80`命令的声明版本:

```
# serviceNodePort.yaml
kind: Service
apiVersion: v1
metadata:
  name: nginx
spec:
  selector:
    run: nginx
  ports:
  - protocol: TCP
    port: 80
  type: NodePort

```

用于 Pod 到 Pod 访问的 ClusterIP 清单之间的唯一区别是添加了属性`type`并将其设置为`NodePort`。如果未声明，该属性的值默认为`ClusterIP`。

## 互联网到 Pod 连接用例

从互联网访问 pod 包括创建`LoadBalancer`服务类型。`LoadBalancer`服务类型类似于`NodePort`服务类型，因为它将在 Kubernetes 集群的每个节点的离散端口上发布公开的对象。不同的是，除此之外，它还会与谷歌云平台的负载平衡器进行交互，并分配一个可以将流量导向这些端口的公共 IP 地址。

以下示例创建了一个包含三个 Nginx 副本的集群，并将部署公开到 Internet。请注意，最后一个命令使用了`-w`标志，以便等待分配外部公共 IP 地址:

```
$ kubectl run nginx --image=nginx --replicas=3
deployment.apps/nginx created

$ kubectl expose deploy/nginx --type=LoadBalancer \
    --port=80
service/nginx exposed

$ kubectl get services -w
NAME  TYPE         CLUSTER-IP    EXT-IP          nginx LoadBalancer 10.39.249.178 <pending>
nginx LoadBalancer 10.39.249.178 35.189.65.215

```

每当负载平衡器被分配一个公共 IP 地址时，就会填充`service.status.loadBalancer.ingress.ip`属性——或者其他云供应商(如 AWS)的`.hostname`。如果我们想以编程方式捕获公共 IP 地址，我们必须做的就是等待，直到设置了这个属性。我们可以通过 Bash 中的一个 *while 循环*来检测这个解决方案，例如:

```
while [ -z $PUBLIC_IP ]; \
 do PUBLIC_IP=$(kubectl get service/nginx \
 -o jsonpath \
 --template="{.status.loadBalancer.ingress[*].ip}");\
 sleep 1; \
 done; \
 echo $PUBLIC_IP
35.189.65.215

```

`kubectl expose deploy/nginx --type=LoadBalancer --port=80`的声明性版本出现在下一个代码清单中。用于 LAN-to-Pod 用例的清单之间的唯一区别是`type`属性被设置为`LoadBalancer`。使用`kubectl apply -f serviceLoadBalancer.yaml`命令应用清单。在应用这个命令之前，我们可能想通过首先发出`kubectl delete service/nginx`命令来处理任何正在运行的冲突服务。

```
# serviceLoadBalancer.yaml
kind: Service
apiVersion: v1
metadata:
  name: nginx
spec:
  selector:
    run: nginx
  ports:
  - protocol: TCP
    port: 80
  type: LoadBalancer

```

## 访问不同名称空间中的服务

到目前为止，我们看到的所有例子都存在于*默认的*名称空间中。这就好像默认情况下每个`kubectl`命令都添加了`-n default`标志。因此，我们从来不需要关心完整的 DNS 名称。每当我们通过键入`kubectl expose deploy/nginx`来公开 Nginx 部署时，我们都可以从 Pods 访问结果服务，而不需要任何额外的域组件，例如，通过键入`wget http://nginx`。

但是，如果使用了更多的名称空间，事情可能会变得棘手，了解与每个服务相关的完整 DNS 记录的形状可能会很有用。让我们假设在`default`名称空间中有一个`nginx`服务——在这种情况下，不需要指明特定的名称空间，因为这是默认名称空间——在`production`名称空间中有另一个名称相同的服务，如下所示:

```
# nginx in the default namespace

$ kubectl run nginx --image=nginx --port=80
deployment.apps/nginx created

$ kubectl expose deploy/nginx
service/nginx exposed

# nginx in the production namespace

$ kubectl create namespace production
namespace/production created

$ kubectl run nginx --image=nginx --port=80 \
    -n production
deployment.apps/nginx created

$ kubectl expose deploy/nginx -n production
service/nginx exposed

```

结果是两个名为`nginx`的服务存在于不同的名称空间中:

```
$ kubectl get services --all-namespaces | grep nginx
NAMESPACE  NAME  TYPE      CLUSTER-IP      PORT(S)
default    nginx ClusterIP 10.39.243.143   80/TCP
production nginx ClusterIP 10.39.244.112   80/TCP

```

我们是在`10.39.243.143`还是`10.39.244.112`发布 Nginx 服务将取决于请求 Pod 运行的名称空间:

```
$ kubectl run test --rm -ti --image=alpine \
    --restart=Never \
    -- getent hosts nginx | awk '{ print $1 }'
10.39.243.143

$ kubectl run test --rm -ti --image=alpine \
    --restart=Never \
    -n production \
    -- getent hosts nginx | awk '{ print $1 }'
10.39.244.112

```

当使用`nginx`作为主机时，`default`空间中的单元将连接到`10.39.243.143`，而`production`名称空间中的单元将连接到`10.39.244.112`。从`production`到达`default` ClusterIP 的方法是使用完整的域名，反之亦然。

默认配置使用`service-name.namespace.svc.cluster.local`约定，其中`service-name`是`nginx`，在我们的示例中`namespace`是`default`或`production`:

```
$ kubectl run test --rm -ti --image=alpine \
    --restart=Never \
    -- sh -c \
    "getent hosts nginx.default.svc.cluster.local; \
    getent hosts nginx.production.svc.cluster.local"
10.39.243.143     nginx.default.svc.cluster.local
10.39.244.112     nginx.production.svc.cluster.local

```

## 在不同的端口上公开服务

命令及其等价的声明形式将自省目标对象，并在其声明的端口上公开它。如果没有可用的端口信息，那么我们使用`--port`标志或`service.spec.ports.port`属性来指定端口。在我们前面的例子中，暴露的端口总是与实际的 Pod 的端口一致；每当公开的端口不同于发布的端口时，必须使用服务清单中的`--target-port`标志或`service.spec.ports.targetPort`属性来指定。

在下一个例子中，我们像往常一样在端口 80 上创建一个 Nginx 部署，但是在公共负载平衡器的端口`8000`上公开它。请注意，鉴于暴露端口和发布端口不同，我们必须使用`--target-port`标志指定暴露端口:

```
$ kubectl run nginx --image=nginx
deployment.apps/nginx created

$ kubectl expose deploy/nginx --port=8000 \
    --target-port=80 \
    --type=LoadBalancer
service/nginx exposed

$ kubectl get services -w
NAME  TYPE         EXTERNAL-IP   PORT(S)
nginx LoadBalancer <pending>     8000:31937/TCP
nginx LoadBalancer 35.189.65.99  8000:31937/TCP

```

结果是 Nginx 现在可以通过端口 8000 在公共互联网上访问，即使它是在 Pod 级别的端口 80 上公开的:

```
$ curl -s -i http://35.189.65.99:8000 | grep title
<title>Welcome to nginx!</title>

```

为了完整起见，这里我们给出了所呈现的`kubectl expose`命令的声明性等价物；使用`kubectl apply -f serviceLoadBalancerMapped.yaml`命令应用。我们可能需要首先通过运行`kubectl delete service/nginx`来删除使用命令式方法创建的服务:

```
# serviceLoadBalancerMapped.yaml
kind: Service
apiVersion: v1
metadata:
  name: nginx
spec:
  selector:
    run: nginx
  ports:
  - protocol: TCP
    port: 8000
    targetPort: 80
  type: LoadBalancer

```

## 暴露多个端口

一个 Pod 可以公开多个端口，因为它包含多个容器，或者因为一个容器监听多个端口。例如，web 服务器通常在端口 80 上监听常规的未加密流量，在端口 443 上监听 TLS 流量。服务清单中的`spec.ports`属性需要一个端口声明数组，所以我们所要做的就是将更多的元素添加到这个数组中，记住无论何时定义了两个或更多的端口，都必须给每个端口一个惟一的名称，这样它们才能被区分开来:

```
# serviceMultiplePorts.yaml
kind: Service
apiVersion: v1
metadata:
  name: nginx
spec:
  selector:
    run: nginx
  ports:
  - name: http  # user-defined name
    protocol: TCP
    port: 80
    targetPort: 80

  - name: https # user-defined name
    protocol: TCP
    port: 443
    targetPort: 443
  type: LoadBalancer

```

## 金丝雀释放

canary 发布背后的想法是，我们只向一部分用户公开服务的新版本，然后再向整个用户群推广，这样我们可以观察新服务的行为一段时间，直到我们确信它不存在运行时缺陷。

实现该策略的一个简单方法是创建一个服务对象，在 canary 发布期间，该服务对象在其负载平衡集群中包含一个新的 Pod“金丝雀”。例如，假设生产群集在其当前版本中包括 Pod 版本 1.0 的三个副本，我们可以包括 Pod 版本 2.0 的一个实例，以便 1/4 的流量(平均)到达新的 Pod。

这个策略的关键成分是标签和选择器，我们已经在第 [2](2.html) 章中介绍过了。我们所要做的就是为将要投入生产的 pod 添加一个标签，并在服务对象中添加一个匹配的选择器。这样，我们可以预先创建服务对象，并让 pod 声明一个标签，使它们被服务对象自动选择。这在行动中比在言语中更容易看到；让我们一步一步地遵循这个过程。

我们首先创建一个服务清单，其选择器将寻找标签`prod`等于`true`的 pod:

```
# myservice.yaml
kind: Service
apiVersion: v1
metadata:
  name: myservice
spec:
  selector:
    prod: "true"
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: LoadBalancer

```

应用清单后，我们可以保持第二个窗口打开，在该窗口中，我们将交互地查看哪些端点加入和离开集群:

```
$ kubectl apply -f myservice.yaml
service/myservice created

$ kubectl get endpoints/myservice -w
NAME        ENDPOINTS   AGE
myservice   <none>      29m

```

假设我们还没有创建任何 Pods，集群中没有端点，这可以从`ENDPOINTS`列下的值`<none>`得到证明。让我们创建由三个副本组成的“现任”v1 生产部署:

```
$ kubectl run v1 --image=nginx --port=80 \
    --replicas=3 --labels="prod=true"
deployment.apps/v1 created

```

如果我们检查让`kubectl get endpoints/myservice -w`运行的终端窗口，我们会注意到将添加三个新的端点。例如:

```
$ kubectl get endpoints/myservice -w
NAME      ENDPOINTS
myservice 10.36.2.10:80
myservice <none>
myservice 10.36.2.11:80
myservice 10.36.0.6:80,10.36.2.11:80
myservice 10.36.0.6:80,10.36.1.8:80,10.36.2.11:80

```

由于我们已经请求了一个外部 IP，我们可以使用`curl`来检查我们的 v1 服务是否可操作:

```
$ kubectl get service/myservice
NAME      TYPE         EXTERNAL-IP   PORT(S)
myservice LoadBalancer 35.197.192.45 80:30385/TCP

$ curl -I -s http://35.197.192.45 | grep Server
Server: nginx/1.13.8

```

现在是时候介绍金丝雀豆荚了。让我们创建一个 v2 部署。不同之处在于标签`prod`被设置为`false`，并且我们将使用 Apache 服务器而不是 Nginx 作为新版本的容器映像:

```
$ kubectl run v2 --image=httpd --port=80 \
    --replicas=3 --labels="prod=false"
deployment.apps/v2 created

```

截至目前，我们可以看到总共有六个 Pod 副本。`-L <LABEL>`显示标签的值:

```
$ kubectl get pods -L prod
NAME                  READY  STATUS   RESTARTS  PROD
v1-3781799777-219m3   1/1    Running  0         true
v1-3781799777-qc29z   1/1    Running  0         true
v1-3781799777-tbj4f   1/1    Running  0         true
v2-3597628489-2kl05   1/1    Running  0         false
v2-3597628489-p8jcv   1/1    Running  0         false
v2-3597628489-zc95w   1/1    Running  0         false

```

为了让 v2 Pods 之一进入`myservice`集群，我们所要做的就是相应地设置标签。这里我们选择名为`v2-3597628489-2kl05`的 Pod，并将其`prod`标签设置为`true`:

```
$ kubectl label pod/v2-3597628489-2kl05 \
    prod=true --overwrite
pod "v2-3597628489-2kl05" labeled

```

在标签操作之后，如果我们检查运行命令`kubectl get endpoints/myservice -w`的窗口，我们将看到一个额外的端点被添加。此时，如果我们反复点击公共 IP 地址，我们会注意到一些请求到达了 Apache web 服务器:

```
$ while true ; do curl -I -s http://35.197.192.45 \
    | grep Server ; done
Server: nginx/1.13.8
Server: nginx/1.13.8
Server: nginx/1.13.8
Server: nginx/1.13.8
Server: Apache/2.4.29 (Unix)
Server: nginx/1.13.8
Server: nginx/1.13.8
...

```

一旦我们对 v2 的行为感到满意，我们就可以将 v2 的其余部分投入生产。如前所示，这可以通过贴标签逐步实现；然而，此时，最好创建一个正式的部署清单，这样一旦应用，Kubernetes 就会以无缝的方式引入 v2 Pods 并淘汰 v2 Pods 请参考第 [3](3.html) 章了解有关滚动和蓝/绿部署的更多详细信息。

总结这一节，标签和选择器的组合为我们提供了向服务消费者公开哪些 pod 的灵活性。一个实际应用是金丝雀释放的仪器。

## Canary 版本和不一致的版本

canary 版本可能包括内部代码增强或错误修复，但也可能向用户引入新功能。这样的新特征可能涉及视觉用户界面本身(例如，HTML 和 CSS)以及支持这样的界面的数据 API。每当是后者的情况时，每个请求可能落在任何随机 Pod 上的事实可能是有问题的。例如，第一个请求可以检索依赖于 v2 REST API 的新的 v2 AngularJS 代码，但是当第二个请求命中负载平衡器时，所选择的 Pod 可以是 v1，并且提供这种所述 API 的不正确版本。本质上，当 canary 版本引入外部变化时——无论是 UI 还是数据方面——我们通常希望用户保持相同的版本，无论是当前版本还是 canary 版本。

用户登陆同一个服务实例的技术术语是*粘性会话*或*会话相似性*——后者是 Kubernetes 使用的。根据有多少数据可用于识别单个用户，有无数种实现会话相似性的方法。例如，附加到 URL 上的 cookies 或会话标识符可以用在 web 应用程序的场景中，但是如果接口是协议缓冲区或 Thrift 而不是 HTTP 呢？唯一能够从另一个用户中识别出一个给定用户的细节是他们的客户端 IP 地址，这正是服务对象可以用来实现这个行为的。

默认情况下，会话关联性是禁用的。在命令式上下文中实现会话亲缘关系只是简单地将`--session-affinity=ClientIP`标志添加到`kubectl expose`命令中。例如:

```
# Assume there is a Nginx Deployment running
$ kubectl expose deploy/nginx --type=LoadBalancer \
    --session-affinity=ClientIP
service "nginx" exposed

```

声明性版本包括设置`service.spec.sessionAffinity`属性和应用运行`kubectl apply -f serviceSessionAffinity.yaml`命令的清单:

```
# serviceSessionAffinity.yaml
kind: Service
apiVersion: v1
metadata:
  name: nginx
spec:
  sessionAffinity: ClientIP
  selector:
    run: nginx
  ports:
  - protocol: TCP
    port: 80
  type: LoadBalancer

```

基于 IP 的会话相似性的限制是多个用户可能共享同一个 IP 地址，这在公司和学校中是典型的情况。同样，同一个逻辑用户可能看起来来自多个 IP 地址，就像用户在家中使用他们的 Wi-Fi 宽带路由器并通过他们的智能手机使用 LTE 或类似技术观看网飞的情况一样。对于这样的场景，服务的能力是不够的；因此，最好使用具有第 7 层自检能力的服务，如*入口*控制器。更多信息请参考 [`https://kubernetes.io/docs/concepts/services-networking/ingress/`](https://kubernetes.io/docs/concepts/services-networking/ingress/) 。

## 正常启动和关闭

受益于宽限启动属性的应用程序仅在其自举过程完成后才准备好接受用户请求，而宽限关闭意味着应用程序不应突然停止，从而破坏其用户或其依赖关系的完整性。换句话说，应用程序应该能够告诉“我已经喝了咖啡，洗了澡，我准备好工作了”以及“我今天到此为止了”。我现在要去刷牙睡觉了。”

服务控制器使用两种主要机制来决定给定的 Pod 是否应该成为其集群的一部分:Pod 的标签和 Pod 的就绪状态，这是使用就绪探测器实现的(参见第 [2](2.html) 章)。例如，在基于 Spring 的 Java 应用程序中，从应用程序启动到 Spring Boot 框架完全初始化并准备好接受 http 请求之间有几秒钟的延迟。

## 零停机部署

Kubernetes 的部署控制器通过向服务控制器注册新的 Pod 并以协调的方式删除旧的 Pod 来实现零停机部署，以便最终用户始终获得最少数量的 Pod 副本。如第 [3](3.html) 章所述，零停机部署可以实施为滚动部署或蓝/绿部署。

只需几个步骤就能看到零停机部署的实际效果。首先，我们创建一个常规的 Nginx 部署对象，并通过公共负载平衡器公开它。我们设置了`--session-affinity=ClientIP`,以便消费客户端在准备就绪后可以无缝过渡到新的升级 Pod:

```
$ kubectl run site --image=nginx --replicas=3
deployment.apps/site created

$ kubectl expose deploy/site \
    --port=80 --session-affinity=ClientIP \
    --type=LoadBalancer
service/site exposed

# Confirm public IP address
$ kubectl get services -w
NAME    TYPE          EXTERNAL-IP      PORT(S)
site    LoadBalancer  35.197.210.194   80:30534/TCP

```

然后，我们打开一个单独的终端窗口，让一个简单的 http 客户端在无限循环中运行:

```
$ while true ; do curl -s -I http://35.197.210.194/ \
    | grep Server; sleep 1 ; done
Server: nginx/1.17.1
Server: nginx/1.17.1
Server: nginx/1.17.1
...

```

现在我们所要做的就是将`deploy/site`转换到一个新的部署，这可以通过简单地改变它的底层容器映像来实现。让我们使用来自 Docker Hub 的 Apache HTTPD 映像:

```
$ kubectl set image deploy/site site=httpd
deployment.extensions/site image updated

```

如果我们返回到运行示例客户端的窗口，我们将看到很快迎接我们的将是 Apache HTTPD 服务器，而不是 Nginx:

```
Server: nginx/1.17.1
Server: nginx/1.17.1
Server: nginx/1.17.1
Server: Apache/2.4.39 (Unix)
Server: Apache/2.4.39 (Unix)
Server: Apache/2.4.39 (Unix)
...

```

使用`kubectl get pod -w`命令打开另一个平行窗口来监视 Pod 活动也很有趣，这样我们就可以观察新的 Pod 是如何启动的，旧的 Pod 是如何被终止的。

## pod 的端点

在大多数情况下，服务控制器的角色是为两个或更多 pod 提供单个端点。但是，在某些情况下，我们可能希望确定服务控制器选择的那些 pod 的特定端点。

`kubectl get endpoints/<SERVICE-NAME>`命令直接在屏幕上显示多达三个端点，用于即时调试。例如:

```
$ kubectl get endpoints/nginx -o wide
NAME      ENDPOINTS
nginx     10.4.0.6:80,10.4.1.6:80,10.4.2.6:80

```

和我们之前看到的一样，可以使用`kubectl describe service/<SERVICE-NAME>`命令检索相同的信息。如果我们想要一种更程序化的方法，允许我们对端点的数量和值的变化做出反应，我们可以使用 JSONPath 查询。例如:

```
$ kubectl get endpoints/nginx -o jsonpath \
    --template= "{.subsets[*].addresses[*].ip}"
10.4.0.6 10.4.1.6 10.4.2.6

```

## 管理摘要

正如我们已经看到的，服务可以使用`kubectl expose`命令强制创建，也可以使用清单文件声明创建。使用`kubectl get services`命令列出服务，使用`kubectl delete service/<SERVICE-NAME>`命令删除服务。例如:

```
$ kubectl get services
NAME        TYPE         CLUSTER-IP   EXTERNAL-IP
kubernetes  ClusterIP    10.7.240.1   <none>
nginx       LoadBalancer 10.7.241.102 35.186.156.253

$ kubectl delete services/nginx
service "nginx" deleted

```

请注意，当服务被删除时，底层部署仍将继续运行，因为它们都有独立的生命周期。

## 摘要

在本章中，我们了解到服务控制器有助于在服务消费者和 pod 之间创建一个间接层，以便于工具化属性，如自我修复、金丝雀释放、负载平衡、正常启动和关闭以及零停机部署。

我们讨论了具体的连接用例，如 Pod 到 Pod、LAN 到 Pod 和 Internet 到 Pod，并看到后者特别有用，因为它允许使用公共 IP 地址访问我们的应用程序。

我们还解释了如何使用完整的 DNS 记录来消除跨不同名称空间的冲突服务，以及在声明多个端口时命名端口的需要。