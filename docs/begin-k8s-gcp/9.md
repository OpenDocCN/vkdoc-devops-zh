# 九、状态集

可以说，十二因素应用方法是云原生应用最广泛的原则之一。称为“后勤服务”的因素 IV 表示后勤服务应被视为附属资源。其中一段，在 [`https://12factor.net/backing-services`](https://12factor.net/backing-services) 处，写着:

> 十二要素应用的代码没有区分本地服务和第三方服务。对于应用来说，两者都是附加的资源，可以通过 URL 或存储在配置中的其他定位器/凭证来访问。十二因素应用的部署应该能够在不改变应用代码的情况下，将本地 MySQL 数据库与第三方(如 Amazon RDS)管理的数据库进行交换。同样，本地 SMTP 服务器可以与第三方 SMTP 服务(如邮戳)交换，而无需更改代码。在这两种情况下，只有配置中的资源句柄需要更改。

十二因素应用方法没有说明交付*我们自己的*支持服务的最佳实践，因为它假设应用将在一个无状态的 PaaS(如 Heroku、Cloud Foundry 或 Google App Engine)上运行，这些都是完全托管的服务。如果我们需要实现自己的后台服务，而不是依赖于，比如说，Google Bigtable，会怎么样？Kubernetes 实现后台服务的答案是 StatefulSet 控制器。

后台服务与无状态的十二因素应用有着不同的动态。缩放不是一件小事；“向下”扩展可能会导致数据丢失，而向上扩展可能会导致现有群集的不适当复制或重新共享。一些后台服务根本不打算扩展，至少不能自动扩展。

后台服务在如何实现高可伸缩性方面也有很大的不同。有些使用管理-工作(主-从)策略(例如 MySQL 和 MongoDB)，而有些使用多主架构，例如 Memcached 和 Cassandra。Kubernetes 中的 StatefulSet 控制器不能对每个数据存储的性质做出宽泛的假设；因此，它侧重于底层的、原始的属性，例如稳定的网络身份，这些属性可以根据离散的问题或手头需要的属性，有选择地帮助实现它们。

在这一章中，我们将从头开始构建一个原始的键/值数据存储，它将用于内部化实现 StatefulSets 的原则，而没有遗漏可能不适用于单个特定产品(如 MySQL 或 MongoDB)的细节的风险。随着本章的深入，我们将丰富所述原始键/值数据存储。在第一部分中，我们将介绍顺序 Pod 创建、稳定的网络身份和使用无头服务的原则——后者是发布后台服务的关键。然后，我们将查看 Pod 生命周期事件，我们可以利用这些事件来实现正常的启动和关闭功能。最后，我们将展示如何实现基于存储的持久性，这也是*有状态性*的最终目的。

## 原始键/值存储

我们将要看到的可能被认为是穷人的 Memcached 或 BerkleyDB。这个键/值存储只执行三项任务:保存键/值对，通过唯一键查找和检索值，以及列出所有现有键。密钥作为常规文件保存在文件系统中，其中文件名是密钥，其内容是值。没有输入验证、删除功能和任何种类的安全措施。前面提到的三个函数( *save* 、 *load、*和 *allKeys* )分别使用 Python 3:

```
#!/usr/bin/python3
# server.py
from flask import Flask
import os
import sys

if len(sys.argv) < 3:
  print("server.py PORT DATA_DIR")
  sys.exit(1)

app     = Flask(__name__)
port    = sys.argv[1]
dataDir = sys.argv[2] + '/'

@app.route('/save/<key>/<word>')
def save(key, word):
  with open(dataDir + key, 'w') as f:
       f.write(word)
  return word

@app.route('/load/<key>')
def load(key):
  try:
    with open(dataDir + key) as f:
      return f.read()
  except FileNotFoundError:
      return "_key_not_found_"

@app.route('/allKeys')
def allKeys():
  keys = ".join(map(lambda x: x + ",",
         filter(lambda f:
                os.path.isfile(dataDir+'/'+f),
                os.listdir(dataDir)))).rstrip(',')
  return keys

if __name__ == '__main__':
  app.run(host='0.0.0.0', port=port)

```

请注意，在本章的文件夹下找到的实际文件`server.py`具有额外的特性(换句话说就是代码),这些特性在给出的清单中被省略了。所述省略的特征有助于处理平稳的启动和关闭，并且将在前面的几个部分中讨论。

要在本地试验服务器，我们可以首先安装 Flask，然后通过传递端口号和数据目录作为参数来启动服务器:

```
$ sudo pip3 install Flask
$ mkdir -p /tmp/data
$ ./server.py 1080 /tmp/data

```

一旦服务器启动并运行，我们就可以通过插入和检索一些键/值对来“玩”它:

```
$ curl http://localhost:1080/save/title/Sapiens
Sapiens
$ curl http://localhost:1080/save/author/Yuval
Yuval
$ curl http://localhost:1080/allKeys
author,title
$ curl http://localhost:1080/load/title
Sapiens
$ curl http://localhost:1080/load/author
Yuval

```

## 最小状态集清单

在上一节中，我们已经介绍了用 Python 编写的基于键/值存储 HTTP 的服务器，现在我们将使用 StatefulSet 控制器运行它。

最小 StatefulSet 清单在很大程度上类似于部署清单:它允许定义副本的数量、具有一个或多个容器的 Pod 模板等等:

```
# wip/server.yaml
# Minimal manifest for running server.py
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: server
spec:
  selector:
    matchLabels:
      app: server
  serviceName: server
  replicas: 3
  template:
    metadata:
      labels:
        app: server
    spec:
      containers:
      - name: server
        image: python:alpine
        args:
        - bin/sh
        - -c
        - >-
          pip install flask;
          python -u /var/scripts/server.py 80
          /var/data
        ports:
        - containerPort: 80
        volumeMounts:
          - name: scripts
            mountPath: /var/scripts
          - name: data
            mountPath: /var/data
      volumes:
        - name: scripts
          configMap:
            name: scripts
        - name: data
          emptyDir:
            medium: Memory

```

我们使用的不是容器化的`server.py`，而是包含 Python 3 解释器的现成 Docker 映像`python:alpine`。文件`server.py`必须“上传”为名为`scripts`的配置图，其设置如下:

```
#!/bin/sh
# wip/configmap.sh
kubectl delete configmap scripts \
  --ignore-not-found=true
kubectl create configmap scripts \
  --from-file=../server.py

```

另外，请注意使用名为`data`的卷，该卷使用 RAM 内存设置为类型为`emptyData`的卷。这意味着我们的键/值存储目前作为内存中的缓存工作，而不是在服务器崩溃后仍然存在的持久性存储。我们将很快详细阐述这方面的内容。

现在，我们已经拥有了将键/值存储作为有状态集合运行所需的一切:

```
$ cd wip
$ ./configmap.sh
$ kubectl apply -f server.yaml
statefulset.apps/server created

```

当我们用`kubectl get pods`列出结果 Pod 时，我们可以注意到，与部署不同，Pod 名称遵循一个连续的顺序，从`0,`开始，而不是有一个随机的后缀。我们将在下一节讨论顺序 Pod 创建属性:

```
$ kubectl get pods
NAME       READY     STATUS    RESTARTS   AGE
server-0   1/1       Running   0          5s
server-1   1/1       Running   0          0s
server-2   0/1       Pending   0          0s

```

为了证明键/值存储服务器工作正常，我们可以用其中一个 pod 建立一个代理，设置一个键/值对，然后检索它的值:

```
$ kubectl port-forward server-0 1080:80
Forwarding from 127.0.0.1:1080 -> 80
...

# Set a key/value pair
$ curl http://localhost:1080/save/title/Sapiens
Sapiens

# Retrieve the value for the title key
$ curl http://localhost:1080/load/title
Sapiens

```

如果`kubectl port-forward`报告了一个错误，比如`bind: address already in use`，这意味着我们让`server.py`在端口 1080 上运行，或者其他进程正在使用这个端口。如果源代码中的端口碰巧被永久地分配给了某个其他应用，读者可能会更改该端口。

## 顺序 Pod 创建

默认情况下，部署控制器并行创建所有的 pod(除非升级时可能会应用`.maxSurge`约束),以加速该过程。相反，StatefulSet 控制器按顺序创建 pod，从`0`开始，直到定义的副本数减一。该行为由`statefulset.spec.podManagementPolicy`属性控制，其默认值为`OrderedReady`。另一个可能的值是`Parallel,`，它产生与部署和复制集控制器相同的行为。

在前面的部分中，我们可以通过在应用`kubectl apply -f server.yaml`之前运行`kubectl get pods -w`来看到连续的 Pod 创建过程:

```
$ kubectl get pods -w
NAME       READY  STATUS
server-0   0/1    Pending
server-0   0/1    ContainerCreating
server-0   1/1    Running
server-1   0/1    Pending
server-1   0/1    ContainerCreating
server-1   1/1    Running
server-2   0/1    Pending
server-2   0/1    ContainerCreating
server-2   1/1    Running

```

为什么顺序 Pod 创建很重要？因为后台服务通常具有依赖于可靠假设的语义，这些假设是关于先前已经创建了哪些确切的 pod 以及接下来将创建哪些 pod:

*   在基于管理人员-工作人员范例的后备存储中，比如 MongoDB 或 MySQL，工作人员 pod pod-1 和 pod-2 可能希望首先定义 pod-0(管理人员),以便可以向它注册。

*   Pod 创建序列可以包括按比例增加现有的集群，其中数据可以从 pod-0 复制到 pod-1、pod-2 和 pod-3。

*   pod 删除序列可能需要逐个注销工作线程。

正如在最后一点中提到的，顺序 Pod 创建的属性也反向工作:具有最高索引的 Pod 总是首先终止。例如，通过发出`kubectl scale statefulset/server --replicas=0`将键/存储集群减少到 0 个副本会导致以下行为:

```
$ kubectl get pods -w
NAME       READY     STATUS
server-0   1/1       Running
server-1   1/1       Running
server-2   1/1       Running
server-2   1/1       Terminating
server-2   0/1       Terminating
server-1   1/1       Terminating
server-1   0/1       Terminating
server-0   1/1       Terminating
server-0   0/1       Terminating

```

## 稳定的网络身份

在无状态应用中，每个副本的特定身份和位置是短暂的。只要我们能够到达负载均衡器，哪个特定的副本服务于我们的请求并不重要。对于后台服务来说，情况不一定如此，比如我们的原始键/值存储或 Memcached 之类的服务。许多多主存储在不产生中心争用点的情况下解决规模问题的方法是，让每个客户机(或等效的委托代理)知道每个服务器主机，以便客户机自己决定在哪里存储和检索数据。

因此，在 StatefulSets 的情况下，根据数据存储的水平扩展策略，对于客户端来说，确切地知道它们已经将数据保存到的 Pod 可能是至关重要的，以便在扩展、重启和失败事件之后，加载请求总是与用于原始保存请求的原始 Pod 相匹配。

例如，如果我们将键`title`设置在 Pod `server-0`上，我们知道我们可以稍后返回并从完全相同的 Pod 中检索它。相反，如果 Pod 由常规部署控制器管理，那么 Pod 将被赋予一个随机的名称，例如`server-1539155708-55dqs`或`server-1539155708-pkj2w`。即使客户端可以记住这样的随机名称，也不能保证存储的键/值对在删除或放大/缩小事件后仍然存在。

稳定网络身份的属性对于应用允许跨多个计算和数据资源扩展数据的分片机制至关重要。分片意味着给定的数据集被分解成块，每个块最终位于不同的服务器上。根据手头数据的类型和更均匀分布的字段或属性，将数据集分成块的标准可能会有所不同；例如，对于联系人实体，名字和姓氏是很好的属性，而性别则不是。

让我们假设我们的数据集由关键字`a`、`b`、`c`和`d`组成。我们如何在三台服务器上平均分配每封信？最简单的解决方案是应用模运算。通过获取每个字母的 ASCII 十进制代码并获取服务器数量的模，我们获得了一个廉价的分片解决方案，如表 [9-1](#Tab1) 所示。

表 9-1

将模运算符应用于 ASCII 字母

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

钥匙

 | 

小数

 | 

以…为模

 | 

计算机网络服务器

 |
| --- | --- | --- | --- |
| a | Ninety-seven | 97 % 3 = 1 | 服务器-1 |
| b | Ninety-eight | 98 % 3 = 2 | 服务器-2 |
| c | Ninety-nine | 99 % 3 = 0 | 服务器-0 |
| d | One hundred | 100 % 3 = 1 | 服务器-1 |

Cassandra 或 Memached 等生产级后备存储中的实际哈希算法将更加复杂，并使用一致的哈希算法——这样，当添加或删除新服务器时，总的来说，密钥不会位于不同的服务器上——但基本原理是相同的。

这里的关键见解是，客户端要求服务器有一个稳定的网络身份，因为它们将为每个服务器分配一个密钥子集。这正是区分 StatefulSets 和 Deployments 的关键特性之一。

## 无头服务

如果客户机首先无法到达服务器单元，那么稳定的网络身份就没有多大用处。客户端对 StatefulSet 控制的 Pod 的访问不同于适用于部署控制的 Pod 的访问，因为跨随机 Pod 实例的负载平衡是不合适的；客户需要直接访问离散单元。然而，Pod 将到达的具体节点和 IP 地址只能在运行时确定，因此发现机制仍然是必要的。

在 StatefulSets 的情况下使用的解决方案仍然在于服务控制器(第 [4 章](4.html))，除了它被配置为提供所谓的*无头服务*。无头服务只提供一个 DNS 条目和代理，而不是一个负载均衡器，它是使用常规服务清单设置的，除了`service.spec.clusterIP`属性被设置为`None`:

```
# wip/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: server
  labels:
    app: server
spec:
  ports:
  - port: 80
  clusterIP: None
  selector:
    app: server

```

让我们应用如下的`service.yaml`清单:

```
# Assume wip/server.yaml has been applied first

$ cd wip
$ kubectl apply -f service.yaml
service/server created

```

为`server`服务创建的 DNS 条目将为每个正在运行并准备就绪的 Pod 提供一个 DNS SRV 记录。我们可以使用`nslookup`命令来获得这样的 DNS SRV 记录:

```
$ kubectl run --image=alpine --restart=Never \
    --rm -i test \
    -- nslookup server

10.36.1.7 server-0.server.default.svc.cluster.local
10.36.1.8 server-2.server.default.svc.cluster.local
10.36.2.7 server-1.server.default.svc.cluster.local

```

## 我们的原始键/值存储的智能客户端

到目前为止，我们已经使用 StatefulSet 控制器成功运行了键/值存储的多个副本，并使用 headless 服务使感兴趣的消费者应用可以访问每个 Pod 端点。我们以前也解释过，可伸缩性是由客户机管理的，而不是由服务器本身管理的——在多主范例中，我们选择在本章中探讨。现在，让我们创建一个智能客户端，它允许我们进一步了解 StatefulSet 的行为。

在最后展示整个源代码之前，我们将描述我们的客户端的关键方面。客户端需要了解的第一件事是它将与之交互的服务器的确切集合，以及它是否应该以只读模式运行:

```
if len(sys.argv) < 2:
  print('client.py SRV_1[:PORT],SRV_2[:PORT],' +
        '... [readonly]')
  sys.exit(1)

# Process input arguments
servers  = sys.argv[1].split(',')
readonly = (True if len(sys.argv) >= 3
            and sys.argv[2] == 'readonly' else False)

```

三副本服务器的典型调用如下——不要运行此示例；这仅用于说明:

```
./server.py \
  server-0.server,server-1.server,server-2.server

```

我们的客户端保存了一组由定义数量的服务器上的英文字母组成的密钥，现在存储在`servers`变量中。当客户端启动时，它将首先打印由虚线下划线的完整字母表，以及通过获取每个密钥的 ASCII 码的模而被选择来存储每个密钥的服务器号:

```
# Print alphabet and selected server for each letter
sn = len(servers)
print(' ' * 20 + string.ascii_lowercase)
print(' ' * 20 + '-' * 26)
print(' ' * 20 + ".join(
                   map(lambda c: str(ord(c) % sn),
                   string.ascii_lowercase)))
print(' ' * 20 + '-' * 26)

```

如果三个服务器被定义为参数，这个代码片段将产生以下输出，这些参数又变成变量`servers`中的一个列表:

```
                    abcdefghijklmnopqrstuvwxyz
                    --------------------------
                    12012012012012012012012012
                    --------------------------

```

让字母表中的每个字母与数字 0、1 或 2 垂直对齐的含义是，按键将按照表 [9-2](#Tab2) 所示进行分配。

表 9-2

字母表字母和指定的服务器

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

键

 | 

计算机网络服务器

 |
| --- | --- |
| `[c,f,i,l,o,r,u,x]` | 服务器-0 .服务器 |
| `[a,d,g,j,m,p,s,v,y]` | 服务器-1 .服务器 |
| `[b,e,h,k,n,q,t,w,z]` | 服务器-2 .服务器 |

现在，在初始化之后，客户端将在一个循环中运行，检查是否在匹配的服务器中找到了每个密钥，如果没有，它将尝试插入它，除非它以只读模式运行:

```
# Iterate through the alphabet repeatedly
while True:
  print(str(datetime.datetime.now())[:19] + ' ',
        end=")
  hits = 0
  for c in string.ascii_lowercase:
    server = servers[ord(c) % sn]
    try:
      r = curl('http://' + server + '/load/' + c)
      # Key found and value match
      if r == c:
        hits = hits + 1
        print('h',end=")
      # Key not found
      elif r == '_key_not_found_':
        if readonly:
          print('m',end=")
        else:
          # Save Key/Value (not read only)
          r = curl('http://' + server +
                   '/save/' + c + '/' + c)
          print('w',end=")
      # Value mismatch
      else:
        print('x',end=")
    except urllib.error.HTTPError as e:
          print(str(e.getcode())[0],end=")
    except urllib.error.URLError as e:
          print('.',end=")
  print(' | hits = {} ({:.0f}%)'
        .format(hits,hits/0.26))
  time.sleep(2)

```

每个键的结果将使用一个状态字母显示。以下是第一次运行客户端的示例，假设 StatefulSet 及其 headless 服务已启动并正在运行:

```
      abcdefghijklmnopqrstuvwxyz
      --------------------------
      12012012012012012012012012
      --------------------------
03:51 wwwwwwwwwwwwwwwwwwwwwwwwww | hits = 0 (0%)
03:53 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
03:55 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
...

```

表 [9-3](#Tab3) 给出了字母表中每个字母的含义，包括输出中显示的`w`和`h`。

表 9-3

字母表的字母栏下每个状态字母的含义

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

信

 | 

描述

 |
| --- | --- |
| `w` | 写:在服务器中没有找到密钥，所以保存了它。 |
| `h` | 命中:在服务器中找到了密钥。 |
| `m` | Miss:未找到密钥，不会保存(只读)。 |
| `x` | 异常:找到了键，但它与其值不匹配。 |
| `.` | 服务器不可访问或网络故障。 |
| `0-9` | 返回了 HTTP 服务器错误。例如，503 就是 5。 |

在仔细检查了每个代码片段之后，我们现在呈现完整的客户端 Python 脚本:

```
#!/usr/bin/python3
# client.py
import string
import time
import sys
import urllib.request
import urllib.error
import datetime

if len(sys.argv) < 2:
  print('client.py SRV_1[:PORT],SRV_2[:PORT],' +
        '... [readonly]')
  sys.exit(1)

# Process input arguments
servers  = sys.argv[1].split(',')
readonly = (True if len(sys.argv) >= 3
            and sys.argv[2] == 'readonly' else False)

# Remove boilerplate from HTTP calls
def curl(url):
  return urllib.request.urlopen(url).read().decode()

# Print alphabet and selected server for each letter
sn = len(servers)
print(' ' * 20 + string.ascii_lowercase)
print(' ' * 20 + '-' * 26)
print(' ' * 20 + ".join(
                   map(lambda c: str(ord(c) % sn),
                       string.ascii_lowercase)))
print(' ' * 20 + '-' * 26)

# Iterate through the alphabet repeatedly
while True:
  print(str(datetime.datetime.now())[:19] + ' ',
        end=")
  hits = 0
  for c in string.ascii_lowercase:
    server = servers[ord(c) % sn]
    try:
      r = curl('http://' + server + '/load/' + c)
      # Key found and value match
      if r == c:
        hits = hits + 1
        print('h',end=")
      # Key not found
      elif r == '_key_not_found_':
        if readonly:
          print('m',end=")
        else:
          # Save Key/Value (not read only)
          r = curl('http://' + server +
                   '/save/' + c + '/' + c)
          print('w',end=")
      # Value mismatch
      else:
        print('x',end=")
    except urllib.error.HTTPError as e:
          print(str(e.getcode())[0],end=")
    except urllib.error.URLError as e:
          print('.',end=")
  print(' | hits = {} ({:.0f}%)'
        .format(hits,hits/0.26))
  time.sleep(2)

```

除了我们之前定义的`server.py`，我们还必须将`client.py`添加到名为`scripts`的配置图中。因此，我们定义了一个名为`wip/configmap2.sh` `:`的新文件

```
#!/bin/sh
# wip/configmap2.sh
kubectl delete configmap scripts \
  --ignore-not-found=true
kubectl create configmap scripts \
  --from-file=server.py --from-file=../client.py

```

最后，我们需要一个 Pod 清单来运行客户端，并为它提供预期的状态集 Pod 名称:

```
# client.yaml
apiVersion: v1
kind: Pod
metadata:
  name: client
spec:
  restartPolicy: Never
  containers:
    - name: client
      image: python:alpine
      args:
      - bin/sh
      - -c
      - "python -u /var/scripts/client.py
        server-0.server,server-1.server,\
        server-2.server"
      volumeMounts:
        - name: scripts
          mountPath: /var/scripts
  volumes:
    - name: scripts
      configMap:
        name: scripts

```

作为最后一个实验，从零开始重置我们的环境是很有趣的，首先启动客户端，并在没有`servers`statefullset 的的情况下观察它的初始行为*:*

```
# Clean up the environment first

$ cd wip
$ ./configmap2.sh
configmap "scripts" deleted
configmap/scripts created

# Run the client
$ kubectl apply -f ../client.yaml
pod/client created

# Query the client's logs
$ kubectl logs -f client

      abcdefghijklmnopqrstuvwxyz
      --------------------------
      12012012012012012012012012
      --------------------------
00:21 .......................... | hits = 0 (0%)
00:24 .......................... | hits = 0 (0%)
00:26 .......................... | hits = 0 (0%)
00:28 .......................... | hits = 0 (0%)

```

`.`(点)字符表示没有可用于任何键的服务器。让我们继续启动服务器及其关联的 headless 服务，同时在一个单独的窗口中查看客户端的日志:

```
# Note: we are still under the wip directory

$ kubectl apply -f server.yaml
statefulset.apps/server created

$ kubectl apply -f service.yaml
service/server created

```

客户端将自动开始将密钥(`w`)保存到出现的每个服务器:

```
      abcdefghijklmnopqrstuvwxyz
      --------------------------
      12012012012012012012012012
      --------------------------
...   ...                          ...
04:14 .......................... | hits = 0 (0%)
04:18 .......................... | hits = 0 (0%)
04:21 .......................... | hits = 0 (0%)
04:23 w..w..w..w..w..w..w..w..w. | hits = 0 (0%)
04:25 h..h..h..h..h..h..h..h..h. | hits = 9 (35%)
04:27 h.wh.wh.wh.wh.wh.wh.wh.wh. | hits = 9 (35%)
04:29 hwhhwhhwhhwhhwhhwhhwhhwhhw | hits = 17 (65%)
04:31 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
04:33 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)

```

这是如何逐行解释发生的事情:

```
04:23 w..w..w..w..w..w..w..w..w. | hits = 0 (0%)

```

服务器`server-1.server`第一个出现，这导致密钥`[a,d,g,j,m,p,s,v,y]`被保存(`w`)到其中。服务器`server-0.server`和`server-2.server`还无法访问:

```
04:25 h..h..h..h..h..h..h..h..h. | hits = 9 (35%)

```

服务器`server-1.server`已经包含导致命中的密钥`[a,d,g,j,m,p,s,v,y]`(`h`)。服务器`server-0.server`和`server-2.server`还不能访问。

```
04:27 h.wh.wh.wh.wh.wh.wh.wh.wh. | hits = 9 (35%)

```

现在服务器`server-2.server`已经启动，密钥`[b,e,h,k,n,q,t,w,z]`已经保存到其中。现在只有`server-0.server`还无法进入；

```
04:29 hwhhwhhwhhwhhwhhwhhwhhwhhw | hits = 17 (65%)

```

服务器`server-0`终于启动了，密钥`[c,f,i,l,o,r,u,x]`已经保存到其中:

```
04:31 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)

```

密钥集现在分布在所有三台服务器上。

请注意，这个例子似乎违反了顺序创建 Pod 的原则。如果我们使用`kubectl get pods -w`观察 Pod 创建行为，我们将看到该原理仍然适用。但是，如果 Pods 启动得足够快，就绪探测变为活动状态的时间加上 DNS 缓存，可能会导致从客户端角度看起来无序的行为。如果要求客户端自己体验有保证的有序 Pod 创建，那么我们需要增加一些延迟，以允许就绪性探测器开始工作，并允许刷新和/或刷新 DNS 缓存。

## 控制后备存储器的创建和终止

正如我们在本章的介绍中所解释的，StatefulSet 控制器不能对后备存储的具体性质做出宽泛的假设，因为每个高可伸缩性和可用性范例以及每个产品的技术设计和约束(例如，MySQL 与 MongoDB)所采取的选择会导致大量的可能性。例如:

*   向主管登记员工

*   向控制器(如 ZooKeeper)注册副本

*   与工人一起复制主数据集

*   在将先行副本标记为*就绪*之前，等待集群的最后一个成员启动并运行

*   领导人改选(并向客户公布选举结果)

*   重新计算哈希值

话虽如此，我们可以对 StatefulSet 的生命周期进行推理，并理解 Kubernetes 管理员拥有哪些*机会*来实施控制。StatefulSet 控制器给我们施加所述控制的主要烹饪成分是

1.  保证 Pod 的创建和终止是有序的，并且它们的身份是可预测的。例如，如果`$HOSTNAME`的值是`server-2`，我们可以预计`server-0`和`server-1`将首先被创建。

2.  使用 headless 服务到达同一组中的其他 pod 的机会:这与第一点有关；如果我们在`server-2`内部，无头服务器将发布 DNS 条目以连接到`server-0`和`sever-1`。

3.  在主容器运行之前，有机会运行一个或多个定制的初始化容器。这些在 StatefulSet 清单中的`statefulset.spec.template.spec.initContainers`处定义。例如，我们可能希望在正式后备存储的容器运行之前，使用初始化容器从外部源导入 SQL 脚本。官方后台存储的 Docker 图像可能是供应商提供的，用自定义代码“污染它”可能不是一个好主意。

4.  当每个主容器初始化时，以及当它们使用在`statefulset.spec.template.spec.containers.lifecycle`声明的*生命周期钩子*如`postStart`和`preStop`终止时，运行命令的机会。在前面的小节中，我们将把这个特性用于我们的基本键/值存储。

5.  捕捉 SIGTERM Linux 信号的机会，该信号在终止时被发送到每个主容器的每个第一个进程。当容器接收到 SIGTERM 信号时，它们可以在由`statefulset.spec.template.spec.terminationGracePeriodSeconds`属性定义的秒数内运行正常关闭代码。

6.  Pod 的默认活性和准备就绪探测器(见第 [2](2.html) 章)允许决定给定的 Pod 何时对世界可用。

## Pod 生命周期事件的顺序

在上一节中，我们已经看到，在创建和销毁后备存储单元时，有多种运行代码的机会，但是我们没有讨论何时适合应用各种选项。例如，我们应该使用 *Init Containers* 还是 *PostStart hook* 为 MySQL 数据库设置初始表吗？要回答这些问题，有必要了解整个 Pod 生命周期中发生的事件的顺序。为此，我们将首先介绍*的创建*生命周期，然后是*的终止*生命周期。

### 注意

本节假设了各种对读者来说可能是新的一般概念:

*   宽限期:在进程被强制终止之前，允许它运行正常关闭任务的时间。

*   平稳启动和关闭:分别执行“拆除”和“拆除”任务，帮助最大限度地减少中断，防止系统、进程或数据处于不一致的状态。

*   钩子:一个占位符，允许插入触发器、脚本或其他代码执行机制。

*   Sigkill:发送给进程以使其立即终止的信号。这个信号通常不能被捕捉或忽略。

*   SIGTERM:发送给进程请求终止的信号。这个信号可以被进程捕获、解释或忽略。作为正常关闭策略的一部分，实现进程级清理代码是有帮助的。

*   稳态:其特征变量不随时间变化的状态。

创建生命周期包括 Pod 的启动(因为第一次创建 StatefulSet，或者由于缩放事件)。这意味着将一个 Pod 从不存在状态变为*运行*状态。表 [9-4](#Tab1) 显示了最相关的生命周期事件的顺序( *C0…C4，S* )，其中 ***C*** 代表创建， ***S*** 代表稳定状态。稳定状态是指 Pod 不会发生与启动或关闭过程无关的生命周期变化。在第二行中，P 代表挂起，而 R 代表运行。

表 9-4

状态集控制的 Pod 创建生命周期事件

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"> <col class="tcol7 align-left"></colgroup> 
| 

描述

 | 

无着丝粒的

 | 

C1

 | 

C2

 | 

C3

 | 

补体第四成份缺乏

 | 

S

 |
| --- | --- | --- | --- | --- | --- | --- |
| Pod 状态 | P | P | 稀有 | 稀有 | 稀有 | 稀有 |
| 初始化容器运行 |   | ●执行下列操作 |   |   |   |   |
| 主容器运行 |   |   | ●执行下列操作 |   |   |   |
| 启动后挂钩运行 |   |   | ●执行下列操作 |   |   |   |
| 活性探测运行 |   |   |   | ●执行下列操作 |   |   |
| 就绪探测运行 |   |   |   | ●执行下列操作 |   |   |
| 此端点已发布 |   |   |   |   | ●执行下列操作 |   |
| N-1 个端点已发布 | ●执行下列操作 | ●执行下列操作 | ●执行下列操作 | ●执行下列操作 | ●执行下列操作 | ●执行下列操作 |
| 发布了 N+1 个端点 |   |   |   |   |   | ●执行下列操作 |

请注意，表 [9-4](#Tab4) 代表了一个粗略的指南，一些关键的考虑因素也适用:

*   *Pod 状态*是由`pod.status.phase`属性提供的正式 Pod 阶段。虽然挂起(P)和运行(R)意味着是正式阶段，但是`kubectl get pod`命令可能显示中间状态，例如在 *C0* 和 *C1* 之间的*初始化*以及在 *C1* 和 *C2* 之间的*pod 初始化*。

*   如果 *Init 容器*因退出而失败，并返回一个非零退出代码，主容器将不会被执行。

*   *主容器*和与 *PostStart 挂钩*关联的命令并行运行，Kubernetes 不保证先运行哪一个。

*   在主容器启动之前，*活性和准备就绪探测器*开始运行。

*   适用的 *Pod 的端点*是在内部准备就绪探测为肯定之后的某个时间由服务控制器发布的*，但是 DNS 生存时间(TTL)设置(在服务器和客户端)和网络传播问题可能会延迟其他副本和客户端对 Pod 的可见性。*

**   *N-1 个端点*(例如`server-0.server`和`server-1.server`如果参考箱 N 是`server-2`)被认为*是可访问的，因为`server-2`仅在`server-1`变为*就绪*时被初始化；但是，它们在被查询时可能已经失败，或者可能暂时无法访问。由于这个原因，防弹代码应该总是 ping 并探测一个依赖的 Pod，而不是盲目地假设它必须启动并运行，因为有顺序的 Pod 创建保证。*

    *   *N+2 个端点*(例如，如果参考 Pod N 是`server-2`，则为`server-3.server`和`server-4.server`)将仅在当前 Pod 准备就绪后被初始化。因此，如果某些代码需要等待将来的 Pods 变得可用，它们必须作为主容器运行，并且有一个 DNS 探测/ping 循环。* 

 *现在让我们来看一下终端 Pod 生命周期(表 [9-5](#Tab5) ),每当一个状态集被缩减或一个单独的 Pod 被删除时，该生命周期从 *T1* 开始。第一列 ***S*** ，表示触发终止事件之前的 Pod 的稳定状态。

表 9-5

状态集控制的 Pod 终止生命周期事件

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"></colgroup> 
| 

描述

 | 

S

 | 

一种网络的名称(传输率可达 1.54mbps)

 | 

T2

 | 

T3

 |
| --- | --- | --- | --- | --- |
| Pod 状态 | 稀有 | T | T | T |
| 主容器运行 | ●执行下列操作 | ●执行下列操作 | ●执行下列操作 |   |
| 预停止挂钩运行 |   | ●执行下列操作 |   |   |
| 宽限期开始 |   | ●执行下列操作 |   |   |
| 宽限期已结束 |   |   |   | ●执行下列操作 |
| 主集装箱信号术语 |   |   | ●执行下列操作 | ●执行下列操作 |
| 主容器信号 |   |   |   |   |
| 此端点已发布 | ●执行下列操作 |   |   |   |
| N-1 个端点已发布 | ●执行下列操作 | ●执行下列操作 | ●执行下列操作 | ●执行下列操作 |
| 发布了 N+1 个端点 |   |   |   |   |

同样，当考虑表 [9-5](#Tab5) 时，相当值得注意的事项适用:

*   在 *T1* 的终止事件是“残酷的”,没有留下人们可能需要的那么多优雅的关闭范围。特别是，被终止的 Pod 被立即从无头服务中删除；因此为什么在*这个端点发布的*上的●(点)字符在 *T1* 本身就消失了。虽然底层应用对于那些已经预先通过 TCP 连接建立的客户端可能仍然是可访问的，但是那些恰好在 T1 之后查询 DNS 服务或端点控制器的客户端将“看不到”终止的 Pod。

*   *预停挂钩*和*宽限期*将在 *T1* 一起开始。 *SIGTERM* 信号将由主容器在*T2*处接收，在预停止挂钩完成之后，但在宽限期结束之前*。根据`terminationGracePeriodSeconds`属性的值，无论需要什么样的正常关机代码，都必须在分配的时间内完成。*

*   在 *T3* 处，当*宽限期*结束时，主容器将被杀死，不再有机会恢复或运行缓解代码。

## 使用 Pod 生命周期挂钩实现正常关机

在上两节中，我们已经讨论了可用于控制状态集生命周期的各种选项，而状态集又是其每个组成 pod 的单个生命周期的集合。现在，在这一节中，我们将回到面向实验室的工作流，并向我们的原始键/值存储添加一种简单形式的正常关闭。

我们的正常关机包括每当 Pod 终止时返回 503 HTTP 错误，而不是让客户端简单地超时。即使一旦确认了终止事件，就将 Pod 从无头服务 DNS 中删除，如果 Pod 突然超时而没有通知，则在上次检查 DNS 条目之前记得 IP 地址(和/或已经建立了 TCP 连接)的客户端可能会表现出不稳定的行为。尽管这个解决方案看起来很简单，但它可以帮助智能客户端“后退”一段时间，实现*断路器*模式，和/或从不同的源访问数据。

为此，我们将在处理任何 HTTP 请求之前检查是否存在名为`_shutting_down_`的文件:

```
if os.path.isfile(dataDir + '_shutting_down_'):
  return "_shutting_down_", 503

```

前面的`if`语句用于指示服务是否即将关闭，现在位于更新后的`server.py`脚本中每个 Flask HTTP 函数的顶部。请注意，为了简洁和避免干扰，本章前面给出的`server.py`代码清单没有显示`save()`、`load()`和`allKeys()`函数之后的两行代码。

既然我们在客户端有了一个廉价的优雅关闭机制，我们需要在服务器端实现它。我们在这里需要做的是一旦收到终止事件就创建一个`_shutting_down_`文件，并在 Pod 启动时删除它。目的是使用该文件的存在与否来分别表示服务器是否将要关闭。为了实现该文件的创建和删除，我们将分别使用`preStop`和`postStart` pod 生命周期挂钩(参见表 [9-4](#Tab4) 和 [9-5](#Tab5) ):

```
# server.yaml
...
spec:
  template:
    spec:
      containers:
      - name: server
        lifecycle:
          postStart:
            exec:
              command:
              - /bin/sh
              - -c
              - rm -f /var/data/_shutting_down_
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - touch /var/data/_shutting_down_
...

```

这个片段包含在*新的和最终的* `server.yaml`清单中，直接位于章节的根目录下，而不是`wip/`，其中我们还将`terminationGracePeriodSeconds`设置为`10`，这样观察终止行为花费的时间就少了(默认为 30 秒)。为了简单起见，我们还在单个 YAML 文件中添加了服务清单，使用了`---`(三连字符)YAML 符号，这样我们只需一个命令就可以创建最终的服务器:

```
# Memory-based key/value store
# server.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: server
  labels:
    app: server
spec:
  ports:
  - port: 80
  clusterIP: None
  selector:
    app: server
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: server
spec:
  selector:
    matchLabels:
      app: server
  serviceName: server
  replicas: 3
  template:
    metadata:
      labels:
        app: server
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: server
        image: python:alpine
        args:
        - bin/sh
        - -c
        - >-
          pip install flask;
          python -u /var/scripts/server.py 80
          /var/data
        ports:
        - containerPort: 80
        volumeMounts:
          - name: scripts
            mountPath: /var/scripts
          - name: data
            mountPath: /var/data
        lifecycle:
          postStart:
            exec:
              command:
              - /bin/sh
              - -c
              - rm -f /var/data/_shutting_down_
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - touch /var/data/_shutting_down_
      volumes:
        - name: scripts
          configMap:
            name: scripts
        - name: data
          emptyDir:
            medium: Memory

```

## 观察状态集故障

在上一节中，我们通过利用`postStart`和`preStop`生命周期挂钩，在`server.yaml`和`server.py`中实现了优雅关闭功能。在本节中，我们将看到这种功能的实际应用。让我们从将当前工作目录更改为章节的根目录并运行新定义的文件开始:

```
# clean up the environment first

$ ./configmap.sh
configmap/scripts created

$ kubectl apply -f server.yaml
service/server created
statefulset.apps/server created

$ kubectl apply -f client.yaml
pod/client created

```

既然已经创建了服务器和客户机对象，我们可以再次跟踪客户机的日志:

```
$ kubectl logs -f client
      abcdefghijklmnopqrstuvwxyz
      --------------------------
      12012012012012012012012012
      --------------------------
24:41 wwwwwwwwwwwwwwwwwwwwwwwwww | hits = 0 (0%)
24:43 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
24:45 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
24:47 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
...

```

在保持`kubectl logs -f client`在单独的终端窗口上运行的同时，我们现在可以看到通过发出`kubectl delete pod/server-1`命令从 StatefulSet 中删除一个 Pod 的效果:

```
34:45 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
34:47 5hh5hh5hh5hh5hh5hh5hh5hh5h | hits = 17 (65%)
34:49 5hh5hh5hh5hh5hh5hh5hh5hh5h | hits = 17 (65%)
34:51 5hh5hh5hh5hh5hh5hh5hh5hh5h | hits = 17 (65%)
34:53 .hh.hh.hh.hh.hh.hh.hh.hh.h | hits = 17 (65%)
34:55 .hh.hh.hh.hh.hh.hh.hh.hh.h | hits = 17 (65%)
34:57 .hh.hh.hh.hh.hh.hh.hh.hh.h | hits = 17 (65%)
35:00 .hh.hh.hh.hh.hh.hh.hh.hh.h | hits = 17 (65%)
35:02 .hh.hh.hh.hh.hh.hh.hh.hh.h | hits = 17 (65%)
35:04 whhwhhwhhwhhwhhwhhwhhwhhwh | hits = 17 (65%)
35:06 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)

```

在这个日志中，我们看到客户端在第`34:47`秒和第`34:51`秒之间为我们刚刚删除的服务器显示了一个数字`5`(代替`503`)。在此期间，服务器和客户端都可以实现代码以无影响的方式脱离。在`34:53`和`35:02`之间，客户端无法到达服务器 1，如`.`(点)所暗示的，直到它最终设法在`35:04`再次保存密钥，如`w`(写)所指示的。客户端最终在第二次`35:06`报告所有服务器上的`h`(命中)。

如果读者想知道为什么被删除的服务器会在一段时间后自动复活，这是因为 StatefulSet 控制器的职责是确保运行时规范与清单中声明的状态相匹配。假设我们已经删除了一个 Pod，StatefulSet 控件已经采取了纠正措施，以便声明的副本数量和有效运行的副本数量相匹配。

## 放大和缩小

在由部署控制器管理的无状态 pod 的情况下，零停机扩展的魔力在有状态集的情况下更难实现。我们将首先讨论*向上扩展*，然后讨论*向下扩展*，因为每个都有自己的挑战。

向上扩展事件会导致现有客户端可能不知道的新 pod 的出现，除非它们经常检查 DNS SRV 记录并依次更新它们自己。然而，这将打乱哈希算法，并导致后备存储中大量的未命中，就像我们的原始键/值 1 一样。

缩小规模的事件比扩大规模的事件更具破坏性，不仅仅是因为我们正在减少数据保存副本的数量，还因为在 Kubernetes 基于生命周期技术合同以仁慈的方式终止我们的 pod 之前，我们只有一段时间来做任何必要的数据重新分区。

结论是，如果我们的目标是将中断减少到最低限度，我们需要在针对状态集发出`kubectl scale`命令之前考虑和采取额外的步骤。

实际上，我们面临的挑战是，无论服务器数量是增加还是减少，我们都必须重新计算密钥哈希。表 [9-6](#Tab6) 以关键字`a`、`b`、`c`和`d`为例，给出了 2 和 3 副本的最终选定服务器。

表 9-6

对 a、b、c 和 d 应用模 2 和模 3 的效果

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"></colgroup> 
| 

钥匙

 | 

十二月

 | 

以…为模

 | 

N = 2

 | 

N = 3

 |
| --- | --- | --- | --- | --- |
| a | Ninety-seven | 97 %氮 | 服务器-1 | 服务器-1 |
| b | Ninety-eight | 98 %氮 | 服务器-0 | 服务器-2 |
| c | Ninety-nine | 99 %氮 | 服务器-1 | 服务器-0 |
| d | One hundred | 99 %氮 | 服务器-0 | 服务器-1 |

因此，如果服务器的数量从三个副本缩减到两个，我们首先应该将存储在服务器 0-2 中的键值对仅分发到服务器 0-1，反之亦然，如果从两个副本扩展到三个副本。我们已经创建了一个最简单的程序，在一个名为`rebalance.py`的脚本中捕获这个过程。该 Python 脚本采用现有服务器列表和未来服务器列表来执行必要的键/值对重新分区:

```
#!/usr/bin/python3
# rebalance.py
import sys
import urllib.request

if len(sys.argv) < 3:
  print('rebalance.py AS_IS_SRV_1[:PORT],' +
        'AS_IS_SRV_2[:PORT]... ' +
        'TO_BE_SRV_1[:PORT],TO_BE_SRV_2[:PORT],...')
  sys.exit(1)

# Process arguments
as_is_servers = sys.argv[1].split(',')
to_be_servers = sys.argv[2].split(',')

# Remove boilerplate from HTTP calls
def curl(url):
  return urllib.request.urlopen(url).read().decode()

# Copy key/vale pairs from AS IS to TO BE servers
urls = []
for server in as_is_servers:
  keys = curl('http://' + server +
              '/allKeys').split(',')
  print(server + ': ' + str(keys))
  for key in keys:
    print(key + '=',end=")
    value = curl('http://' + server +
                 '/load/' + key)
    sn = ord(key) % len(to_be_servers)
    target_server = to_be_servers[sn]
    print(value + ' ' + server +
          '->' + target_server)
    urls.append('http://' + target_server +
                '/save/' + key + '/' + value)
for url in urls:
  print(url,end=")
  print(' ' + curl(url))

```

像在`server.py`和`client.py`的情况下一样，使用配置图上传脚本:

```
#!/bin/sh
# configmap.sh
kubectl delete configmap scripts \
  --ignore-not-found=true
kubectl create configmap scripts \
  --from-file=server.py \
  --from-file=client.py --from-file=rebalance.py

```

如前所述，缩放并不简单，需要小心控制；因此，我们将考虑一个 Pod 清单来执行从三个到两个名为`rebalance-down.yaml`的副本的缩减迁移:

```
# rebalance-down.yaml
# Reduce key/store cluster to 2 replicas from 3
apiVersion: v1
kind: Pod
metadata:
  name: rebalance
spec:
  restartPolicy: Never
  containers:
  - name: rebalance
    image: python:alpine
    args:
    - bin/sh
    - -c
    - "python -u /var/scripts/rebalance.py
      server-0.server,server-1.server,\
      server-2.server
      server-0.server,server-1.server"
    volumeMounts:
      - name: scripts
        mountPath: /var/scripts
  volumes:
  - name: scripts
    configMap:
      name: scripts

```

同样，我们还将`rebalance-up.yaml`定义为从两个副本扩展到三个副本:

```
# rebalance-up.yaml
# Scale key/store cluster to 3 replicas from 2
apiVersion: v1
kind: Pod
metadata:
  name: rebalance
spec:
  restartPolicy: Never
  containers:
  - name: rebalance
    image: python:alpine
    args:
    - bin/sh
    - -c
    - "python -u /var/scripts/rebalance.py
      server-0.server,server-1.server
      server-0.server,server-1.server,\
      server-2.server"
    volumeMounts:
      - name: scripts
        mountPath: /var/scripts
  volumes:
  - name: scripts
    configMap:
      name: scripts

```

现在，我们已经定义了重新平衡脚本和清单来扩展和缩减我们的集群，我们可以清理环境并再次部署服务器和客户端，这样我们就不会受到 Kubernetes 集群中以前示例的干扰:

```
# clean up the environment first

$ ./configmap.sh
configmap/scripts created

$ kubectl apply -f server.yaml
service/server created
statefulset.apps/server created

$ kubectl apply -f client.yaml
pod/client created

```

## 分频

在上一节中，我们已经讨论了这样一个事实，即伸缩并不是微不足道的，直截了当地发出一个`kubectl scale`命令可能会导致不必要的中断。在这一节中，我们将看到如何以有序的方式缩减我们的原始键/值存储。

我们将经历以下步骤:

1.  以只读模式运行新的目标客户端(以两个副本为目标)，以便我们可以观察迁移的结果。

2.  停止读/写三副本客户端窗格。

3.  将键/值对从三个 Pod 集群迁移到两个 Pod 集群。

4.  将三个副本的集群缩减为两个副本。

让我们从定义一个名为`client-ro-2.yaml`的 Pod 清单开始，以只读模式运行仅针对`server-0`和`server-1`的`client.py`:

```
# client-ro-2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: client-ro-2
spec:
  restartPolicy: Never
  containers:
    - name: client-ro-2
      image: python:alpine
      args:
      - bin/sh
      - -c
      - >
        python -u /var/scripts/client.py
        server-0.server,server-1.server readonly
      volumeMounts:
        - name: scripts
          mountPath: /var/scripts
  volumes:
    - name: scripts
      configMap:
        name: scripts

```

现在让我们应用它并遵循它的日志:

```
$ kubectl apply -f client-ro-2.yaml
pod/client-ro-2 created

$ kubectl logs -f client-ro-2

      abcdefghijklmnopqrstuvwxyz
      --------------------------
      10101010101010101010101010
      --------------------------
33:33 hmmmmhhmmmmhhmmmmhhmmmmhhm | hits = 9 (35%)
33:35 hmmmmhhmmmmhhmmmmhhmmmmhhm | hits = 9 (35%)
33:37 hmmmmhhmmmmhhmmmmhhmmmmhhm | hits = 9 (35%)

```

请注意，在实际场景中，在重新分区完成后，我们将只运行针对新的较小 StatefulSet 的客户端。然而，在这里我们可以更快地实时观察迁移的效果。还要注意，现在只包括了服务器`1`和`0`，大多数键查找都会导致未命中。

现在是微妙的部分，读取密钥并重新计算较小的双副本集群的新散列，这是`reblance-down.yaml`清单的工作，它又执行`rebalance.py`。在我们这样做之前，我们必须首先阻止客户端 Pod 对即将弃用的*、*三副本集群执行写入操作:

```
$ kubectl delete --grace-period=1 pod/client
pod "client" deleted

$ kubectl apply -f rebalance-down.yaml                            pod/rebalance created

$ kubectl logs -f rebalance
server-0.server:
['x', 'u', 'r', 'o', 'l', 'i', 'f', 'c']
x=x server-0.server->server-0.server
u=u server-0.server->server-1.server
...
server-1.server:
['y', 'v', 's', 'p', 'm', 'j', 'g', 'd', 'a']
y=y server-1.server->server-1.server
v=v server-1.server->server-0.server
...
server-2.server:
['z', 'w', 't', 'q', 'n', 'k', 'h', 'e', 'b']
z=z server-2.server->server-0.server
w=w server-2.server->server-1.server
...

```

到`pod/rebalance`完成时，运行`kubectl log -f client-ro-2`的窗口将显示整个键集的点击次数:

```
28:45 hmmmmhhmmmmhhmmmmhhmmmmhhm | hits = 9 (35%)
28:47 hmmmmhhmmmmhhmmmmhhmmmmhhm | hits = 9 (35%)
28:49 hmmmmhhmmmmhhmmmmhhmmmmhhm | hits = 9 (35%)
28:51 hmhhhhhhhhhhhhhhhhhhhhhhhh | hits = 25 (96%)
28:53 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
28:55 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
28:57 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)

```

此时，我们可以将群集缩减为两个副本，之后，可以针对现在更小的双副本群集以读/写模式启动客户端:

```
$ kubectl scale statefulset/server --replicas=2
statefulset.apps/server scaled

```

## 按比例放大

在上一部分中，我们刚刚将我们的群集从三个副本缩减到两个副本。为了观察正在进行的扩展操作，我们将以与之前类似的方式工作，首先启动一个只读客户端，目标是在`client-ro-3.yaml`中定义的三个副本:

```
# client-ro-3.yaml
apiVersion: v1
kind: Pod
metadata:
  name: client-ro-3
spec:
  restartPolicy: Never
  containers:
    - name: client-ro-3
      image: python:alpine
      args:
      - bin/sh
      - -c
      - "python -u /var/scripts/client.py
        server-0.server,server-1.server,\
        server-2.server readonly"
      volumeMounts:
        - name: scripts
          mountPath: /var/scripts
  volumes:
    - name: scripts
      configMap:
        name: scripts

```

当我们运行这个客户端时，我们期望在`server-2`上看到一个失败，用`.`(点)表示。这正是我们所期待的，因为`server-2`还没有运行:

```
$ kubectl apply -f client-ro-3.yaml
pod/client-ro-3 created

$ kubectl logs -f client-ro-3

      abcdefghijklmnopqrstuvwxyz
      --------------------------
      12012012012012012012012012
      --------------------------
43:57 h.hh.hh.hh.hh.hh.hh.hh.hh. | hits = 17 (65%)
43:59 h.hh.hh.hh.hh.hh.hh.hh.hh. | hits = 17 (65%)
44:01 h.hh.hh.hh.hh.hh.hh.hh.hh. | hits = 17 (65%)
44:03 h.hh.hh.hh.hh.hh.hh.hh.hh. | hits = 17 (65%)

```

由于启用了写功能的客户端不需要知道新的副本(`client.yaml`不被认为正在运行)，因此可以安全地将群集扩展到三个副本，而无需更多操作:

```
$ kubectl scale statefulset/server --replicas=3
statefulset.apps/server scaled

```

紧接着，我们应该在运行`kubectl logs -f client-ro-3`的窗口上观察到由`.(`点表示的服务器故障变成由字母`m`表示的未命中:

```
...
49:54 h.hh.hh.hh.hh.hh.hh.hh.hh. | hits = 17 (65%)
49:56 h.hh.hh.hh.hh.hh.hh.hh.hh. | hits = 17 (65%)
49:58 h.hh.hh.hh.hh.hh.hh.hh.hh. | hits = 17 (65%)
50:00 hmhhmhhmhhmhhmhhmhhmhhmhhm | hits = 17 (65%)
50:02 hmhhmhhmhhmhhmhhmhhmhhmhhm | hits = 17 (65%)
50:06 hmhhmhhmhhmhhmhhmhhmhhmhhm | hits = 17 (65%)
...

```

到目前为止，一切顺利；我们现在可以通过应用`rebalance-up.yaml`将键/值对重新划分到一个三副本集群中:

```
$ kubectl delete pod/rebalance
pod "rebalance" deleted

$ kubectl apply -f rebalance-up.yaml
pod/rebalance created

```

每当我们跟踪`client-ro-3`日志的终端窗口显示未命中变成命中时，我们就成功地扩大了集群:

```
53:24 hmhhmhhmhhmhhmhhmhhmhhmhhm | hits = 17 (65%)
53:26 hmhhmhhmhhmhhmhhmhhhhhmhhh | hits = 19 (73%)
53:28 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
53:30 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
53:32 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)

```

此时，针对三个副本的群集启用读/写客户端是安全的。

### 注意

Kubernetes 将 StatefulSet 的 pod 称为*副本*，因为我们在使用`kubectl scale`命令时使用了`replicas`属性和副本语义。但是，StatefulSet 的副本不一定是一字不差的无状态副本，因为它们通常是部署的副本。从逻辑的角度来看，假设我们使用每个 StatefulSet 的 Pod 实例来存储数据集的一个子集，那么将它们视为“分区”会有所帮助

## 关于扩大和缩小业务规模的结论

向上和向下扩展操作以一种原始的、相当手工的方式演示了每当集群大小改变时重新散列键的问题，并且数据块必须被重新安排到不同数量的服务器中。

大多数高级的现成后备存储(如 MongoDB)通过实现异步复制算法，使用户免受我们刚刚看到的那种手动操作。那何必呢？因为尽管我们原始的键/值存储可能过于简单，但它有助于直观地了解现成解决方案所采取的权衡，以提供几乎零停机扩展的假象。

在这方面，我们的原始键/值存储是可伸缩的，但不是高度可用的，这不是因为数据存储在内存中(我们将在本章结束之前解决这一问题)，而是因为单个副本故障会导致数据不可访问。例如，Cassandra 不仅可以使用与本文中使用的哈希方案类似的方法进行扩展，而且还具有很高的可用性:可以对其进行配置，使得相同的数据在被认为“持久化”之前被写入两个或更多的节点。

潜在高度可用(除了高度可伸缩之外)的后备存储带来的复杂性是，它们带来了处理最终一致性的挑战。例如，我们可以很容易地修改我们的客户机，以便使用一个复制方案(如`(` `totalReplicaCount + 1) % totalReplicaCount`)将一个密钥保存到两个或多个副本中。然而，每当读取两个键并且它们的值不同时，客户端需要调用哪个键。在我们的原始键/值存储的情况下，我们可以很容易地修改它，以提供一个时间戳，这样客户端就可以将最近的一个作为有效的。

## 正确的有状态性:磁盘持久性

到目前为止，我们一直将键/值存储视为内存中的缓存，一旦副本打喷嚏，它就会丢失数据。这是故意的，因为我们到目前为止讨论的所有状态属性都与磁盘持久性相对正交:

*   稳定的网络身份

*   有序 Pod 创建

*   服务发现和无头服务

*   扩展策略

此外，我们决定通过简单地替换卷`data`的底层实现来区分 RAM 和磁盘存储。到目前为止，所有示例都依赖于基于 RAM 的文件系统，如下所示:

```
# server.yaml
    ...
    volumes:
      - name: data
        emptyDir:
          medium: Memory

```

降低磁盘持久性的途径是使用`hostPath`卷类型。`hostPath`卷类型允许直接在节点的文件系统上存储数据；然而，这种方法是有问题的，因为它只有在 Pods 总是被安排在相同的节点上运行时才有效。另一个问题是，节点级存储并不意味着是持久的:即使 Pods 被调度到相同的节点，节点的文件系统也不能保证在节点崩溃或重启后仍然存在。

读者可能已经猜到，我们需要的是*附加网络存储*，它的生命周期独立于 Kubernetes 工作节点的生命周期。在 GCP，这只是创建一个“持久磁盘”的问题，只需发出一条命令:

```
$ gcloud compute disks create my-disk --size=1GB
Created
NAME     ZONE            SIZE_GB  TYPE         STATUS
my-disk  europe-west2-a  1        pd-standard  READY

```

然后，我们可以将`data`卷与 StatefulSet 清单中名为`my-disk`的 GCP 磁盘相关联:

```
# server.yaml
    ...
    volumes:
      - name: data
        gcePersistentDisk:
          pdName: my-disk
          fsType: ext4

```

上述方法的局限性在于只有一个 Pod 可以对`my-disk`进行读/写访问。所有其他窗格可能只有只读访问权限。如果我们通过修改`server.yaml`中的`volumes`声明来尝试上述方法，我们将看到只有`server-0`会成功启动，而`server-1`会失败(因此`server-2`不会被调度)。这是因为只有一个 Pod ( `server-0`)可以对永久磁盘`my-disk`进行读/写访问。

我们的键/值存储所采用的多主机方案要求所有副本(以及 pod)具有完全的读/写访问权限。此外，可伸缩系统的要点是数据分布在多个磁盘上，而不是存储在由多个服务器访问的单个中央磁盘上。理想情况下，我们需要为每个副本创建一个单独的磁盘。大致如下的东西:

```
# Example only, don't run these commands
$ gcloud compute disks create my-disk-server-0
$ gcloud compute disks create my-disk-server-1
$ gcloud compute disks create my-disk-server-2

```

这种方法需要预先规划给定数量的副本所需的磁盘数量。如果 Kubernetes 能够代表我们为每个需要持久存储的 Pod 发出前面的`gcloud compute disks create`命令(或者使用底层 API)会怎么样？好消息，可以！欢迎来到持续批量索赔。

## 持续量声明

持久卷声明可以理解为一种机制，允许 Kubernetes 根据每个 Pod 的身份按需创建磁盘，这样，如果一个 Pod 崩溃或被重新调度，每个 Pod 及其关联的卷之间就会保持 1:1 的链接(Kubernetes 行话中的*绑定*)。无论一个 Pod 在哪个节点上“醒来”，Kubernetes 总是会附加其对应的*绑定*卷，用于`server-0`、`disk-0`；对于`server-1`、`disk-1`；等等。

虽然 Kubernetes 可能运行在有多个能够授予卷(如`disk-0`和`disk-1`)的存储阵列的环境中，但这些存储阵列也因云供应商而异。例如，在 AWS 中，这种块存储功能被称为亚马逊弹性存储(EBS ),实现方式与 GCP 持久磁盘不同。问题是，Kubernetes 怎么知道向谁要一卷呢？嗯，存储阵列(或等效物)功能在 Kubernetes 中体现为一个名为*存储类*的对象。

针对给定的存储类执行持久卷声明。Google Kubernetes 引擎(GKE)提供了一个名为`standard`的现成存储类:

```
$ kubectl get storageclass
NAME                 PROVISIONER            AGE
standard (default)   kubernetes.io/gce-pd   40m

$ kubectl describe storageclass/standard
Name:                 standard
IsDefaultClass:       Yes
Annotations:          storageclass.*.kubernetes.io/*
Provisioner:          kubernetes.io/gce-pd
Parameters:           type=pd-standard
AllowVolumeExpansion: <unset>
MountOptions:         <none>
ReclaimPolicy:        Delete
VolumeBindingMode:    Immediate
Events:               <none>

```

除非另有说明，否则每当按需创建 GKE 磁盘(Kubernetes 中的*卷*)时，就会使用`standard`存储类。StorageClass 有一种驱动程序，它允许 Kubernetes 编排磁盘(或一般的块设备)的创建，而不需要管理员向外部存储接口(如`gcloud compute disk create`)发出手动命令。

让我们的 StatefulSet 清单请求磁盘到标准的 StorageClass 只需要很少的额外代码。这是在`statefulset.spec`下创建以下条目的问题:

```
# server-disk.yaml
    ...
    volumeClaimTemplates:
      - metadata:
          name: data
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi

```

在这个清单片段中，我们将卷命名为`data`，请求一个 1GB 的永久磁盘，并将访问模式设置为`ReadWriteOnce`，这意味着没有其他副本会对其提供的卷进行独占读/写访问。

如前所述，持久卷声明可以理解为一种代表我们使用云提供商的存储接口(在我们的例子中是 GCP)创建磁盘的机制。现在让我们看看这确实是真的。我们将修改原始的`server.yaml`文件以包含上面的持久卷声明，并删除旧的`emptyDir`卷定义。新生成的文件被称为`server-disk.yaml`。让我们再次清理我们的环境，启动我们新定义的基于磁盘的服务器:

```
# clean up the environment first

$ ./configmap.sh
configmap "scripts" deleted
configmap/scripts created

$ kubectl apply -f server-disk.yaml
service/server created
statefulset.apps/server created

```

几秒钟后，我们可以通过运行`kubectl get pv`检查是否已经创建了三个卷(每个副本一个)。请注意，为了简洁起见，已经删除和/或简化了一些细节:

```
$ kubectl get pv
NAME           CAP STATUS CLAIM
pvc-42339fcc-* 1Gi Bound  default/data-server-0
pvc-4cb81b93-* 1Gi Bound  default/data-server-1
pvc-59792e64-* 1Gi Bound  default/data-server-2

```

我们现在还可以看到，GCP 将相同的卷视为正确的 Google Cloud 持久磁盘，就好像我们手动创建了它们一样:

```
$ gcloud compute disks list
...
gke-my-cluster-f8fca-pvc-42339fcc-*
gke-my-cluster-f8fca-pvc-4cb81b93-*
gke-my-cluster-f8fca-pvc-59792e64-*

```

我们现在不仅有三个独立的卷，每个卷对应一个服务器(`server-0`、`server-1`、`server-2`)，而且我们还有一个自称为一流 Kubernetes 公民的卷:

```
$ kubectl get pvc
NAME          STATUS VOLUME         CAP MODE
data-server-0 Bound  pvc-42339fcc-* 1Gi RWO
data-server-1 Bound  pvc-4cb81b93-* 1Gi RWO
data-server-2 Bound  pvc-59792e64-* 1Gi RWO

```

持久卷声明的关键特征是 pod 的生命周期独立于卷的生命周期。换句话说，我们可以删除 pod，让它们崩溃，扩展 stateful set——甚至残酷地删除它。无论发生什么情况，与每个 Pod 关联的卷都将被重新连接。让我们启动我们的测试客户端，这样我们就可以证明事实确实如此:

```
$ kubectl apply -f client.yaml
pod/client created

$ kubectl logs -f client

      abcdefghijklmnopqrstuvwxyz
      --------------------------
      12012012012012012012012012
      --------------------------
53:25 wwwwwwwwwwwwwwwwwwwwwwwwww | hits = 0 (0%)
53:27 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
53:29 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
53:31 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)

```

在第二次`25`之前，密钥已经被写入三个复制品一次。既然密钥已经由适当的持久性存储备份，我们应该会看到服务器故障，但不会再出现写操作(字母`w`)。我们将从发出`kubectl delete pod/server-2`命令删除`server-2`开始，看看这是否确实是真的:

```
57:04 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
57:06 h5hh5hh5hh5hh5hh5hh5hh5hh5 | hits = 17 (65%)
57:08 h5hh5hh5hh5hh5hh5hh5hh5hh5 | hits = 17 (65%)
57:10 h5hh5hh5hh5hh5hh5hh5hh5hh5 | hits = 17 (65%)
57:12 h5hh5hh5hh5hh5hh5hh5hh5hh5 | hits = 17 (65%)
57:15 h5hh5hh5hh5hh5hh5hh5hh5hh5 | hits = 17 (65%)
57:17 h.hhhhhhhhhhhhhhhhhhhhhhhh | hits = 25 (96%)
58:25 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)

```

这里我们看到在`57:06`和`57:15`之间，`server-2`正在关闭(数字`five`代表 HTTP 错误`503`)。然后在`57:17`服务器变得不可访问一段时间，然后再次恢复在线。请注意，既没有`m`(未命中)也没有`w`(写入)字母，因为`server-2`从未丢失任何数据。

现在让我们做一些更激进的事情，通过发出`kubectl delete statefulset/server`命令删除 StatefulSet 本身:

```
26:03 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
26:06 55555555555555555555555555 | hits = 0 (0%)
26:08 55555555555555555555555555 | hits = 0 (0%)
26:10 55555555555555555555555555 | hits = 0 (0%)
26:12 55555555555555555555555555 | hits = 0 (0%)
26:14 55555555555555555555555555 | hits = 0 (0%)
26:16 .......................... | hits = 0 (0%)
27:25 .......................... | hits = 0 (0%)
27:27 .......................... | hits = 0 (0%)

```

请注意，所有服务器都进入关闭模式，然后变得不可访问，如`.`(点)所示。在我们确认所有的 pod 都已终止后，我们通过发出`kubectl apply -f server-disk.yaml`命令再次启动 StatefulSet:

```
28:05 .......................... | hits = 0 (0%)
28:07 .......................... | hits = 0 (0%)
28:10 ..h..h..h..h..h..h..h..h.. | hits = 8 (31%)
28:12 ..h..h..h..h..h..h..h..h.. | hits = 8 (31%)
28:14 ..h..h..h..h..h..h..h..h.. | hits = 8 (31%)
28:16 ..h..h..h..h..h..h..h..h.. | hits = 8 (31%)
28:18 ..h..h..h..h..h..h..h..h.. | hits = 8 (31%)
28:20 h.hh.hh.hh.hh.hh.hh.hh.hh. | hits = 17 (65%)
28:22 h.hh.hh.hh.hh.hh.hh.hh.hh. | hits = 17 (65%)
28:25 h.hh.hh.hh.hh.hh.hh.hh.hh. | hits = 17 (65%)
28:27 h.hh.hh.hh.hh.hh.hh.hh.hh. | hits = 17 (65%)
28:29 h.hh.hh.hh.hh.hh.hh.hh.hh. | hits = 17 (65%)
28:31 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
28:33 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)
28:35 hhhhhhhhhhhhhhhhhhhhhhhhhh | hits = 26 (100%)

```

在这里，我们看到服务器逐渐联机，当它们联机时，客户端注册`h` (hit ),这意味着密钥已被成功检索，无需再次写入。请注意，在本例中，我们排除了由于部分写入、文件句柄未关闭或其他应用级故障导致磁盘上文件损坏的可能性。

## 摘要

在本章中，我们使用 StatefulSets 从头实现了一个键/值数据存储支持服务，帮助我们观察这种控制器类型保证的关键属性，如顺序 Pod 创建和稳定的网络身份。后者，稳定的网络身份是通过使用无头服务和一致的持久性来公开 pod 的基础，这两个特性也在本章中讨论。

我们还研究了 Pod 生命周期事件及其在集群设置和管理伸缩事件(以及首次启动有状态集群)中的相关性。对于读者来说，当考虑数据分区和复制方面时，即席缩放(使用`kubectl scale`)命令是困难的；秤事件前后通常都需要额外的步骤，如运行脚本和/或管理程序。*