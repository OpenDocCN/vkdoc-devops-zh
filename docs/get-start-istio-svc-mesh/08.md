# 八、日志和跟踪

在前一章中，我们使用了 Istio 提供的一些可观察性特性。我们能够捕获应用指标，但是指标只是可观察性的一个方面。可观察性是指在所有可能的维度上收集数据。在应用停机期间，查看可观察性的各个方面有助于开发人员理解应用行为，以便执行事件分析。有许多工具可以帮助实现这一目标。但是，了解添加一个行为所需的成本是很重要的。运营团队应该能够配置他们选择的工具，而不需要开发人员。在本章中，我们将看到如何无缝地捕获额外的行为，如请求跟踪和应用日志。我们还将使用 Istio 即插即用模型，该模型为捕获额外的应用行为提供了统一的机制。

## 分布式跟踪

在微服务应用中，一个请求通常部分由部署在集群中的多个应用提供服务。分布式跟踪是跨不同应用跟踪请求流的过程。跟踪通常通过显示某个时刻的请求、响应和延迟来描述应用的行为。运营团队通常使用跟踪来确定哪些服务导致了应用的性能问题。分布式跟踪有很多解决方案，比如 Zipkin、Jagger、Skywalking 等等。Istio 服务网格可以与它们一起工作。跟踪通常由特使代理生成。然后，这些跟踪被发送到跟踪器后端。

分布式跟踪依赖于一组额外的 HTTP 头。这些是广为人知的 b3 请求头。这些头构建了一个请求上下文，用于标识父请求，然后从一个系统传播到另一个系统。特使代理可以为每个传出请求生成这些头。但是对于每个传入的请求，头部必须传播到子请求。如果没有正确完成，那么 Envoy 将生成新的头，因此这些跨度将不会相互关联。

总之，以下一组头必须从传入请求传播到所有传出的子请求:

*   `x-request-id`

*   `x-b3-traceid`

*   `x-b3-spanid`

*   `x-b3-parentspanid`

*   `x-b3-sampled`

*   `x-b3-flags`

这里有一些特定于语言的 OpenTracing 库，可以帮助实现所需的头传播。OpenTracing 的细节超出了本书的范围。要了解更多信息，请参考前面提到的某个库。

在我们继续之前，我们需要将一个 tracer 应用部署到我们的 Kubernetes 集群。在这一章中，我们将使用 Jagger，一个在优步开发的开源分布式追踪应用。Jagger 基于 Dapper 和 OpenZipkin 提出的概念。出于我们的目的，我们将使用 Jagger 操作符( [`https://github.com/jaegertracing/jaeger-operator`](https://github.com/jaegertracing/jaeger-operator) )部署 Jagger。Kubernetes 有一个操作扩展，可以用来在 Kubernetes 集群上打包和部署应用。第一步，我们需要通过执行以下命令来安装 Jagger 操作符:

```
$ git clone https://github.com/jaegertracing/jaeger-operator.git
$ kubectl create namespace observability
namespace/observability created
$ kubectl create -f jaeger-operator/deploy/crds/jaegertracing_v1_jaeger_crd.yaml
serviceaccount/jaeger-operator created
$ kubectl create -f jaeger-operator/deploy/service_account.yaml
serviceaccount/jaeger-operator created
$ kubectl create -f jaeger-operator/deploy/role.yaml
clusterrole.rbac.authorization.k8s.io/jaeger-operator created
$ kubectl create -f jaeger-operator/deploy/role_binding.yaml
clusterrolebinding.rbac.authorization.k8s.io/jaeger-operator created
$ kubectl create -f jaeger-operator/deploy/operator.yaml
deployment.apps/jaeger-operator created

```

这些执行的命令在`observability`名称空间中部署操作符。Kubernetes 操作符的细节超出了本书的范围。请参考 Kubernetes 文档以了解更多信息。

我们可以验证操作符，如下所示:

```
$ kubectl get all -n observability
NAME                                  READY   STATUS    RESTARTS   AGE
pod/jaeger-operator-5574c4fb9-4vn5q   1/1     Running   0          2m4s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/jaeger-operator   1/1     1            1           2m4s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/jaeger-operator-5574c4fb9   1         1         1       2m4s

```

Jagger 操作符现在可用于我们的 Kubernetes 集群。它用于部署 Jagger 实例。我们将部署尽可能简单的配置，即默认的配置有内存存储的 AllInOne Jagger 包。“一体化”映像在单个 pod 中部署了代理、收集器、查询、ingester 和 Jaeger UI。这可以通过使用以下配置来实现:

```
---
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: simplest

```

使用以下命令将此配置应用到我们的 Kubernetes 集群:

```
$kubectl apply -f jagger.yaml

```

现在让我们检查 Jagger 安装是否工作正常。我们可以首先检查我们的集群中已部署的服务，如下所示:

`$kubectl get all`

```
NAME                                         READY   STATUS              RESTARTS   AGE
pod/frontend-deployment-c9c975b4-p8z2t       2/2     Running             38         35d
pod/simplest-56c7bd47bf-z7cnx                0/1     ContainerCreating   0          16s
pod/webapp-deployment-6.2-654c5fd8f9-mrc22   2/2     Running             140        43d

NAME                                 TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                                  AGE
service/simplest-agent               ClusterIP  None           <none>        5775/TCP,5778/TCP,6831/TCP,6832/TCP      16s
service/simplest-collector           ClusterIP  10.152.183.169 <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP   16s
service/simplest-collector-headless  ClusterIP  None           <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP   17s
service/simplest-query               ClusterIP  10.152.183.25  <none>        16686/TCP                                16s

```

我们可以看到部署的所有组件都在运行。现在让我们通过查找最简单的查询服务的节点端口地址来打开 UI。见图 [8-1](#Fig1) 。

![../images/483921_1_En_8_Chapter/483921_1_En_8_Fig1_HTML.jpg](../images/483921_1_En_8_Chapter/483921_1_En_8_Fig1_HTML.jpg)

图 8-1

Jagger UI

我们只使用内存模式部署了 Jagger。这对于测试来说已经足够好了。Jagger 提供了在生产环境中部署它的配置选项(使用持久性存储)。参考 Jagger 文档了解更多信息。

一旦 Jagger 可用，Istio 服务网格就需要引用它。有几种方法可以实现这一点。

*   如果我们正在安装服务网格，我们可以在变量`global.tracer.zipkin.address=jagger-FQDN:16686`中提供 Jagger 地址。

*   在现有的安装中，我们需要编辑配置并指定`trace_zipkin_url`变量。让我们使用以下命令来编辑我们的配置:

```
$ kubectl -n istio-system edit deployment istio-telemetry

```

我们现在已经有了正确的基础设施。接下来，我们需要指示边车开始生成轨迹。特使代理可被配置成对所有接收到的请求的子集进行采样。这可以通过以下方式之一实现:

*   作为 Istio 安装的一部分，设置`pilot.traceSampling`变量。

*   使用以下命令将`PILOT_TRACE_SAMPLING`变量设置为现有安装:

```
$ kubectl -n istio-system edit deploy istio-pilot

```

在此之后，Envoy 将生成请求跨度，并将它们发送到 Jagger 服务器。我们可以通过执行前端 Java 应用的请求来验证这一点。

```
$ for i in {1..500}; do  curl http://10.152.183.230/;echo "; done

```

现在让我们查找 Jagger UI 并搜索以前执行的请求。所有请求对于前端和 webapp 应用都有一个跨度。参见图 [8-2](#Fig2) 。

![../images/483921_1_En_8_Chapter/483921_1_En_8_Fig2_HTML.jpg](../images/483921_1_En_8_Chapter/483921_1_En_8_Fig2_HTML.jpg)

图 8-2

贾格尔痕迹

如图 [8-2](#Fig2) 所示，Jagger 提供了所有已执行请求的直方图。它显示了处理请求所花费的时间以及调用的应用。我们可以单击各个跟踪，详细查看各个应用的延迟和时间线。

## 应用日志

Istio 不为管理 Kubernetes 集群中运行的应用生成的日志提供任何支持。这在大型部署中是一个巨大的挑战，因为为我们的每个应用生成的日志都保留在运行该应用的容器中。让我们回头看看我们在第 [3](03.html) 章开发的例子。我们用 Java 创建了一个前端应用，用 Python 创建了一个 web 服务后端。我们在集群中部署了两个后端实例和一个前端应用实例。我们可以对我们的前端执行请求，这将调用后端。为了调试行为，我们需要查看应用日志。这是通过使用以下命令对每个容器执行日志查找来实现的:

```
$ kubectl logs pod/frontend-deployment-c9c975b4-p8z2t -c frontend
2019-08-26 13:52:42.032  INFO 1 --- [           main] istio.IstioFrontendApplication           : Starting IstioFrontendApplication v0.0.1-SNAPSHOT on frontend-deployment-c9c975b4-p8z2t with PID 1 (/app.war started by root in /)
2019-08-26 13:52:42.039  INFO 1 --- [           main] istio.IstioFrontendApplication           : No active profile set, falling back to default profiles: default
2019-08-26 13:53:01.243  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2019-08-26 13:53:01.471  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2019-08-26 13:53:01.471  INFO 1 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.19]

```

这是非常麻烦和容易出错的，因为我们需要查找每个容器。此外，随着容器重启，我们会丢失应用日志中的信息。日志记录包含最详细的系统状态，团队必须能够参考日志来跟踪、验证和诊断应用的状态。因此，我们可以说应用日志的处理不够好，我们需要一个更好的解决方案来做到这一点。

除了应用日志之外，应用还可以创建访问日志。传统上，我们在前端代理中看到过这种情况，Apache HTTP 服务器正在创建`access.log`文件。日志记录包含我们的应用收到的请求以及对它的响应。这是非常有用的信息。现在，如果我们看一下 Istio 服务网格，所有请求都是通过 Envoy sidecar 发出的。sidecar 因此跟踪它接收到的请求-响应。我们可以配置 Envoy 代理来打印这些日志或创建一个日志文件。但这对我们没有帮助，因为容器重启会丢失所有这些信息。

Kubernetes 描述了一种集群级日志记录方法，该方法利用了 ELK、Splunk、Stackdriver 等日志记录后端应用。该方法利用了服务网格使用的 sidecar 模式。应用日志的完整解决方案如下所示:

![../images/483921_1_En_8_Chapter/483921_1_En_8_Fig3_HTML.jpg](../images/483921_1_En_8_Chapter/483921_1_En_8_Fig3_HTML.jpg)

图 8-3

立方结构记录日志

*   部署在集群中的应用需要将日志写入文件。日志文件在由`volume-mount`创建的位置创建。

*   我们运行安装了导出卷的第二个容器。容器将运行一个能够执行日志解析的`fluentd`进程。

*   sidecar 容器然后读取日志并将它们发送到适当的后端。参见图 [8-3](#Fig3) 。

看一下前面的解决方案，我们的前端应用的配置如下所示:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-deployment
###############   OMITTED FOR BREVITY
      containers:
      - name: frontend
        image: frontend-app:1.0
        imagePullPolicy: Never
        env:
        - name: LOG_PATH
          value: /var/log
        volumeMounts:
        - name: varlog
          mountPath: /var/log
###############   OMITTED FOR BREVITY
      - name: log-agent
        image: k8s.gcr.io/fluentd-gcp:1.30
        env:
        - name: FLUENTD_ARGS
          value: -c /etc/fluentd-config/fluentd.conf
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: config-volume
          mountPath: /etc/fluentd-config
###############   OMITTED FOR BREVITY

```

在前面的代码中，我们完成了以下工作:

*   我们已经将`/var/log`卷添加到我们的前端 Spring Boot 容器中。路径已经导出到`LOG_PATH`变量。该变量指示 Spring Boot 在`/var/log`路径创建`spring.log`文件。

*   接下来，我们的 pod 中有一个日志代理容器。容器用配置文件`/etc/fluentd-config/fluentd.conf`运行`fluentd`进程。

下面的`fluentd.conf`文件读取我们的应用生成的日志，并将它们发送到 ELK 聚合器:

```
<source>
  type tail
  format /^\[[^ ]* (?<time>[^\]]*)\] \[(?<level>[^\]]*)\] (?<message>.*)$/
  time_format %b %d %H:%M:%S %Y
  path /var/log/spring.log
  pos_file /var/log/agent/spring.log.pos
  tag hostname.system
</source>
<match *.**>
  type forward
<server>...</server>
<!--removed for Brevity -->
</match>

```

在应用之前的配置之前，我们需要部署`fluentd`聚合器。这可以通过使用 Kubernetes 操作符来完成。配置`fluentd`的细节超出了本书的范围。详见 [`https://github.com/vmware/kube-fluentd-operator`](https://github.com/vmware/kube-fluentd-operator) 。

最后，我们需要设置`fluentd`聚合器。`fluentd`聚合器需要向 ELK 栈发送数据。它需要使用以下示例配置运行:

```
<match **>
       type elasticsearch
       log_level info
       host elasticsearch
       port 9200
       logstash_format true
       buffer_chunk_limit 2M
       buffer_queue_limit 8
       flush_interval 5s
       num_threads 2
 </match>

```

如果前面的配置有效，我们将在 ELK 栈中看到我们的应用日志。现在下一步是将 Istio 生成的访问日志发送到 ELK 实例。在下一节中，我们将看看 Mixer 扩展，它可以用来发送所需的日志。

## 搅拌器

Istio 使用可扩展的混合器子系统捕获所有遥测数据。该子系统非常灵活，允许对不同的监控和警报系统采用即插即用的方法。这种抽象使运营团队能够改变他们的应用监控方法，而不需要任何开发更改。Istio Mixer 部署了许多适配器。这些适配器中的每一个都向诸如 Prometheus、StatsD 等监控系统提交所需的数据。Envoy sidecar 为每个请求调用 Mixer，从而通过适配器捕获所有数据。由于 Envoy 为它收到的每个请求调用 Mixer，因此将 Mixer 组件嵌入到 sidecar 中听起来可能是合乎逻辑的。但是具有独立混合器组件的方法具有以下优点:

*   混音器是一个独立制造的组件；因此，它更符合 Istio 设计原则。另一方面，Envoy 是 Lyft 的代理服务。这种固有的差异使得 Mixer 更容易扩展到完整的方法。

*   该方法使系统更具容错性。Mixer 有许多外部依赖性，因此更容易出现网络故障。另一方面，特使不能容忍失败。即使混音器依赖项不可用，它也必须保持运行。

*   拥有独立的混合器和特使组件的方法使整个生态系统更加安全。Mixer 与各种外部系统集成。因此，它可能有许多安全漏洞。但是这些问题在调音台级别上受到限制。每个 Envoy 实例都可以配置为具有非常窄的交互范围，从而限制潜在攻击的影响。

*   Istio 为每个实例部署了一个边车；因此，边车必须尽可能轻。将所有第三方改编从边车中分离出来使得边车更加灵活。

混频器使 Istio 系统更加灵活，但也增加了系统的复杂性。混合器目前支持以下三种用例:

*   前提条件检查

*   配额管理，如 API 限制

*   遥测报告，如日志和请求

Istio 提供了多种适配器，可以使用 Mixer 进行配置。我们可以尝试扩展上一节中的日志示例。我们能够在 ELK 实例中发送我们的应用日志。我们现在需要发送访问日志。现在让我们试着用混音器来实现这一点。在我们继续之前，调用以下命令来获取可用适配器的列表:

```
$ kubectl get crd -listio=mixer-adapter
NAME                              CREATED AT
adapters.config.istio.io          2019-07-14T07:46:10Z
bypasses.config.istio.io          2019-07-14T07:45:59Z
circonuses.config.istio.io        2019-07-14T07:45:59Z
deniers.config.istio.io           2019-07-14T07:46:00Z
fluentds.config.istio.io          2019-07-14T07:46:00Z
Kubernetes envs.config.istio.io   2019-07-14T07:46:00Z
listcheckers.config.istio.io      2019-07-14T07:46:00Z
memquotas.config.istio.io         2019-07-14T07:46:01Z
noops.config.istio.io             2019-07-14T07:46:01Z
opas.config.istio.io              2019-07-14T07:46:02Z
prometheuses.config.istio.io      2019-07-14T07:46:02Z
rbacs.config.istio.io             2019-07-14T07:46:03Z
redisquotas.config.istio.io       2019-07-14T07:46:03Z
servicecontrols.config.istio.io   2019-07-14T07:46:04Z
signalfxs.config.istio.io         2019-07-14T07:46:04Z
solarwindses.config.istio.io      2019-07-14T07:46:04Z
stackdrivers.config.istio.io      2019-07-14T07:46:05Z
statsds.config.istio.io           2019-07-14T07:46:05Z
stdios.config.istio.io            2019-07-14T07:46:05Z

```

Istio 1.2 附带了丰富的适配器，如 Zipkin、StatsD、Stackdriver、CloudWatch 等。可以在 [`https://istio.io/docs/reference/config/policy-and-telemetry/adapters/`](https://istio.io/docs/reference/config/policy-and-telemetry/adapters/) 访问适配器的完整列表。

既然我们知道有各种适配器可用，我们将尝试为我们的应用配置它们。每个可用的适配器都可以使用图 [8-4](#Fig4) 所示的组件进行配置。

![../images/483921_1_En_8_Chapter/483921_1_En_8_Fig4_HTML.jpg](../images/483921_1_En_8_Chapter/483921_1_En_8_Fig4_HTML.jpg)

图 8-4

适配器组件

### 处理者

处理程序描述了需要如何调用适配器。它提供了必要的选项，可用于配置相关适配器的行为。可用处理程序的列表取决于服务网格中部署的适配器。此外，我们需要参考适配器文档来了解可用的配置选项。作为一个例子，让我们看看`fluentds.config.istio.io`适配器。该适配器用于将访问日志发送到`fluentd`聚合器 demon。

```
---
apiVersion: config.istio.io/v1alpha2
kind: handler
metadata:
  name: fluentdhandler
  namespace: istio-system
spec:
  compiledAdapter: fluentd
  params:
    address: "fluentd-aggregator-host:port"

```

注意，我们没有描述日志格式，我们为应用日志描述了日志格式。

### 情况

一个实例定义了我们需要为一个请求捕获什么数据。数据以一组属性的形式表示。属性表示为名称和类型。类型定义属性保存的数据种类。因此，我们可以说一个属性描述了一个请求的单个属性。例如，属性可用于指定 HTTP 响应代码或每个 HTTP 头。对于每个请求，Envoy sidecar 都将相关的属性发送到混合器子系统。特使边车通过使用可用的环境/请求/响应值来生成这些属性。

Istio 有一组在所有请求中都可用的公共属性。这些属性的列表可在 [`https://istio.io/docs/reference/config/policy-and-telemetry/attribute-vocabulary/`](https://istio.io/docs/reference/config/policy-and-telemetry/attribute-vocabulary/) 获得。

适配器不能理解任何类型的数据。适配器理解的数据在四个*模板*中编译。每个模板都有一组可以被适配器捕获的属性。每个适配器都有一个可用于发送数据的模板列表。因此，实例可以被定义为属性的映射，由侧柜发送到相关适配器的模板中。以下命令显示可用模板的列表:

```
$ kubectl get crd -listio=mixer-instance
NAME                                    CREATED AT
apikeys.config.istio.io                 2019-07-14T07:46:06Z
authorizations.config.istio.io          2019-07-14T07:46:06Z
checknothings.config.istio.io           2019-07-14T07:46:06Z
edges.config.istio.io                   2019-07-14T07:46:07Z
instances.config.istio.io               2019-07-14T07:46:11Z
Kubernetes es.config.istio.io            2019-07-14T07:46:06Z
listentries.config.istio.io             2019-07-14T07:46:07Z
logentries.config.istio.io              2019-07-14T07:46:07Z
metrics.config.istio.io                 2019-07-14T07:46:08Z
quotas.config.istio.io                  2019-07-14T07:46:08Z
reportnothings.config.istio.io          2019-07-14T07:46:08Z
servicecontrolreports.config.istio.io   2019-07-14T07:46:08Z
tracespans.config.istio.io              2019-07-14T07:46:09Z

```

我们可以使用以下命令来确定具有先前模板的现有实例:

```
$ kubectl get logentries.config.istio.io --all-namespaces
NAMESPACE      NAME           AGE
istio-system   accesslog      41d
istio-system   tcpaccesslog   41d
We can look at the accesslog definition to know which details are captured by it:
$ kubectl get logentries.config.istio.io accesslog -n istio-system -o yaml
apiVersion: config.istio.io/v1alpha2
kind: logentry
metadata:
  // REMOVED FOR BREVITY
spec:
  monitored_resource_type: '"global"'
  severity: '"Info"'
  timestamp: request.time
  variables:
    apiClaims: request.auth.raw_claims | ""
    apiKey: request.api_key | request.headers["x-api-key"] | ""
    .......
    destinationApp: destination.labels["app"] | ""
    destinationIp: destination.ip | ip("0.0.0.0")
    destinationName: destination.name | ""
    latency: response.duration | "0ms"
    method: request.method | ""
    protocol: request.scheme | context.protocol | "http"
    receivedBytes: request.total_size | 0
    // REMOVED for brevity

```

前面的日志条目捕获了完整的请求-响应属性集。该模板还为缺失的属性分配默认值。前一项是预编译的实例。但是如果它不可用，我们可以使用以下 YAML 配置添加一个实例:

```
 apiVersion: config.istio.io/v1alpha2
kind: instance
metadata:
  name: myaccesslog
  namespace: istio-system
spec:
  compiledTemplate: logentry
  params:
    severity: '"info"'
    timestamp: request.time
    variables:
      source: source.labels["app"] | source.workload.name | "unknown"
      user: source.user | "unknown"
      destination: destination.labels["app"] | destination.workload.name | "unknown"
      responseCode: response.code | 0
      responseSize: response.size | 0
      latency: response.duration | "0ms"
    monitored_resource_type: '"UNSPECIFIED"'

```

### 规则

规则将部署的处理程序与部署的实例结合起来。它在调用相关实例之前匹配指定条件的请求。规则必须指定处理程序和实例的完全限定名。如果它们都部署在同一个名称空间中，那么规则可以使用短名称。以下规则将`accesslog`和`myaccesslog`日志发送到前面创建的`fluentd`处理程序:

```
---
apiVersion: config.istio.io/v1alpha2
kind: rule
metadata:
  name: fluentdrule
  namespace: istio-system
spec:
  match: "true"
  actions:
   - handler: fluentdhandler
     instances: [ myaccesslog, accesslog ]

```

现在，我们可以使用以下命令部署所有这些组件:

```
$ kubectl apply -f fluentd-adapter.yaml

```

接下来，让我们使用`curl`命令来访问我们的服务。这将生成日志并将它们发送到 ELK 实例。让我们通过在 Kibana 中进行查找来验证这一点。

前面的例子被用作一个简单的垫脚石。Istio 也可以扩展到其他用例。

*   使用 StatsD 构建统计数据并将其发送到 Icinga/Nargios

*   验证类似配额的 API 限制

## 摘要

在这一章中，我们看了 Istio 提供的可观察性特性。我们从捕获请求跟踪开始，以确定应用性能。Istio 允许我们使用自己选择的跟踪解决方案。接下来，我们希望捕获应用级日志。我们意识到 Istio 没有提供应用日志的解决方案。然而，我们可以扩展我们的 Kubernetes 集群，为我们部署的应用提供一个日志解决方案。该解决方案不需要任何额外的开发工作。为此，我们部署了一个 ELK 实例，并使用 Sidecar 模式路由应用日志。下一个目标是扩展日志解决方案，以包含由 Istio 生成的日志。在旅途中，我们使用混合器组件来实现边车日志摄取。总之，我们使用了 Istio 的可扩展性特性，该特性可用于将其与第三方系统连接。