# 8.应用自我修复

当您在群集上启动一个 Pod 时，它被安排在群集的特定节点上。如果节点在给定时刻无法继续托管该 Pod，则该 Pod 将不会在新节点上重新启动—应用不会自我修复。

让我们尝试一下，在一个有多个工作者的集群上(例如，在第 [1](01.html) 章中安装的集群上)。

首先，运行一个 Pod 然后检查它在哪个节点上被调度:

```
$ kubectl run nginx --image=nginx
pod/nginx created
$ kubectl get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP           NODE
nginx   1/1     Running   0          12s   10.244.1.8   worker-0

```

这里，Pod 已经被安排在节点`worker-0`上。

让我们将此节点置于维护模式，看看 Pod 会发生什么情况:

```
$ kubectl drain worker-0 --force
node/worker-0 cordoned
WARNING: deleting Pods not managed by ReplicationController, ReplicaSet, Job, Daemon\
Set or StatefulSet: default/nginx
evicting pod "nginx"
pod/nginx evicted
node/worker-0 evicted
$ kubectl get pods
No resources found in default namespace.

```

您可以看到您创建的 Pod 已经消失，并且没有在另一个节点中重新创建。我们在这里完成实验。您可以使您的节点再次可调度:

```
$ kubectl uncordon worker-0
node/worker-0 uncordoned

```

## 控制员呼叫救援

我们在第 [5](05.html) 章“Pod 控制器”一节中已经看到，如果一个节点停止工作，使用 Pod 控制器可以确保您的 Pod 被调度到另一个节点。

让我们再体验一次，用一个`Deployment`:

```
$ kubectl create deployment nginx --image=nginx
deployment.apps/nginx created
$ kubectl get pods -o wide
NAME                     READY   STATUS   RESTARTS   AGE   IP          NODE
nginx-554b9c67f9-ndtsz   1/1     Running  0          11s   10.244.1.9 worker-0
$ kubectl drain worker-0
node/worker-0 cordoned
evicting pod "nginx-554b9c67f9-ndtsz"
pod/nginx-554b9c67f9-ndtsz evicted
node/worker-0 evicted
$ kubectl get pods -o wide
NAME                     READY   STATUS   RESTARTS   AGE      IP            NODE
nginx-554b9c67f9-5kz5v   1/1     Running  0          4s      10.244.2.9    worker-1

```

这一次，我们可以看到一个 Pod 已在集群的另一个节点中重新创建——我们的应用现在在节点驱逐后仍然存在。

## 活性探针

可以为 Pod 的每个容器定义一个活性探测器。如果 kubelet 不能成功地执行给定次数的探测，则认为容器不健康，并在同一个 Pod 中重新启动。

该探针应用于检测容器是否无响应。

活性探测有三种可能性:

*   发出一个 HTTP 请求。

    如果您的容器是一个 HTTP 服务器，您可以添加一个总是以成功响应进行回复的端点，并使用该端点定义探测。如果您的后端不再健康，这个端点可能也不会响应。

*   执行命令。

    大多数服务器应用都有相关的 CLI 应用。您可以使用这个 CLI 在服务器上执行一个非常简单的操作。如果服务器不健康，很可能它也不会响应这个简单的请求。

*   建立 TCP 连接。

    当运行在容器中的服务器通过非 HTTP 协议(在 TCP 之上)进行通信时，您可以尝试打开应用的套接字。如果服务器不正常，它可能不会响应此连接请求。

您必须使用声明形式来声明活跃度探测器。

## 关于就绪性探测的说明

注意，也可以为容器定义一个**就绪探测器**。就绪探测器的主要作用是指示 Pod 是否准备好为网络请求提供服务。当就绪探测成功时，Pod 将被添加到匹配`Services`的后端列表中。

稍后，在容器执行期间，如果准备就绪探测失败，Pod 将从`Services`的后端列表中删除。这对于检测容器不能处理更多的连接(例如，如果它已经在处理大量的连接)并停止发送新的连接是有用的。

## HTTP 请求活性探测

```
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    livenessProbe:
      httpGet:
        scheme: HTTP
        port: 80
        path: /healthz

```

这里，我们定义了一个查询`/healthz`端点的探测器。由于 **nginx** 在默认情况下没有被配置为回复这个路径，它将回复一个 404 响应代码，探测将会失败。这不是一个真实的案例，但是它模拟了一个 nginx 服务器对一个简单请求的错误回复。

您可以在 Pod 事件中看到，在三次失败的探测之后，容器被重新启动:

```
$ kubectl describe pod nginx
[...]
Events:
  Type     Reason     Age                From              Message
  ----     ------     ----               ----              -------
  Normal   Started    31s                kubelet, minikube Started container nginx
  Normal   Pulling    0s (x2 over 33s)   kubelet, minikube Pulling image "nginx"
  Warning  Unhealthy  0s (x3 over 20s)   kubelet, minikube Liveness probe failed: HTTP probe failed with statuscode: 404
  Normal   Killing    0s                 kubelet, minikube Container nginx failed liveness probe, will be restarted

```

## 命令活性探测

```
apiVersion: v1

kind: Pod
metadata:
  name: postgres
spec:
  containers:
  - image: postgres
    name: postgres
    livenessProbe:
      initialDelaySeconds: 10
      exec:
        command:
        - "psql"
        - "-h"
        - "localhost"
        - "-U"
        - "unknownUser"
        - "-c"
        - "select 1"

```

这里，活跃度探测器试图使用`psql`命令连接到服务器，并以用户`unknownUser`的身份执行一个非常简单的 SQL 查询(`SELECT 1`)。由于该用户不存在，查询将失败。

您可以在 Pod 事件中看到，在三次失败的探测之后，容器被重新启动:

```
$ kubectl describe pod postgres
[...]
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  <unknown>                default-scheduler  Successfully assigned default/postgres to minikube
  Warning  Unhealthy  0s (x3 over 20s)         kubelet, minikube  Liveness probe failed: psql: error: could not connect to server: FATAL: role "unknownUser" does not exist
  Normal   Killing    0s                       kubelet, minikube  Container postgres failed liveness probe, will be restarted

```

## TCP 连接活性探测

```
apiVersion: v1

kind: Pod
metadata:
  name: postgres
spec:
  containers:
  - image: postgres
    name: postgres
    livenessProbe:
      initialDelaySeconds: 10
      tcpSocket:
        port: 5433

```

这里，活跃度探测器试图连接到`5433`端口上的容器。由于`postgres`监听端口`5432`，连接将失败。

您可以在 Pod 事件中看到，在三次失败的探测之后，容器被重新启动:

```
$ kubectl describe pod postgres
[...]
Events:
  Type     Reason     Age               From                Message
  ----     ------     ----              ----                -------
  Normal   Started    25s               kubelet, minikube   Started container postgres
  Warning  Unhealthy  0s (x3 over 15s)  kubelet, minikube   Liveness probe failed: dial tcp 172.17.0.3:5433: connect: connection refused
  Normal   Killing    0s                kubelet, minikube   Container postgres failed  liveness probe, will be restarted

```

## 资源限制和服务质量(QoS)等级

您可以为每个 Pods 容器定义资源(CPU 和内存)请求和限制。

资源请求值用于在至少具有所请求的可用资源的节点中调度 Pod(参见第 [9](09.html) 章“资源请求”一节)。

如果不声明限制，每个容器仍然可以访问节点的所有资源；在这种情况下，如果一些容器在给定的时间没有使用所有它们请求的资源，一些其他容器将能够使用它们，反之亦然。

相反，如果为容器声明了限制，容器将被约束到那些特定的资源。如果它试图分配比它的限制更多的内存，它将得到一个内存分配错误，可能会崩溃或工作在降级模式；并且它只能访问其限制范围内的 CPU。

根据是否声明了请求和限制值，为 Pod 保证了不同的服务质量:

*   如果一个 Pod 的所有容器都声明了对所有资源(CPU 和内存)的请求和限制，并且限制等于请求，则该 Pod 将以*保证的* QoS 等级运行。

*   或者如果 Pod 的至少一个容器具有资源请求或限制，则 Pod 将以*可突发的* QoS 等级运行。

*   否则，如果没有为其容器声明请求或限制，则该 Pod 将以*尽力而为* QoS 等级运行。

如果一个节点用完了不可压缩的资源(内存)，相关联的 kubelet 可以决定弹出一个或多个 pod，以防止资源完全匮乏。

被驱逐的 pod 取决于它们的服务质量等级:首先是*尽力而为*的 pod，然后是*突发*的 pod，最后是*保证*的 pod。