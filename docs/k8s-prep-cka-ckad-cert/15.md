# 十五、升级集群

升级 Kubernetes 集群分两个阶段完成。首先升级控制面板节点，然后升级工作节点。可以升级到下一个次要版本或同一次要版本的任何其他下一个修补程序版本。例如，当您的群集使用版本 1.18.6 时，您可以升级到 1.18.p(其中 p >= 7)和 1.19.x(无论 x 的值如何)，但不能升级到 1.20.x。

由于控制面板和工作器在主机系统上运行，您还需要升级这些系统。您将看到如何准备集群，以便在不中断应用的情况下进行这些操作。

最后，您将了解如何备份和恢复集群证书和数据。

## 升级控制器

首先，安装想要的 kubeadm 版本:

```
$ gcloud compute ssh controller
Welcome to controller
$ sudo apt update && apt-cache policy kubeadm
$ sudo apt update && \
   sudo apt-get install \
   -y --allow-change-held-packages \
   kubeadm=1.19.0-00
$ sudo apt-mark hold kubeadm
$ kubeadm version -o short
v1.19.0

```

排空控制器节点:

```
$ kubectl drain controller --ignore-daemonsets \
   --delete-local-data
node/controller evicted

```

检查可能的升级计划:

```
$ sudo kubeadm upgrade plan

[...]

Components that must be upgraded manually after you have upgraded the control plane \
with 'kubeadm upgrade apply':
COMPONENT   CURRENT       AVAILABLE
Kubelet     3 x v1.18.6   v1.19.0

Upgrade to the latest stable version

:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.18.6   v1.19.0
Controller Manager   v1.18.6   v1.19.0
Scheduler            v1.18.6   v1.19.0
Kube Proxy           v1.18.6   v1.19.0
CoreDNS              1.6.7     1.6.7
Etcd                 3.4.3     3.4.3-0

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.19.0

```

显示了几种可能性:一种是升级到系列的最新版本，另一种是升级到使用了`kubeadm`版本的最新稳定版本。

在前面的屏幕中，仅显示升级到最新稳定版本的计划。

让我们开始升级:

```
$ sudo kubeadm upgrade apply v1.19.0
[...]
[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.19.0". Enjoy!

```

再次使节点可调度:

```
$ kubectl uncordon controller
node/controller uncordoned

```

升级控制器上的`kubelet`和`kubectl`:

```
$ sudo apt-get update && \
   sudo apt-get install \
   -y --allow-change-held-packages \
   kubelet=1.19.0-00 kubectl=1.19.0-00
$ sudo apt-mark hold kubelet kubectl

```

此时，控制器节点应该显示最新版本:

```
$ kubectl get nodes
NAME         STATUS   ROLES   AGE   VERSION
controller   Ready    master  13d   v1.19.0
worker-0     Ready    <none>  13d   v1.18.6
worker-1     Ready    <none>  13d   v1.18.6

```

## 提升工人

对每个工人重复这些步骤。

首先，从您的计算机中清空节点:

```
$ kubectl drain worker-0 --ignore-daemonsets
node/worker-0 cordoned
node/worker-0  drained

```

然后，连接到节点并安装所需版本的 kubeadm:

```
$ gcloud compute ssh worker-0
Welcome to worker-0
$ sudo apt update && \
   sudo apt-get install \
   -y --allow-change-held-packages \
   kubeadm=1.19.0-00
$ sudo apt-mark hold kubeadm
$ kubeadm version -o short
v1.19.0

```

升级节点:

```
$ sudo kubeadm upgrade node
[...]
[upgrade] The configuration for this node was successfully updated!

```

升级节点上的`kubelet`和`kubectl`:

```
$ sudo apt-get update && \
   sudo apt-get install \
   -y --allow-change-held-packages \
   kubelet=1.19.0-00 kubectl=1.19.0-00
$ sudo apt-mark hold kubelet kubectl

```

从您的计算机上，再次使节点可调度:

```
$ kubectl uncordon worker-0
node/worker-0 uncordoned

```

升级所有工作节点后，您应该在群集的每个节点上获得最新版本:

```
$ kubectl get nodes
NAME         STATUS   ROLES   AGE   VERSION
controller   Ready    master  13d   v1.19.0
worker-0     Ready    <none>  13d   v1.19.0
worker-1     Ready    <none>  13d   v1.19.0

```

## 升级操作系统

如果您需要重新启动集群节点的主机进行维护(例如，进行内核或硬件升级)，您首先需要**清空**节点:

```
$ kubectl drain $NODENAME --ignore-daemonsets
node/nodename drained

```

排空节点将产生两种效果:

*   从该节点中逐出所有单元，由副本集控制的所有单元将在另一个节点上重新安排。

*   使此节点不可计划，以便在维护期间不会在此节点上计划新的 Pod。

现在，您可以安全地对操作系统或硬件进行维护操作。

维护结束后，您可以**取消对节点的授权**,使节点再次可调度:

```
$ kubectl uncordon $NODENAME
node/nodename uncordoned

```

## 备份群集

集群安装后备份文件`/etc/kubernetes/pki/ca.crt`和`/etc/kubernetes/pki/ca.key`。

使用以下命令定期创建`etcd`数据库的快照

```
$ kubectl exec -it -n kube-system etcd-controller \
   sh -- -c "ETCDCTL_API=3 etcdctl snapshot save snapshot.db \
   --cacert /etc/kubernetes/pki/etcd/server.crt \
   --cert /etc/kubernetes/pki/etcd/ca.crt \
   --key /etc/kubernetes/pki/etcd/ca.key"
Snapshot saved at snapshot.db
$ kubectl cp -n kube-system etcd-controller:snapshot.db snapshot.db

```

## 恢复集群

按照第 [1 章](01.html)中的步骤重新安装控制器，并在运行`kubeadm init`前停止。

将 ca.crt 和 ca.key 复制到/etc/kubernetes/pki 中，恢复良好的权限:

```
# ls -l /etc/kubernetes/pki/
total  8
-rw-r--r-- 1 root root 1025 Feb 1 10:43 ca.crt
-rw------- 1 root root 1675 Feb 1 10:43 ca.key

```

将 snapshot.db 放在/mnt 中，然后运行:

```
$ docker run --rm \
   -v '/mnt:/backup' \
   -v '/var/lib/etcd:/var/lib/etcd' \
   --env ETCDCTL_API=3 \
   'k8s.gcr.io/etcd-amd64:3.1.12' \
   /bin/sh -c \
   "etcdctl snapshot restore /backup/snapshot.db ; mv /default.etcd/member/ /var/lib/etcd/"

```

再次安装控制面板:

```
$ gcloud config set compute/zone us-west1-c # or your selected zone
Updated property [compute/zone].
$ KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute instances describe controller \
  --zone $(gcloud config get-value compute/zone) \
  --format='get(networkInterfaces[0].accessConfigs[0].natIP)')
$ sudo kubeadm init \
  --pod-network-cidr=10.244.0.0/16 \
  --ignore-preflight-errors=NumCPU \
  --apiserver-cert-extra-sans=$KUBERNETES_PUBLIC_ADDRESS \
  --ignore-preflight-errors=DirAvailable--var-lib-etcd

```